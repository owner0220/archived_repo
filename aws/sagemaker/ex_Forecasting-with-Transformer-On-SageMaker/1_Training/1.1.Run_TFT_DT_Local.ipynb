{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b27d946-a068-458a-9a19-a4e1f4abfcf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [모듈 1.1] 노트북 인스턴스에서 분산 훈련 하기\n",
    "\n",
    "이 노트북은 커널을 'conda_python3' 를 사용합니다.\n",
    "---\n",
    "세이지 메이커의 훈련 잡으로 모델 훈련을 하기 전에, 노트북에서 훈련 코드 (TFT_Train.py) 를 실행합니다. 현재 이 노트북 인스턴스가 보유한 모든 GPUs 를 사용 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444f2fd-db36-4489-a066-8bca587dc171",
   "metadata": {},
   "source": [
    "# 1. 환경 설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a23fa-eee6-4b29-945e-751105551487",
   "metadata": {},
   "source": [
    "## 파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41139d8-0a16-4a4b-874e-895745acd082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_gpus:  8\n",
      "epochs:  1\n",
      "model_dir:  model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "epochs = 1\n",
    "num_gpus = torch.cuda.device_count()\n",
    "model_dir = 'model'\n",
    "# num_gpus = 4\n",
    "train_notebook = True\n",
    "\n",
    "print(\"num_gpus: \", num_gpus)\n",
    "print(\"epochs: \", epochs)\n",
    "print(\"model_dir: \", model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5260fbeb-2154-4d55-a127-291244231ec4",
   "metadata": {},
   "source": [
    "## 환경 변수 설정\n",
    "\n",
    "- 아래의 세이지 메이커 도커 컨테이너의 환경 변수 (에: SM_MODEL_DIR) 는 이 노트북에서는 실제 필요 하지 않습니다.\n",
    "- 아래의 환경 변수는 추후에 세이지 메이커의 도커 컨테이너안에서는 자동으로 환경 변수가 설정이 됩니다. 여기서는 추후에 훈련 스크립트인 'scripts/TFT_Train.py' 의 수정없이 사용을 하기 위해서 세이지 메이커 관련 환경 변수를 설정 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c0eafa-d780-44aa-89da-6934f675e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_notebook:\n",
    "\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "    src_dir = os.getcwd()\n",
    "    os.environ['SM_MODEL_DIR'] = f'{src_dir}/{model_dir}'\n",
    "    os.environ['SM_NUM_GPUS'] = str(num_gpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c07adc-d09c-41b0-b695-5275163bb916",
   "metadata": {},
   "source": [
    "# 2. 훈련 코드를 직접 로컬에서 실행\n",
    "- --n_gpus {num_gpus} --epochs {epochs} 와 같은 파이라미터를 전달하여 실행 합니다.\n",
    "- 실행 완료 후에 model_dir 경로에 모델 가중치 파일이 저장 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811536df-a0b1-4e8b-9e86-5bee8aedd412",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on notebook\n",
      "***** Arguments *****\n",
      "epochs=1\n",
      "seed=100\n",
      "train_batch_size=64\n",
      "model_dir=/home/ec2-user/SageMaker/Forecasting-with-Transformer-On-SageMaker/1_Training/model\n",
      "n_gpus=8\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Number of parameters in network: 29.7k\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 8 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \n",
      "3  | prescalers                         | ModuleDict                      | 256   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 119   \n",
      "----------------------------------------------------------------------------------------\n",
      "29.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "29.7 K    Total params\n",
      "0.119     Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                           | 0/21 [00:00<?, ?it/s][W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch 0:  95%|▉| 20/21 [00:09<00:00,  2.00it/s, loss=253, v_num=3, train_loss_st\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00,  3.64it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 21/21 [00:11<00:00,  1.87it/s, loss=253, v_num=3, train_loss_st\u001b[A\n",
      "Epoch 0: 100%|█| 21/21 [00:11<00:00,  1.83it/s, loss=253, v_num=3, train_loss_st\u001b[A\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python src/TFT_Train.py --n_gpus {num_gpus} --epochs {epochs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc8308-d46e-4017-82d4-5d5b0288b2c2",
   "metadata": {},
   "source": [
    "# 3. 모델 가중치 파일 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad560bd-200f-46af-8548-e4506cb55265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pth']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564849c-9754-4a10-b417-86ec80a74d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
