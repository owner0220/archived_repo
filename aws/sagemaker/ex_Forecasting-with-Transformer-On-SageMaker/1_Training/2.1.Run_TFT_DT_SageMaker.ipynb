{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b27d946-a068-458a-9a19-a4e1f4abfcf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [모듈 2.1] 세이지메이커에서 분산 훈련 하기\n",
    "\n",
    "이 노트북은 커널을 'conda_python3' 를 사용합니다.\n",
    "\n",
    "---\n",
    "이 노트북은 PyTorch Lightning 의 Multi GPUs 기능으로 1개의 인스턴스에서 (ml.g4dn.12xlarge) 에서 훈련 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444f2fd-db36-4489-a066-8bca587dc171",
   "metadata": {},
   "source": [
    "# 1. 환경 설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9bd08-1fe9-4306-ae4b-8cd0a99ac564",
   "metadata": {},
   "source": [
    "## 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e68172-0ed2-448b-ad99-0659323e4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97204ea5-68c0-4f1e-8dec-0318772d38a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker.__version__\n",
    "\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a23fa-eee6-4b29-945e-751105551487",
   "metadata": {},
   "source": [
    "## 파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f41139d8-0a16-4a4b-874e-895745acd082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_gpus:  8\n",
      "epochs:  5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "epochs = 5\n",
    "num_gpus = torch.cuda.device_count()\n",
    "# model_dir = 'model'\n",
    "# num_gpus = 4\n",
    "# train_notebook = True\n",
    "\n",
    "print(\"num_gpus: \", num_gpus)\n",
    "print(\"epochs: \", epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bcda9-79ef-4f79-9721-ee13e2618439",
   "metadata": {},
   "source": [
    "# 2. 세이지 메이크 로컬 모드 훈련\n",
    "#### 로컬의 GPU, CPU 여부로 instance_type 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07a8e179-f1a7-40b3-a275-fcba9bf11ede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  3 14:24:01 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:17.0 Off |                    0 |\n",
      "| N/A   29C    P0    40W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:18.0 Off |                    0 |\n",
      "| N/A   28C    P0    41W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:00:19.0 Off |                    0 |\n",
      "| N/A   27C    P0    38W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:00:1A.0 Off |                    0 |\n",
      "| N/A   28C    P0    41W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   27C    P0    41W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   28C    P0    40W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   28C    P0    41W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   28C    P0    43W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "try:\n",
    "    if subprocess.call(\"nvidia-smi\") == 0:\n",
    "        ## Set type to GPU if one is present\n",
    "        instance_type = \"local_gpu\"\n",
    "    else:\n",
    "        instance_type = \"local\"        \n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e25ed9c-3f2a-40b1-ae9b-6ec7b8b8687f",
   "metadata": {},
   "source": [
    "## 로컬 모드로 훈련 실행\n",
    "- 아래의 두 라인이 로컬모드로 훈련을 지시 합니다.\n",
    "```python\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.LocalSession(), # 로컬 세션을 사용합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdb1893b-20a3-4867-bdac-eec5206e1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': epochs, \n",
    "                   'n_gpus': num_gpus,\n",
    "                    }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e76fe8b-e1ed-4438-8d2d-3a662f37c558",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-04-03-14-24-10-049\n",
      "INFO:sagemaker.local.local_session:Starting training job\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker.local.image:No AWS credentials found in session but credentials from EC2 Metadata Service are available.\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-56zlg:\n",
      "    command: train\n",
      "    container_name: ktn1e20lvt-algo-1-56zlg\n",
      "    deploy:\n",
      "      resources:\n",
      "        reservations:\n",
      "          devices:\n",
      "          - capabilities:\n",
      "            - gpu\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.12.1-gpu-py38\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-56zlg\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmpf53v9rtk/algo-1-56zlg/output/data:/opt/ml/output/data\n",
      "    - /tmp/tmpf53v9rtk/algo-1-56zlg/output:/opt/ml/output\n",
      "    - /tmp/tmpf53v9rtk/algo-1-56zlg/input:/opt/ml/input\n",
      "    - /tmp/tmpf53v9rtk/model:/opt/ml/model\n",
      "    - /opt/ml/metadata:/opt/ml/metadata\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker-compose -f /tmp/tmpf53v9rtk/docker-compose.yaml up --build --abort-on-container-exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ktn1e20lvt-algo-1-56zlg ... \n",
      "Creating ktn1e20lvt-algo-1-56zlg ... done\n",
      "Attaching to ktn1e20lvt-algo-1-56zlg\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:13,698 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:13,763 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:13,772 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:13,775 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:13,783 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:13,848 botocore.credentials INFO     Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:14,050 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m /opt/conda/bin/python3.8 -m pip install -r requirements.txt\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting pytorch-forecasting==0.10.3\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading pytorch_forecasting-0.10.3-py3-none-any.whl (141 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.4/141.4 kB 5.5 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting pytorch-lightning==1.6.3\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading pytorch_lightning-1.6.3-py3-none-any.whl (584 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 584.0/584.0 kB 53.4 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: pyarrow==11.0.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (11.0.0)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting tensorboard==2.12.0\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 81.4 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting scikit-learn<1.2,>=0.24\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading scikit_learn-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31.2/31.2 MB 52.8 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.7.0)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting statsmodels\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading statsmodels-0.13.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 82.8 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: scipy<2.0,>=1.8 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.10.0)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: pandas<2.0.0,>=1.3.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.5.3)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: torch<2.0,>=1.7 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.12.1+cu113)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting optuna<3.0.0,>=2.3.0\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.2/308.2 kB 38.0 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting pyDeprecate<0.4.0,>=0.3.1\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (23.0)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting torchmetrics>=0.4.1\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.2/519.2 kB 37.6 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (4.4.0)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (1.23.5)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (5.4.1)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (2023.1.0)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (4.64.1)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (2.28.2)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (65.6.3)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting grpcio>=1.48.2\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading grpcio-1.53.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/5.0 MB 93.0 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (3.20.2)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting absl-py>=0.4\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 20.1 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 94.3 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting google-auth<3,>=1.6.3\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading google_auth-2.17.1-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 178.1/178.1 kB 23.6 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting tensorboard-plugin-wit>=1.6.0\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 56.1 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (0.38.4)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting markdown>=2.6.8\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.9/93.9 kB 14.5 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (2.2.3)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 65.3 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (4.7.2)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting cachetools<6.0,>=2.0.0\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (1.16.0)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting pyasn1-modules>=0.2.1\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 23.5 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting requests-oauthlib>=0.7.0\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard==2.12.0->-r requirements.txt (line 6)) (4.13.0)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting cmaes>=0.8.2\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting alembic\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading alembic-1.10.2-py3-none-any.whl (212 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.2/212.2 kB 26.1 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting cliff\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading cliff-4.2.0-py3-none-any.whl (81 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.0/81.0 kB 14.1 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting sqlalchemy>=1.1.0\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading SQLAlchemy-2.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 85.0 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting colorlog\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas<2.0.0,>=1.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2.8.2)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas<2.0.0,>=1.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2022.7.1)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (2.1.1)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (3.4)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (1.26.14)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (2022.12.7)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<1.2,>=0.24->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.2.0)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<1.2,>=0.24->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.1.0)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard==2.12.0->-r requirements.txt (line 6)) (2.1.2)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (0.11.0)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (9.4.0)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (4.38.0)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.0.9)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.0.7)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: importlib-resources>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (5.10.2)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.4.4)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting patsy>=0.5.2\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.8/233.8 kB 32.7 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (22.2.0)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting yarl<2.0,>=1.0\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 262.1/262.1 kB 30.1 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting frozenlist>=1.1.1\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.3/161.3 kB 25.1 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting aiosignal>=1.1.2\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting async-timeout<5.0,>=4.0.0a3\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting multidict<7.0,>=4.5\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 19.1 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.12.0->-r requirements.txt (line 6)) (3.13.0)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (0.4.8)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting oauthlib>=3.0.0\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 24.3 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.8/site-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2.0.2)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting Mako\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 12.7 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting stevedore>=2.0.1\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading stevedore-5.0.0-py3-none-any.whl (49 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.6/49.6 kB 9.1 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting PrettyTable>=0.7.2\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading prettytable-3.6.0-py3-none-any.whl (27 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting cmd2>=1.0.0\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading cmd2-2.4.3-py3-none-any.whl (147 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.2/147.2 kB 23.6 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting autopage>=0.4.0\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Requirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (0.2.6)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting pyperclip>=1.6\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Preparing metadata (setup.py): started\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Preparing metadata (setup.py): finished with status 'done'\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Collecting pbr!=2.1.0,>=2.0.0\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Downloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112.7/112.7 kB 17.0 MB/s eta 0:00:00\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Building wheels for collected packages: pyperclip\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Building wheel for pyperclip (setup.py): started\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Building wheel for pyperclip (setup.py): finished with status 'done'\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11124 sha256=5adaf9f860b6baf483ab552ed2810646d846483cf319847f2f8d17fb86cca81b\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Stored in directory: /root/.cache/pip/wheels/7f/1a/65/84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Successfully built pyperclip\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Installing collected packages: tensorboard-plugin-wit, pyperclip, tensorboard-data-server, sqlalchemy, pyDeprecate, pyasn1-modules, PrettyTable, pbr, patsy, oauthlib, multidict, Mako, grpcio, frozenlist, colorlog, cmd2, cmaes, cachetools, autopage, async-timeout, absl-py, yarl, torchmetrics, stevedore, scikit-learn, requests-oauthlib, markdown, google-auth, alembic, aiosignal, statsmodels, google-auth-oauthlib, cliff, aiohttp, tensorboard, optuna, pytorch-lightning, pytorch-forecasting\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Attempting uninstall: scikit-learn\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Found existing installation: scikit-learn 1.2.1\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Uninstalling scikit-learn-1.2.1:\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Successfully uninstalled scikit-learn-1.2.1\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Successfully installed Mako-1.2.4 PrettyTable-3.6.0 absl-py-1.4.0 aiohttp-3.8.4 aiosignal-1.3.1 alembic-1.10.2 async-timeout-4.0.2 autopage-0.5.1 cachetools-5.3.0 cliff-4.2.0 cmaes-0.9.1 cmd2-2.4.3 colorlog-6.7.0 frozenlist-1.3.3 google-auth-2.17.1 google-auth-oauthlib-0.4.6 grpcio-1.53.0 markdown-3.4.3 multidict-6.0.4 oauthlib-3.2.2 optuna-2.10.1 patsy-0.5.3 pbr-5.11.1 pyDeprecate-0.3.2 pyasn1-modules-0.2.8 pyperclip-1.8.2 pytorch-forecasting-0.10.3 pytorch-lightning-1.6.3 requests-oauthlib-1.3.1 scikit-learn-1.1.3 sqlalchemy-2.0.8 statsmodels-0.13.5 stevedore-5.0.0 tensorboard-2.12.0 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 torchmetrics-0.11.4 yarl-1.8.2\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [notice] To update, run: pip install --upgrade pip\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:36,899 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:36,899 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:36,969 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:36,980 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:37,050 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:37,060 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:37,130 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:37,140 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:37,144 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Training Env:\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m {\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"channel_input_dirs\": {},\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"current_host\": \"algo-1-56zlg\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"current_instance_group\": \"homogeneousCluster\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"current_instance_group_hosts\": [],\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"current_instance_type\": \"local\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"distribution_hosts\": [\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m         \"algo-1-56zlg\"\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     ],\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"distribution_instance_groups\": [],\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m         \"algo-1-56zlg\"\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     ],\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m         \"epochs\": 5,\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m         \"n_gpus\": 8\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     },\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"input_data_config\": {},\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"instance_groups\": [],\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"instance_groups_dict\": {},\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"is_hetero\": false,\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"is_modelparallel_enabled\": null,\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"is_smddpmprun_installed\": true,\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"job_name\": \"pytorch-training-2023-04-03-14-24-10-049\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"master_hostname\": \"algo-1-56zlg\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-04-03-14-24-10-049/source/sourcedir.tar.gz\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"module_name\": \"TFT_Train\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"num_cpus\": 64,\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"num_gpus\": 8,\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"num_neurons\": 0,\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m         \"current_host\": \"algo-1-56zlg\",\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m             \"algo-1-56zlg\"\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m         ]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     },\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m     \"user_entry_point\": \"TFT_Train.py\"\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m }\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Environment variables:\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_HOSTS=[\"algo-1-56zlg\"]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_HPS={\"epochs\":5,\"n_gpus\":8}\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_USER_ENTRY_POINT=TFT_Train.py\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-56zlg\",\"hosts\":[\"algo-1-56zlg\"]}\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_INPUT_DATA_CONFIG={}\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_CHANNELS=[]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_CURRENT_HOST=algo-1-56zlg\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_CURRENT_INSTANCE_TYPE=local\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_CURRENT_INSTANCE_GROUP_HOSTS=[]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_INSTANCE_GROUPS=[]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_INSTANCE_GROUPS_DICT={}\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_IS_HETERO=false\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_MODULE_NAME=TFT_Train\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_NUM_CPUS=64\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_NUM_GPUS=8\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_NUM_NEURONS=0\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-04-03-14-24-10-049/source/sourcedir.tar.gz\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1-56zlg\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[],\"current_instance_type\":\"local\",\"distribution_hosts\":[\"algo-1-56zlg\"],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-56zlg\"],\"hyperparameters\":{\"epochs\":5,\"n_gpus\":8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[],\"instance_groups_dict\":{},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-04-03-14-24-10-049\",\"log_level\":20,\"master_hostname\":\"algo-1-56zlg\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-04-03-14-24-10-049/source/sourcedir.tar.gz\",\"module_name\":\"TFT_Train\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-56zlg\",\"hosts\":[\"algo-1-56zlg\"]},\"user_entry_point\":\"TFT_Train.py\"}\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"5\",\"--n_gpus\",\"8\"]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_HP_EPOCHS=5\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m SM_HP_N_GPUS=8\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/flash_attn-0.1-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/einops-0.6.0-py3.8.egg\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m /opt/conda/bin/python3.8 TFT_Train.py --epochs 5 --n_gpus 8\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:24:41,627 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Not running on notebook\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ***** Arguments *****\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m epochs=5\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m seed=100\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m train_batch_size=64\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m model_dir=/opt/ml/model\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m n_gpus=8\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m GPU available: True, used: True\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Number of parameters in network: 29.7k\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ----------------------------------------------------------------------------------------------------\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m distributed_backend=nccl\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m All distributed processes registered. Starting with 8 processes\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ----------------------------------------------------------------------------------------------------\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:364:364 [0] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:364:364 [0] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m NCCL version 2.10.3+cuda11.3\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:735:735 [5] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:885:885 [7] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:585:585 [3] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:437:437 [1] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:810:810 [6] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:585:585 [3] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:735:735 [5] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:437:437 [1] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:885:885 [7] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:810:810 [6] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:510:510 [2] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:510:510 [2] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:660:660 [4] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m algo-1-56zlg:660:660 [4] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m | Name                               | Type                            | Params\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 0  | loss                               | QuantileLoss                    | 0     \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 1  | logging_metrics                    | ModuleList                      | 0     \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 3  | prescalers                         | ModuleDict                      | 256   \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 20 | output_layer                       | Linear                          | 119   \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 29.7 K    Trainable params\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 0         Non-trainable params\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 29.7 K    Total params\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 0.119     Total estimated model params size (MB)\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Sanity Checking: 0it [00:00, ?it/s]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:16.976 algo-1-56zlg:437 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:16.977 algo-1-56zlg:810 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:17.012 algo-1-56zlg:735 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:17.013 algo-1-56zlg:364 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:17.014 algo-1-56zlg:585 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:17.016 algo-1-56zlg:885 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:17.017 algo-1-56zlg:510 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:17.020 algo-1-56zlg:660 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:17.132 algo-1-56zlg:437 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:17.133 algo-1-56zlg:810 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:17.169 algo-1-56zlg:735 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:17.172 algo-1-56zlg:364 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:17.173 algo-1-56zlg:585 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:17.177 algo-1-56zlg:885 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:17.178 algo-1-56zlg:510 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [2023-04-03 14:26:17.182 algo-1-56zlg:660 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Training: 0it [00:00, ?it/s]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Training:   0%|          | 0/21 [00:00<?, ?it/s]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:   0%|          | 0/21 [00:00<?, ?it/s]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch 0:   5%|▍         | 1/21 [00:00<00:10,  1.98it/s]    | 1/21 [00:00<00:10,  1.98it/s]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:   5%|▍         | 1/21 [00:00<00:10,  1.98it/s, loss=430, v_num=0, train_loss_step=430.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  10%|▉         | 2/21 [00:00<00:07,  2.48it/s, loss=430, v_num=0, train_loss_step=430.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  10%|▉         | 2/21 [00:00<00:07,  2.47it/s, loss=430, v_num=0, train_loss_step=430.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  10%|▉         | 2/21 [00:00<00:07,  2.47it/s, loss=364, v_num=0, train_loss_step=297.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  14%|█▍        | 3/21 [00:01<00:06,  2.67it/s, loss=364, v_num=0, train_loss_step=297.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  14%|█▍        | 3/21 [00:01<00:06,  2.67it/s, loss=364, v_num=0, train_loss_step=297.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  14%|█▍        | 3/21 [00:01<00:06,  2.67it/s, loss=388, v_num=0, train_loss_step=435.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  19%|█▉        | 4/21 [00:01<00:06,  2.83it/s, loss=388, v_num=0, train_loss_step=435.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  19%|█▉        | 4/21 [00:01<00:06,  2.83it/s, loss=388, v_num=0, train_loss_step=435.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  19%|█▉        | 4/21 [00:01<00:06,  2.83it/s, loss=373, v_num=0, train_loss_step=330.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  24%|██▍       | 5/21 [00:01<00:05,  2.98it/s, loss=373, v_num=0, train_loss_step=330.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  24%|██▍       | 5/21 [00:01<00:05,  2.98it/s, loss=373, v_num=0, train_loss_step=330.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  24%|██▍       | 5/21 [00:01<00:05,  2.98it/s, loss=378, v_num=0, train_loss_step=395.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  29%|██▊       | 6/21 [00:02<00:05,  2.99it/s, loss=378, v_num=0, train_loss_step=395.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  29%|██▊       | 6/21 [00:02<00:05,  2.99it/s, loss=378, v_num=0, train_loss_step=395.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  29%|██▊       | 6/21 [00:02<00:05,  2.99it/s, loss=356, v_num=0, train_loss_step=250.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  33%|███▎      | 7/21 [00:02<00:04,  3.06it/s, loss=356, v_num=0, train_loss_step=250.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  33%|███▎      | 7/21 [00:02<00:04,  3.06it/s, loss=356, v_num=0, train_loss_step=250.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  33%|███▎      | 7/21 [00:02<00:04,  3.06it/s, loss=355, v_num=0, train_loss_step=347.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  38%|███▊      | 8/21 [00:02<00:04,  3.10it/s, loss=355, v_num=0, train_loss_step=347.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  38%|███▊      | 8/21 [00:02<00:04,  3.10it/s, loss=355, v_num=0, train_loss_step=347.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  38%|███▊      | 8/21 [00:02<00:04,  3.10it/s, loss=347, v_num=0, train_loss_step=288.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  43%|████▎     | 9/21 [00:02<00:03,  3.13it/s, loss=347, v_num=0, train_loss_step=288.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  43%|████▎     | 9/21 [00:02<00:03,  3.13it/s, loss=347, v_num=0, train_loss_step=288.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  43%|████▎     | 9/21 [00:02<00:03,  3.13it/s, loss=352, v_num=0, train_loss_step=399.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  48%|████▊     | 10/21 [00:03<00:03,  3.15it/s, loss=352, v_num=0, train_loss_step=399.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  48%|████▊     | 10/21 [00:03<00:03,  3.14it/s, loss=352, v_num=0, train_loss_step=399.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  48%|████▊     | 10/21 [00:03<00:03,  3.14it/s, loss=349, v_num=0, train_loss_step=320.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  52%|█████▏    | 11/21 [00:03<00:03,  3.17it/s, loss=349, v_num=0, train_loss_step=320.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  52%|█████▏    | 11/21 [00:03<00:03,  3.17it/s, loss=349, v_num=0, train_loss_step=320.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  52%|█████▏    | 11/21 [00:03<00:03,  3.17it/s, loss=342, v_num=0, train_loss_step=273.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  57%|█████▋    | 12/21 [00:03<00:02,  3.12it/s, loss=342, v_num=0, train_loss_step=273.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  57%|█████▋    | 12/21 [00:03<00:02,  3.12it/s, loss=342, v_num=0, train_loss_step=273.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  57%|█████▋    | 12/21 [00:03<00:02,  3.12it/s, loss=341, v_num=0, train_loss_step=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  62%|██████▏   | 13/21 [00:04<00:02,  3.12it/s, loss=341, v_num=0, train_loss_step=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  62%|██████▏   | 13/21 [00:04<00:02,  3.12it/s, loss=341, v_num=0, train_loss_step=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  62%|██████▏   | 13/21 [00:04<00:02,  3.12it/s, loss=337, v_num=0, train_loss_step=298.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  67%|██████▋   | 14/21 [00:04<00:02,  3.15it/s, loss=337, v_num=0, train_loss_step=298.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  67%|██████▋   | 14/21 [00:04<00:02,  3.15it/s, loss=337, v_num=0, train_loss_step=298.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  67%|██████▋   | 14/21 [00:04<00:02,  3.15it/s, loss=335, v_num=0, train_loss_step=305.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  71%|███████▏  | 15/21 [00:04<00:01,  3.17it/s, loss=335, v_num=0, train_loss_step=305.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  71%|███████▏  | 15/21 [00:04<00:01,  3.17it/s, loss=335, v_num=0, train_loss_step=305.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  71%|███████▏  | 15/21 [00:04<00:01,  3.17it/s, loss=332, v_num=0, train_loss_step=295.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  76%|███████▌  | 16/21 [00:05<00:01,  3.17it/s, loss=332, v_num=0, train_loss_step=295.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  76%|███████▌  | 16/21 [00:05<00:01,  3.17it/s, loss=332, v_num=0, train_loss_step=295.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  76%|███████▌  | 16/21 [00:05<00:01,  3.17it/s, loss=329, v_num=0, train_loss_step=279.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  81%|████████  | 17/21 [00:05<00:01,  3.18it/s, loss=329, v_num=0, train_loss_step=279.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  81%|████████  | 17/21 [00:05<00:01,  3.18it/s, loss=329, v_num=0, train_loss_step=279.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  81%|████████  | 17/21 [00:05<00:01,  3.18it/s, loss=326, v_num=0, train_loss_step=271.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  86%|████████▌ | 18/21 [00:05<00:00,  3.17it/s, loss=326, v_num=0, train_loss_step=271.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  86%|████████▌ | 18/21 [00:05<00:00,  3.17it/s, loss=326, v_num=0, train_loss_step=271.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  86%|████████▌ | 18/21 [00:05<00:00,  3.17it/s, loss=320, v_num=0, train_loss_step=219.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  90%|█████████ | 19/21 [00:05<00:00,  3.19it/s, loss=320, v_num=0, train_loss_step=219.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  90%|█████████ | 19/21 [00:05<00:00,  3.19it/s, loss=320, v_num=0, train_loss_step=219.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  90%|█████████ | 19/21 [00:05<00:00,  3.19it/s, loss=313, v_num=0, train_loss_step=199.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  95%|█████████▌| 20/21 [00:06<00:00,  3.20it/s, loss=313, v_num=0, train_loss_step=199.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  95%|█████████▌| 20/21 [00:06<00:00,  3.20it/s, loss=313, v_num=0, train_loss_step=199.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:  95%|█████████▌| 20/21 [00:06<00:00,  3.20it/s, loss=312, v_num=0, train_loss_step=285.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.52it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.52it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0: 100%|██████████| 21/21 [00:06<00:00,  3.14it/s, loss=312, v_num=0, train_loss_step=285.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0: 100%|██████████| 21/21 [00:06<00:00,  3.14it/s, loss=312, v_num=0, train_loss_step=285.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0: 100%|██████████| 21/21 [00:07<00:00,  2.78it/s, loss=312, v_num=0, train_loss_step=285.0, val_loss=339.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0: 100%|██████████| 21/21 [00:07<00:00,  2.77it/s, loss=312, v_num=0, train_loss_step=285.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 0:   0%|          | 0/21 [00:00<?, ?it/s, loss=312, v_num=0, train_loss_step=285.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:   0%|          | 0/21 [00:00<?, ?it/s, loss=312, v_num=0, train_loss_step=285.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:   5%|▍         | 1/21 [00:00<00:05,  3.87it/s, loss=312, v_num=0, train_loss_step=285.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:   5%|▍         | 1/21 [00:00<00:05,  3.87it/s, loss=312, v_num=0, train_loss_step=285.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:   5%|▍         | 1/21 [00:00<00:05,  3.86it/s, loss=301, v_num=0, train_loss_step=209.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  10%|▉         | 2/21 [00:00<00:05,  3.65it/s, loss=301, v_num=0, train_loss_step=209.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  10%|▉         | 2/21 [00:00<00:05,  3.64it/s, loss=301, v_num=0, train_loss_step=209.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  10%|▉         | 2/21 [00:00<00:05,  3.64it/s, loss=300, v_num=0, train_loss_step=275.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  14%|█▍        | 3/21 [00:00<00:05,  3.40it/s, loss=300, v_num=0, train_loss_step=275.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  14%|█▍        | 3/21 [00:00<00:05,  3.40it/s, loss=300, v_num=0, train_loss_step=275.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  14%|█▍        | 3/21 [00:00<00:05,  3.40it/s, loss=292, v_num=0, train_loss_step=281.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  19%|█▉        | 4/21 [00:01<00:05,  3.33it/s, loss=292, v_num=0, train_loss_step=281.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  19%|█▉        | 4/21 [00:01<00:05,  3.33it/s, loss=292, v_num=0, train_loss_step=281.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  19%|█▉        | 4/21 [00:01<00:05,  3.33it/s, loss=285, v_num=0, train_loss_step=182.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  24%|██▍       | 5/21 [00:01<00:04,  3.34it/s, loss=285, v_num=0, train_loss_step=182.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  24%|██▍       | 5/21 [00:01<00:04,  3.34it/s, loss=285, v_num=0, train_loss_step=182.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  24%|██▍       | 5/21 [00:01<00:04,  3.33it/s, loss=277, v_num=0, train_loss_step=240.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  29%|██▊       | 6/21 [00:01<00:04,  3.35it/s, loss=277, v_num=0, train_loss_step=240.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  29%|██▊       | 6/21 [00:01<00:04,  3.35it/s, loss=277, v_num=0, train_loss_step=240.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  29%|██▊       | 6/21 [00:01<00:04,  3.35it/s, loss=278, v_num=0, train_loss_step=274.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  33%|███▎      | 7/21 [00:02<00:04,  3.23it/s, loss=278, v_num=0, train_loss_step=274.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  33%|███▎      | 7/21 [00:02<00:04,  3.23it/s, loss=278, v_num=0, train_loss_step=274.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  33%|███▎      | 7/21 [00:02<00:04,  3.23it/s, loss=272, v_num=0, train_loss_step=229.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  38%|███▊      | 8/21 [00:02<00:03,  3.26it/s, loss=272, v_num=0, train_loss_step=229.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  38%|███▊      | 8/21 [00:02<00:03,  3.26it/s, loss=272, v_num=0, train_loss_step=229.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  38%|███▊      | 8/21 [00:02<00:03,  3.26it/s, loss=267, v_num=0, train_loss_step=193.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  43%|████▎     | 9/21 [00:02<00:03,  3.28it/s, loss=267, v_num=0, train_loss_step=193.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  43%|████▎     | 9/21 [00:02<00:03,  3.28it/s, loss=267, v_num=0, train_loss_step=193.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  43%|████▎     | 9/21 [00:02<00:03,  3.28it/s, loss=260, v_num=0, train_loss_step=244.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  48%|████▊     | 10/21 [00:03<00:03,  3.27it/s, loss=260, v_num=0, train_loss_step=244.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  48%|████▊     | 10/21 [00:03<00:03,  3.27it/s, loss=260, v_num=0, train_loss_step=244.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  48%|████▊     | 10/21 [00:03<00:03,  3.27it/s, loss=252, v_num=0, train_loss_step=170.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  52%|█████▏    | 11/21 [00:03<00:03,  3.28it/s, loss=252, v_num=0, train_loss_step=170.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  52%|█████▏    | 11/21 [00:03<00:03,  3.28it/s, loss=252, v_num=0, train_loss_step=170.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  52%|█████▏    | 11/21 [00:03<00:03,  3.28it/s, loss=249, v_num=0, train_loss_step=204.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  57%|█████▋    | 12/21 [00:03<00:02,  3.28it/s, loss=249, v_num=0, train_loss_step=204.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  57%|█████▋    | 12/21 [00:03<00:02,  3.28it/s, loss=249, v_num=0, train_loss_step=204.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  57%|█████▋    | 12/21 [00:03<00:02,  3.28it/s, loss=243, v_num=0, train_loss_step=205.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  62%|██████▏   | 13/21 [00:03<00:02,  3.29it/s, loss=243, v_num=0, train_loss_step=205.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  62%|██████▏   | 13/21 [00:03<00:02,  3.29it/s, loss=243, v_num=0, train_loss_step=205.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  62%|██████▏   | 13/21 [00:03<00:02,  3.28it/s, loss=243, v_num=0, train_loss_step=299.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  67%|██████▋   | 14/21 [00:04<00:02,  3.30it/s, loss=243, v_num=0, train_loss_step=299.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  67%|██████▋   | 14/21 [00:04<00:02,  3.30it/s, loss=243, v_num=0, train_loss_step=299.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  67%|██████▋   | 14/21 [00:04<00:02,  3.30it/s, loss=236, v_num=0, train_loss_step=175.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  71%|███████▏  | 15/21 [00:04<00:01,  3.30it/s, loss=236, v_num=0, train_loss_step=175.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  71%|███████▏  | 15/21 [00:04<00:01,  3.30it/s, loss=236, v_num=0, train_loss_step=175.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  71%|███████▏  | 15/21 [00:04<00:01,  3.30it/s, loss=232, v_num=0, train_loss_step=213.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  76%|███████▌  | 16/21 [00:04<00:01,  3.25it/s, loss=232, v_num=0, train_loss_step=213.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  76%|███████▌  | 16/21 [00:04<00:01,  3.24it/s, loss=232, v_num=0, train_loss_step=213.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  76%|███████▌  | 16/21 [00:04<00:01,  3.24it/s, loss=226, v_num=0, train_loss_step=151.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  81%|████████  | 17/21 [00:05<00:01,  3.26it/s, loss=226, v_num=0, train_loss_step=151.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  81%|████████  | 17/21 [00:05<00:01,  3.26it/s, loss=226, v_num=0, train_loss_step=151.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  81%|████████  | 17/21 [00:05<00:01,  3.26it/s, loss=222, v_num=0, train_loss_step=199.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  86%|████████▌ | 18/21 [00:05<00:00,  3.27it/s, loss=222, v_num=0, train_loss_step=199.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  86%|████████▌ | 18/21 [00:05<00:00,  3.27it/s, loss=222, v_num=0, train_loss_step=199.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  86%|████████▌ | 18/21 [00:05<00:00,  3.27it/s, loss=223, v_num=0, train_loss_step=232.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  90%|█████████ | 19/21 [00:05<00:00,  3.28it/s, loss=223, v_num=0, train_loss_step=232.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  90%|█████████ | 19/21 [00:05<00:00,  3.27it/s, loss=223, v_num=0, train_loss_step=232.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  90%|█████████ | 19/21 [00:05<00:00,  3.27it/s, loss=221, v_num=0, train_loss_step=164.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  95%|█████████▌| 20/21 [00:06<00:00,  3.28it/s, loss=221, v_num=0, train_loss_step=164.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  95%|█████████▌| 20/21 [00:06<00:00,  3.28it/s, loss=221, v_num=0, train_loss_step=164.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:  95%|█████████▌| 20/21 [00:06<00:00,  3.28it/s, loss=215, v_num=0, train_loss_step=171.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1: 100%|██████████| 21/21 [00:06<00:00,  3.22it/s, loss=215, v_num=0, train_loss_step=171.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1: 100%|██████████| 21/21 [00:06<00:00,  3.22it/s, loss=215, v_num=0, train_loss_step=171.0, val_loss=339.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1: 100%|██████████| 21/21 [00:07<00:00,  2.84it/s, loss=215, v_num=0, train_loss_step=171.0, val_loss=244.0, train_loss_epoch=322.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1: 100%|██████████| 21/21 [00:07<00:00,  2.84it/s, loss=215, v_num=0, train_loss_step=171.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 1:   0%|          | 0/21 [00:00<?, ?it/s, loss=215, v_num=0, train_loss_step=171.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:   0%|          | 0/21 [00:00<?, ?it/s, loss=215, v_num=0, train_loss_step=171.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:   5%|▍         | 1/21 [00:00<00:04,  4.09it/s, loss=215, v_num=0, train_loss_step=171.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:   5%|▍         | 1/21 [00:00<00:04,  4.08it/s, loss=215, v_num=0, train_loss_step=171.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:   5%|▍         | 1/21 [00:00<00:04,  4.08it/s, loss=210, v_num=0, train_loss_step=111.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  10%|▉         | 2/21 [00:00<00:05,  3.47it/s, loss=210, v_num=0, train_loss_step=111.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  10%|▉         | 2/21 [00:00<00:05,  3.47it/s, loss=210, v_num=0, train_loss_step=111.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  10%|▉         | 2/21 [00:00<00:05,  3.47it/s, loss=205, v_num=0, train_loss_step=171.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  14%|█▍        | 3/21 [00:00<00:05,  3.33it/s, loss=205, v_num=0, train_loss_step=171.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  14%|█▍        | 3/21 [00:00<00:05,  3.33it/s, loss=205, v_num=0, train_loss_step=171.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  14%|█▍        | 3/21 [00:00<00:05,  3.33it/s, loss=199, v_num=0, train_loss_step=159.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  19%|█▉        | 4/21 [00:01<00:05,  3.37it/s, loss=199, v_num=0, train_loss_step=159.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  19%|█▉        | 4/21 [00:01<00:05,  3.37it/s, loss=199, v_num=0, train_loss_step=159.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  19%|█▉        | 4/21 [00:01<00:05,  3.37it/s, loss=197, v_num=0, train_loss_step=146.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  24%|██▍       | 5/21 [00:01<00:04,  3.38it/s, loss=197, v_num=0, train_loss_step=146.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  24%|██▍       | 5/21 [00:01<00:04,  3.38it/s, loss=197, v_num=0, train_loss_step=146.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  24%|██▍       | 5/21 [00:01<00:04,  3.38it/s, loss=193, v_num=0, train_loss_step=152.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  29%|██▊       | 6/21 [00:01<00:04,  3.37it/s, loss=193, v_num=0, train_loss_step=152.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  29%|██▊       | 6/21 [00:01<00:04,  3.37it/s, loss=193, v_num=0, train_loss_step=152.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  29%|██▊       | 6/21 [00:01<00:04,  3.37it/s, loss=187, v_num=0, train_loss_step=148.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  33%|███▎      | 7/21 [00:02<00:04,  3.41it/s, loss=187, v_num=0, train_loss_step=148.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  33%|███▎      | 7/21 [00:02<00:04,  3.41it/s, loss=187, v_num=0, train_loss_step=148.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  33%|███▎      | 7/21 [00:02<00:04,  3.41it/s, loss=185, v_num=0, train_loss_step=189.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  38%|███▊      | 8/21 [00:02<00:03,  3.40it/s, loss=185, v_num=0, train_loss_step=189.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  38%|███▊      | 8/21 [00:02<00:03,  3.40it/s, loss=185, v_num=0, train_loss_step=189.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  38%|███▊      | 8/21 [00:02<00:03,  3.40it/s, loss=180, v_num=0, train_loss_step=101.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  43%|████▎     | 9/21 [00:02<00:03,  3.40it/s, loss=180, v_num=0, train_loss_step=101.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  43%|████▎     | 9/21 [00:02<00:03,  3.40it/s, loss=180, v_num=0, train_loss_step=101.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  43%|████▎     | 9/21 [00:02<00:03,  3.40it/s, loss=175, v_num=0, train_loss_step=147.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  48%|████▊     | 10/21 [00:02<00:03,  3.39it/s, loss=175, v_num=0, train_loss_step=147.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  48%|████▊     | 10/21 [00:02<00:03,  3.39it/s, loss=175, v_num=0, train_loss_step=147.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  48%|████▊     | 10/21 [00:02<00:03,  3.39it/s, loss=174, v_num=0, train_loss_step=143.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  52%|█████▏    | 11/21 [00:03<00:02,  3.40it/s, loss=174, v_num=0, train_loss_step=143.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  52%|█████▏    | 11/21 [00:03<00:02,  3.40it/s, loss=174, v_num=0, train_loss_step=143.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  52%|█████▏    | 11/21 [00:03<00:02,  3.40it/s, loss=171, v_num=0, train_loss_step=152.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  57%|█████▋    | 12/21 [00:03<00:02,  3.40it/s, loss=171, v_num=0, train_loss_step=152.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  57%|█████▋    | 12/21 [00:03<00:02,  3.40it/s, loss=171, v_num=0, train_loss_step=152.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  57%|█████▋    | 12/21 [00:03<00:02,  3.40it/s, loss=169, v_num=0, train_loss_step=160.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  62%|██████▏   | 13/21 [00:03<00:02,  3.40it/s, loss=169, v_num=0, train_loss_step=160.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  62%|██████▏   | 13/21 [00:03<00:02,  3.40it/s, loss=169, v_num=0, train_loss_step=160.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  62%|██████▏   | 13/21 [00:03<00:02,  3.40it/s, loss=160, v_num=0, train_loss_step=117.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  67%|██████▋   | 14/21 [00:04<00:02,  3.36it/s, loss=160, v_num=0, train_loss_step=117.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  67%|██████▋   | 14/21 [00:04<00:02,  3.36it/s, loss=160, v_num=0, train_loss_step=117.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  67%|██████▋   | 14/21 [00:04<00:02,  3.36it/s, loss=159, v_num=0, train_loss_step=149.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  71%|███████▏  | 15/21 [00:04<00:01,  3.31it/s, loss=159, v_num=0, train_loss_step=149.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  71%|███████▏  | 15/21 [00:04<00:01,  3.31it/s, loss=159, v_num=0, train_loss_step=149.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  71%|███████▏  | 15/21 [00:04<00:01,  3.31it/s, loss=157, v_num=0, train_loss_step=186.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  76%|███████▌  | 16/21 [00:04<00:01,  3.23it/s, loss=157, v_num=0, train_loss_step=186.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  76%|███████▌  | 16/21 [00:04<00:01,  3.23it/s, loss=157, v_num=0, train_loss_step=186.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  76%|███████▌  | 16/21 [00:04<00:01,  3.22it/s, loss=157, v_num=0, train_loss_step=146.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  81%|████████  | 17/21 [00:05<00:01,  3.22it/s, loss=157, v_num=0, train_loss_step=146.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  81%|████████  | 17/21 [00:05<00:01,  3.22it/s, loss=157, v_num=0, train_loss_step=146.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  81%|████████  | 17/21 [00:05<00:01,  3.22it/s, loss=155, v_num=0, train_loss_step=158.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  86%|████████▌ | 18/21 [00:05<00:00,  3.23it/s, loss=155, v_num=0, train_loss_step=158.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  86%|████████▌ | 18/21 [00:05<00:00,  3.23it/s, loss=155, v_num=0, train_loss_step=158.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  86%|████████▌ | 18/21 [00:05<00:00,  3.23it/s, loss=152, v_num=0, train_loss_step=182.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  90%|█████████ | 19/21 [00:05<00:00,  3.25it/s, loss=152, v_num=0, train_loss_step=182.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  90%|█████████ | 19/21 [00:05<00:00,  3.25it/s, loss=152, v_num=0, train_loss_step=182.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  90%|█████████ | 19/21 [00:05<00:00,  3.25it/s, loss=150, v_num=0, train_loss_step=123.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  95%|█████████▌| 20/21 [00:06<00:00,  3.21it/s, loss=150, v_num=0, train_loss_step=123.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  95%|█████████▌| 20/21 [00:06<00:00,  3.21it/s, loss=150, v_num=0, train_loss_step=123.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:  95%|█████████▌| 20/21 [00:06<00:00,  3.21it/s, loss=150, v_num=0, train_loss_step=152.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2: 100%|██████████| 21/21 [00:06<00:00,  3.20it/s, loss=150, v_num=0, train_loss_step=152.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2: 100%|██████████| 21/21 [00:06<00:00,  3.20it/s, loss=150, v_num=0, train_loss_step=152.0, val_loss=244.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2: 100%|██████████| 21/21 [00:07<00:00,  2.81it/s, loss=150, v_num=0, train_loss_step=152.0, val_loss=206.0, train_loss_epoch=207.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2: 100%|██████████| 21/21 [00:07<00:00,  2.81it/s, loss=150, v_num=0, train_loss_step=152.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 2:   0%|          | 0/21 [00:00<?, ?it/s, loss=150, v_num=0, train_loss_step=152.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:   0%|          | 0/21 [00:00<?, ?it/s, loss=150, v_num=0, train_loss_step=152.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:   5%|▍         | 1/21 [00:00<00:05,  3.59it/s, loss=150, v_num=0, train_loss_step=152.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:   5%|▍         | 1/21 [00:00<00:05,  3.59it/s, loss=150, v_num=0, train_loss_step=152.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:   5%|▍         | 1/21 [00:00<00:05,  3.58it/s, loss=152, v_num=0, train_loss_step=154.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  10%|▉         | 2/21 [00:00<00:05,  3.37it/s, loss=152, v_num=0, train_loss_step=154.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  10%|▉         | 2/21 [00:00<00:05,  3.36it/s, loss=152, v_num=0, train_loss_step=154.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  10%|▉         | 2/21 [00:00<00:05,  3.36it/s, loss=151, v_num=0, train_loss_step=165.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  14%|█▍        | 3/21 [00:00<00:05,  3.36it/s, loss=151, v_num=0, train_loss_step=165.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  14%|█▍        | 3/21 [00:00<00:05,  3.36it/s, loss=151, v_num=0, train_loss_step=165.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  14%|█▍        | 3/21 [00:00<00:05,  3.36it/s, loss=150, v_num=0, train_loss_step=121.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  19%|█▉        | 4/21 [00:01<00:04,  3.43it/s, loss=150, v_num=0, train_loss_step=121.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  19%|█▉        | 4/21 [00:01<00:04,  3.43it/s, loss=150, v_num=0, train_loss_step=121.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  19%|█▉        | 4/21 [00:01<00:04,  3.43it/s, loss=149, v_num=0, train_loss_step=139.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  24%|██▍       | 5/21 [00:01<00:04,  3.46it/s, loss=149, v_num=0, train_loss_step=139.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  24%|██▍       | 5/21 [00:01<00:04,  3.46it/s, loss=149, v_num=0, train_loss_step=139.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  24%|██▍       | 5/21 [00:01<00:04,  3.46it/s, loss=148, v_num=0, train_loss_step=120.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  29%|██▊       | 6/21 [00:01<00:04,  3.38it/s, loss=148, v_num=0, train_loss_step=120.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  29%|██▊       | 6/21 [00:01<00:04,  3.38it/s, loss=148, v_num=0, train_loss_step=120.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  29%|██▊       | 6/21 [00:01<00:04,  3.38it/s, loss=145, v_num=0, train_loss_step=105.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  33%|███▎      | 7/21 [00:02<00:04,  3.40it/s, loss=145, v_num=0, train_loss_step=105.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  33%|███▎      | 7/21 [00:02<00:04,  3.39it/s, loss=145, v_num=0, train_loss_step=105.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  33%|███▎      | 7/21 [00:02<00:04,  3.39it/s, loss=142, v_num=0, train_loss_step=129.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  38%|███▊      | 8/21 [00:02<00:03,  3.38it/s, loss=142, v_num=0, train_loss_step=129.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  38%|███▊      | 8/21 [00:02<00:03,  3.38it/s, loss=142, v_num=0, train_loss_step=129.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  38%|███▊      | 8/21 [00:02<00:03,  3.38it/s, loss=145, v_num=0, train_loss_step=158.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  43%|████▎     | 9/21 [00:02<00:03,  3.38it/s, loss=145, v_num=0, train_loss_step=158.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  43%|████▎     | 9/21 [00:02<00:03,  3.38it/s, loss=145, v_num=0, train_loss_step=158.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  43%|████▎     | 9/21 [00:02<00:03,  3.38it/s, loss=145, v_num=0, train_loss_step=146.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  48%|████▊     | 10/21 [00:02<00:03,  3.36it/s, loss=145, v_num=0, train_loss_step=146.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  48%|████▊     | 10/21 [00:02<00:03,  3.36it/s, loss=145, v_num=0, train_loss_step=146.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  48%|████▊     | 10/21 [00:02<00:03,  3.36it/s, loss=145, v_num=0, train_loss_step=145.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  52%|█████▏    | 11/21 [00:03<00:02,  3.35it/s, loss=145, v_num=0, train_loss_step=145.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  52%|█████▏    | 11/21 [00:03<00:02,  3.35it/s, loss=145, v_num=0, train_loss_step=145.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  52%|█████▏    | 11/21 [00:03<00:02,  3.35it/s, loss=145, v_num=0, train_loss_step=139.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  57%|█████▋    | 12/21 [00:03<00:02,  3.33it/s, loss=145, v_num=0, train_loss_step=139.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  57%|█████▋    | 12/21 [00:03<00:02,  3.33it/s, loss=145, v_num=0, train_loss_step=139.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  57%|█████▋    | 12/21 [00:03<00:02,  3.33it/s, loss=145, v_num=0, train_loss_step=175.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  62%|██████▏   | 13/21 [00:03<00:02,  3.36it/s, loss=145, v_num=0, train_loss_step=175.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  62%|██████▏   | 13/21 [00:03<00:02,  3.36it/s, loss=145, v_num=0, train_loss_step=175.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  62%|██████▏   | 13/21 [00:03<00:02,  3.36it/s, loss=147, v_num=0, train_loss_step=140.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  67%|██████▋   | 14/21 [00:04<00:02,  3.37it/s, loss=147, v_num=0, train_loss_step=140.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  67%|██████▋   | 14/21 [00:04<00:02,  3.37it/s, loss=147, v_num=0, train_loss_step=140.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  67%|██████▋   | 14/21 [00:04<00:02,  3.37it/s, loss=146, v_num=0, train_loss_step=135.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  71%|███████▏  | 15/21 [00:04<00:01,  3.35it/s, loss=146, v_num=0, train_loss_step=135.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  71%|███████▏  | 15/21 [00:04<00:01,  3.35it/s, loss=146, v_num=0, train_loss_step=135.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  71%|███████▏  | 15/21 [00:04<00:01,  3.35it/s, loss=144, v_num=0, train_loss_step=152.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  76%|███████▌  | 16/21 [00:04<00:01,  3.34it/s, loss=144, v_num=0, train_loss_step=152.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  76%|███████▌  | 16/21 [00:04<00:01,  3.34it/s, loss=144, v_num=0, train_loss_step=152.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  76%|███████▌  | 16/21 [00:04<00:01,  3.34it/s, loss=143, v_num=0, train_loss_step=120.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  81%|████████  | 17/21 [00:05<00:01,  3.35it/s, loss=143, v_num=0, train_loss_step=120.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  81%|████████  | 17/21 [00:05<00:01,  3.35it/s, loss=143, v_num=0, train_loss_step=120.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  81%|████████  | 17/21 [00:05<00:01,  3.35it/s, loss=140, v_num=0, train_loss_step=94.20, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  86%|████████▌ | 18/21 [00:05<00:00,  3.29it/s, loss=140, v_num=0, train_loss_step=94.20, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  86%|████████▌ | 18/21 [00:05<00:00,  3.29it/s, loss=140, v_num=0, train_loss_step=94.20, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  86%|████████▌ | 18/21 [00:05<00:00,  3.28it/s, loss=136, v_num=0, train_loss_step=107.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  90%|█████████ | 19/21 [00:05<00:00,  3.29it/s, loss=136, v_num=0, train_loss_step=107.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  90%|█████████ | 19/21 [00:05<00:00,  3.29it/s, loss=136, v_num=0, train_loss_step=107.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  90%|█████████ | 19/21 [00:05<00:00,  3.29it/s, loss=134, v_num=0, train_loss_step=92.70, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  95%|█████████▌| 20/21 [00:06<00:00,  3.29it/s, loss=134, v_num=0, train_loss_step=92.70, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  95%|█████████▌| 20/21 [00:06<00:00,  3.29it/s, loss=134, v_num=0, train_loss_step=92.70, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:  95%|█████████▌| 20/21 [00:06<00:00,  3.29it/s, loss=132, v_num=0, train_loss_step=104.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation: 0it [00:00, ?it/s]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3: 100%|██████████| 21/21 [00:06<00:00,  3.26it/s, loss=132, v_num=0, train_loss_step=104.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3: 100%|██████████| 21/21 [00:06<00:00,  3.26it/s, loss=132, v_num=0, train_loss_step=104.0, val_loss=206.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3: 100%|██████████| 21/21 [00:07<00:00,  2.89it/s, loss=132, v_num=0, train_loss_step=104.0, val_loss=177.0, train_loss_epoch=151.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3: 100%|██████████| 21/21 [00:07<00:00,  2.89it/s, loss=132, v_num=0, train_loss_step=104.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 3:   0%|          | 0/21 [00:00<?, ?it/s, loss=132, v_num=0, train_loss_step=104.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:   0%|          | 0/21 [00:00<?, ?it/s, loss=132, v_num=0, train_loss_step=104.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:   5%|▍         | 1/21 [00:00<00:04,  4.14it/s, loss=132, v_num=0, train_loss_step=104.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:   5%|▍         | 1/21 [00:00<00:04,  4.13it/s, loss=132, v_num=0, train_loss_step=104.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:   5%|▍         | 1/21 [00:00<00:04,  4.13it/s, loss=132, v_num=0, train_loss_step=162.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  10%|▉         | 2/21 [00:00<00:04,  4.05it/s, loss=132, v_num=0, train_loss_step=162.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  10%|▉         | 2/21 [00:00<00:04,  4.05it/s, loss=132, v_num=0, train_loss_step=162.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  10%|▉         | 2/21 [00:00<00:04,  4.04it/s, loss=131, v_num=0, train_loss_step=137.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  14%|█▍        | 3/21 [00:00<00:04,  3.89it/s, loss=131, v_num=0, train_loss_step=137.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  14%|█▍        | 3/21 [00:00<00:04,  3.88it/s, loss=131, v_num=0, train_loss_step=137.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  14%|█▍        | 3/21 [00:00<00:04,  3.88it/s, loss=133, v_num=0, train_loss_step=153.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  19%|█▉        | 4/21 [00:01<00:04,  3.78it/s, loss=133, v_num=0, train_loss_step=153.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  19%|█▉        | 4/21 [00:01<00:04,  3.78it/s, loss=133, v_num=0, train_loss_step=153.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  19%|█▉        | 4/21 [00:01<00:04,  3.77it/s, loss=131, v_num=0, train_loss_step=99.00, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  24%|██▍       | 5/21 [00:01<00:04,  3.71it/s, loss=131, v_num=0, train_loss_step=99.00, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  24%|██▍       | 5/21 [00:01<00:04,  3.71it/s, loss=131, v_num=0, train_loss_step=99.00, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  24%|██▍       | 5/21 [00:01<00:04,  3.71it/s, loss=132, v_num=0, train_loss_step=156.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  29%|██▊       | 6/21 [00:01<00:04,  3.52it/s, loss=132, v_num=0, train_loss_step=156.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  29%|██▊       | 6/21 [00:01<00:04,  3.52it/s, loss=132, v_num=0, train_loss_step=156.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  29%|██▊       | 6/21 [00:01<00:04,  3.51it/s, loss=133, v_num=0, train_loss_step=123.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  33%|███▎      | 7/21 [00:02<00:04,  3.42it/s, loss=133, v_num=0, train_loss_step=123.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  33%|███▎      | 7/21 [00:02<00:04,  3.42it/s, loss=133, v_num=0, train_loss_step=123.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  33%|███▎      | 7/21 [00:02<00:04,  3.42it/s, loss=134, v_num=0, train_loss_step=150.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  38%|███▊      | 8/21 [00:02<00:03,  3.32it/s, loss=134, v_num=0, train_loss_step=150.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  38%|███▊      | 8/21 [00:02<00:03,  3.32it/s, loss=134, v_num=0, train_loss_step=150.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  38%|███▊      | 8/21 [00:02<00:03,  3.32it/s, loss=132, v_num=0, train_loss_step=117.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  43%|████▎     | 9/21 [00:02<00:03,  3.37it/s, loss=132, v_num=0, train_loss_step=117.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  43%|████▎     | 9/21 [00:02<00:03,  3.37it/s, loss=132, v_num=0, train_loss_step=117.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  43%|████▎     | 9/21 [00:02<00:03,  3.37it/s, loss=133, v_num=0, train_loss_step=153.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  48%|████▊     | 10/21 [00:02<00:03,  3.35it/s, loss=133, v_num=0, train_loss_step=153.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  48%|████▊     | 10/21 [00:02<00:03,  3.35it/s, loss=133, v_num=0, train_loss_step=153.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  48%|████▊     | 10/21 [00:02<00:03,  3.35it/s, loss=131, v_num=0, train_loss_step=113.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  52%|█████▏    | 11/21 [00:03<00:02,  3.37it/s, loss=131, v_num=0, train_loss_step=113.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  52%|█████▏    | 11/21 [00:03<00:02,  3.37it/s, loss=131, v_num=0, train_loss_step=113.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  52%|█████▏    | 11/21 [00:03<00:02,  3.37it/s, loss=132, v_num=0, train_loss_step=158.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  57%|█████▋    | 12/21 [00:03<00:02,  3.39it/s, loss=132, v_num=0, train_loss_step=158.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  57%|█████▋    | 12/21 [00:03<00:02,  3.39it/s, loss=132, v_num=0, train_loss_step=158.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  57%|█████▋    | 12/21 [00:03<00:02,  3.39it/s, loss=130, v_num=0, train_loss_step=130.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  62%|██████▏   | 13/21 [00:03<00:02,  3.40it/s, loss=130, v_num=0, train_loss_step=130.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  62%|██████▏   | 13/21 [00:03<00:02,  3.40it/s, loss=130, v_num=0, train_loss_step=130.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  62%|██████▏   | 13/21 [00:03<00:02,  3.40it/s, loss=129, v_num=0, train_loss_step=115.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  67%|██████▋   | 14/21 [00:04<00:02,  3.36it/s, loss=129, v_num=0, train_loss_step=115.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  67%|██████▋   | 14/21 [00:04<00:02,  3.36it/s, loss=129, v_num=0, train_loss_step=115.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  67%|██████▋   | 14/21 [00:04<00:02,  3.36it/s, loss=126, v_num=0, train_loss_step=92.70, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  71%|███████▏  | 15/21 [00:04<00:01,  3.30it/s, loss=126, v_num=0, train_loss_step=92.70, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  71%|███████▏  | 15/21 [00:04<00:01,  3.30it/s, loss=126, v_num=0, train_loss_step=92.70, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  71%|███████▏  | 15/21 [00:04<00:01,  3.30it/s, loss=124, v_num=0, train_loss_step=104.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  76%|███████▌  | 16/21 [00:04<00:01,  3.28it/s, loss=124, v_num=0, train_loss_step=104.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  76%|███████▌  | 16/21 [00:04<00:01,  3.28it/s, loss=124, v_num=0, train_loss_step=104.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  76%|███████▌  | 16/21 [00:04<00:01,  3.28it/s, loss=125, v_num=0, train_loss_step=145.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  81%|████████  | 17/21 [00:05<00:01,  3.30it/s, loss=125, v_num=0, train_loss_step=145.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  81%|████████  | 17/21 [00:05<00:01,  3.30it/s, loss=125, v_num=0, train_loss_step=145.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  81%|████████  | 17/21 [00:05<00:01,  3.30it/s, loss=125, v_num=0, train_loss_step=97.60, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  86%|████████▌ | 18/21 [00:05<00:00,  3.32it/s, loss=125, v_num=0, train_loss_step=97.60, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  86%|████████▌ | 18/21 [00:05<00:00,  3.32it/s, loss=125, v_num=0, train_loss_step=97.60, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  86%|████████▌ | 18/21 [00:05<00:00,  3.32it/s, loss=128, v_num=0, train_loss_step=149.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  90%|█████████ | 19/21 [00:05<00:00,  3.34it/s, loss=128, v_num=0, train_loss_step=149.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  90%|█████████ | 19/21 [00:05<00:00,  3.34it/s, loss=128, v_num=0, train_loss_step=149.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  90%|█████████ | 19/21 [00:05<00:00,  3.34it/s, loss=131, v_num=0, train_loss_step=161.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  95%|█████████▌| 20/21 [00:06<00:00,  3.33it/s, loss=131, v_num=0, train_loss_step=161.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  95%|█████████▌| 20/21 [00:06<00:00,  3.33it/s, loss=131, v_num=0, train_loss_step=161.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4:  95%|█████████▌| 20/21 [00:06<00:00,  3.33it/s, loss=133, v_num=0, train_loss_step=137.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4: 100%|██████████| 21/21 [00:06<00:00,  3.26it/s, loss=133, v_num=0, train_loss_step=137.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4: 100%|██████████| 21/21 [00:06<00:00,  3.26it/s, loss=133, v_num=0, train_loss_step=137.0, val_loss=177.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4: 100%|██████████| 21/21 [00:07<00:00,  2.88it/s, loss=133, v_num=0, train_loss_step=137.0, val_loss=170.0, train_loss_epoch=139.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m \u001b[A\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4: 100%|██████████| 21/21 [00:07<00:00,  2.87it/s, loss=133, v_num=0, train_loss_step=137.0, val_loss=170.0, train_loss_epoch=128.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m Epoch 4: 100%|██████████| 21/21 [00:07<00:00,  2.82it/s, loss=133, v_num=0, train_loss_step=137.0, val_loss=170.0, train_loss_epoch=128.0]\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:27:00,105 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:27:00,105 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36mktn1e20lvt-algo-1-56zlg |\u001b[0m 2023-04-03 14:27:00,106 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:creating /tmp/tmpf53v9rtk/artifacts/output/data\n",
      "INFO:root:copying /tmp/tmpf53v9rtk/algo-1-56zlg/output/success -> /tmp/tmpf53v9rtk/artifacts/output\n",
      "INFO:root:copying /tmp/tmpf53v9rtk/model/model.pth -> /tmp/tmpf53v9rtk/artifacts/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mktn1e20lvt-algo-1-56zlg exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "local_estimator = PyTorch(\n",
    "    entry_point=\"TFT_Train.py\",    \n",
    "    source_dir='src',    \n",
    "    role=role,\n",
    "    framework_version='1.12.1',    \n",
    "    py_version='py38',        \n",
    "    instance_count=1,\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.LocalSession(), # 로컬 세션을 사용합니다.\n",
    "    hyperparameters= hyperparameters               \n",
    "    \n",
    ")\n",
    "local_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d49513c-1e54-4fae-b1c6-6fe5ff5839a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. SageMaker Cloud Mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500470c-5e58-4d39-8d2e-b15e364cd945",
   "metadata": {},
   "source": [
    "리소스 프로파일링 관련 링크 입니다.\n",
    "- [프로파일링 셋업](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-configuration-for-profiling.html)\n",
    "- [Debugger Python SDK](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-debugger/tensorflow_builtin_rule/tf-mnist-builtin-rule.html)\n",
    "- [Open the Amazon SageMaker Debugger Insights Dashboard](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-on-studio-insights.html)\n",
    "- [New – Profile Your Machine Learning Training Jobs With Amazon SageMaker Debugger](https://aws.amazon.com/blogs/aws/profile-your-machine-learning-training-jobs-with-amazon-sagemaker-debugger/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d1bb25-6912-4e62-a46a-e01682f20d8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 파라미터 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6e47078-88f2-470a-adcc-82ae4c0305a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = 'ml.g4dn.12xlarge' # AMD Radeon Pro V520 4장 GPU\n",
    "\n",
    "epochs = 500\n",
    "hyperparameters = {'epochs': epochs, \n",
    "                    }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10515ee2-b28f-49d6-9d8f-d09bdb3f9ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-04-03-14-45-07-300\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import os\n",
    "\n",
    "from sagemaker.debugger import ProfilerConfig, ProfilerRule, rule_configs\n",
    "\n",
    "\n",
    "profiler_config=ProfilerConfig(\n",
    "    system_monitor_interval_millis=1000\n",
    ")\n",
    "rules=[\n",
    "    # ProfilerRule.sagemaker(rule_configs.BuiltInRule())\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "]\n",
    "\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"TFT_Train.py\",    \n",
    "    source_dir='src',    \n",
    "    role=role,\n",
    "    framework_version='1.12.1',    \n",
    "    py_version='py38',     \n",
    "    instance_count=1,\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.Session(),\n",
    "    hyperparameters= hyperparameters,\n",
    "    profiler_config=profiler_config,\n",
    "    rules=rules,\n",
    "    \n",
    ")\n",
    "estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66e74128-03f0-48d1-b279-29f25d29e559",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-03 14:45:15 Starting - Starting the training job...\n",
      "2023-04-03 14:45:37 Starting - Preparing the instances for trainingProfilerReport: InProgress\n",
      "......\n",
      "2023-04-03 14:46:43 Downloading - Downloading input data\n",
      "2023-04-03 14:46:43 Training - Downloading the training image..................\n",
      "2023-04-03 14:49:38 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-04-03 14:50:24,783 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-04-03 14:50:24,819 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-03 14:50:24,828 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-04-03 14:50:24,831 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-04-03 14:50:25,043 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting pytorch-forecasting==0.10.3\u001b[0m\n",
      "\u001b[34mDownloading pytorch_forecasting-0.10.3-py3-none-any.whl (141 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.4/141.4 kB 7.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning==1.6.3\u001b[0m\n",
      "\u001b[34mDownloading pytorch_lightning-1.6.3-py3-none-any.whl (584 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 584.0/584.0 kB 72.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow==11.0.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (11.0.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard==2.12.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 107.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas<2.0.0,>=1.3.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.5.3)\u001b[0m\n",
      "\u001b[34mCollecting statsmodels\u001b[0m\n",
      "\u001b[34mDownloading statsmodels-0.13.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 95.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy<2.0,>=1.8 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.7.0)\u001b[0m\n",
      "\u001b[34mCollecting optuna<3.0.0,>=2.3.0\u001b[0m\n",
      "\u001b[34mDownloading optuna-2.10.1-py3-none-any.whl (308 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.2/308.2 kB 51.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn<1.2,>=0.24\u001b[0m\n",
      "\u001b[34mDownloading scikit_learn-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31.2/31.2 MB 59.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch<2.0,>=1.7 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.12.1+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (4.4.0)\u001b[0m\n",
      "\u001b[34mCollecting torchmetrics>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.2/519.2 kB 71.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (1.23.5)\u001b[0m\n",
      "\u001b[34mCollecting pyDeprecate<0.4.0,>=0.3.1\u001b[0m\n",
      "\u001b[34mDownloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (4.64.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 76.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (2.2.3)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.53.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/5.0 MB 123.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.17.1-py2.py3-none-any.whl (178 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 178.1/178.1 kB 33.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (65.6.3)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.4.3-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.9/93.9 kB 19.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 35.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (0.38.4)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 130.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp!=4.0.0a0,!=4.0.0a1\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 99.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 27.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard==2.12.0->-r requirements.txt (line 6)) (4.13.0)\u001b[0m\n",
      "\u001b[34mCollecting alembic\u001b[0m\n",
      "\u001b[34mDownloading alembic-1.10.2-py3-none-any.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.2/212.2 kB 51.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting colorlog\u001b[0m\n",
      "\u001b[34mDownloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting sqlalchemy>=1.1.0\u001b[0m\n",
      "\u001b[34mDownloading SQLAlchemy-2.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 119.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cliff\u001b[0m\n",
      "\u001b[34mDownloading cliff-4.2.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.0/81.0 kB 18.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cmaes>=0.8.2\u001b[0m\n",
      "\u001b[34mDownloading cmaes-0.9.1-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas<2.0.0,>=1.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas<2.0.0,>=1.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<1.2,>=0.24->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<1.2,>=0.24->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard==2.12.0->-r requirements.txt (line 6)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (4.38.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-resources>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (5.10.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (9.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.0.9)\u001b[0m\n",
      "\u001b[34mCollecting patsy>=0.5.2\u001b[0m\n",
      "\u001b[34mDownloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.8/233.8 kB 42.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (22.2.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 31.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 262.1/262.1 kB 48.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.3/161.3 kB 39.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.12.0->-r requirements.txt (line 6)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 38.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.8/site-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2.0.2)\u001b[0m\n",
      "\u001b[34mCollecting Mako\u001b[0m\n",
      "\u001b[34mDownloading Mako-1.2.4-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 15.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting PrettyTable>=0.7.2\u001b[0m\n",
      "\u001b[34mDownloading prettytable-3.6.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting autopage>=0.4.0\u001b[0m\n",
      "\u001b[34mDownloading autopage-0.5.1-py3-none-any.whl (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting cmd2>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading cmd2-2.4.3-py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.2/147.2 kB 35.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting stevedore>=2.0.1\u001b[0m\n",
      "\u001b[34mDownloading stevedore-5.0.0-py3-none-any.whl (49 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.6/49.6 kB 15.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pyperclip>=1.6\u001b[0m\n",
      "\u001b[34mDownloading pyperclip-1.8.2.tar.gz (20 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (0.2.6)\u001b[0m\n",
      "\u001b[34mCollecting pbr!=2.1.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112.7/112.7 kB 33.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: pyperclip\u001b[0m\n",
      "\u001b[34mBuilding wheel for pyperclip (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for pyperclip (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11124 sha256=759fefd405507bd7ac310436a4487381d26a6c624d07064433dcef1c5fa39c4c\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7f/1a/65/84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\u001b[0m\n",
      "\u001b[34mSuccessfully built pyperclip\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tensorboard-plugin-wit, pyperclip, tensorboard-data-server, sqlalchemy, pyDeprecate, pyasn1-modules, PrettyTable, pbr, patsy, oauthlib, multidict, Mako, grpcio, frozenlist, colorlog, cmd2, cmaes, cachetools, autopage, async-timeout, absl-py, yarl, torchmetrics, stevedore, scikit-learn, requests-oauthlib, markdown, google-auth, alembic, aiosignal, statsmodels, google-auth-oauthlib, cliff, aiohttp, tensorboard, optuna, pytorch-lightning, pytorch-forecasting\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scikit-learn\u001b[0m\n",
      "\u001b[34mFound existing installation: scikit-learn 1.2.1\u001b[0m\n",
      "\u001b[34mUninstalling scikit-learn-1.2.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scikit-learn-1.2.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed Mako-1.2.4 PrettyTable-3.6.0 absl-py-1.4.0 aiohttp-3.8.4 aiosignal-1.3.1 alembic-1.10.2 async-timeout-4.0.2 autopage-0.5.1 cachetools-5.3.0 cliff-4.2.0 cmaes-0.9.1 cmd2-2.4.3 colorlog-6.7.0 frozenlist-1.3.3 google-auth-2.17.1 google-auth-oauthlib-0.4.6 grpcio-1.53.0 markdown-3.4.3 multidict-6.0.4 oauthlib-3.2.2 optuna-2.10.1 patsy-0.5.3 pbr-5.11.1 pyDeprecate-0.3.2 pyasn1-modules-0.2.8 pyperclip-1.8.2 pytorch-forecasting-0.10.3 pytorch-lightning-1.6.3 requests-oauthlib-1.3.1 scikit-learn-1.1.3 sqlalchemy-2.0.8 statsmodels-0.13.5 stevedore-5.0.0 tensorboard-2.12.0 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 torchmetrics-0.11.4 yarl-1.8.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-04-03 14:50:40,842 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-04-03 14:50:40,842 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-04-03 14:50:40,882 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-03 14:50:40,929 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-03 14:50:40,978 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-03 14:50:40,988 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 500\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-04-03-14-45-07-300\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-04-03-14-45-07-300/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"TFT_Train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"TFT_Train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":500}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=TFT_Train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=TFT_Train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-04-03-14-45-07-300/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":500},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-04-03-14-45-07-300\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-04-03-14-45-07-300/source/sourcedir.tar.gz\",\"module_name\":\"TFT_Train\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"TFT_Train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"500\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=500\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/flash_attn-0.1-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/einops-0.6.0-py3.8.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 TFT_Train.py --epochs 500\u001b[0m\n",
      "\u001b[34m2023-04-03 14:50:43,394 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mNot running on notebook\u001b[0m\n",
      "\u001b[34m***** Arguments *****\u001b[0m\n",
      "\u001b[34mepochs=500\u001b[0m\n",
      "\u001b[34mseed=100\u001b[0m\n",
      "\u001b[34mtrain_batch_size=64\u001b[0m\n",
      "\u001b[34mmodel_dir=/opt/ml/model\u001b[0m\n",
      "\u001b[34mn_gpus=4\u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mHPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[34mNumber of parameters in network: 29.7k\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed_backend=nccl\u001b[0m\n",
      "\u001b[34mAll distributed processes registered. Starting with 4 processes\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mMissing logger folder: lightning_logs/lightning_logs\u001b[0m\n",
      "\u001b[34mMissing logger folder: lightning_logs/lightning_logs\u001b[0m\n",
      "\u001b[34mMissing logger folder: lightning_logs/lightning_logs\u001b[0m\n",
      "\u001b[34mMissing logger folder: lightning_logs/lightning_logs\u001b[0m\n",
      "\u001b[34mNCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\u001b[0m\n",
      "\u001b[34m| Name                               | Type                            | Params\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m0  | loss                               | QuantileLoss                    | 0     \u001b[0m\n",
      "\u001b[34m1  | logging_metrics                    | ModuleList                      | 0     \u001b[0m\n",
      "\u001b[34m2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \u001b[0m\n",
      "\u001b[34m3  | prescalers                         | ModuleDict                      | 256   \u001b[0m\n",
      "\u001b[34m4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K \u001b[0m\n",
      "\u001b[34m5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K \u001b[0m\n",
      "\u001b[34m6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K \u001b[0m\n",
      "\u001b[34m7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m11 | lstm_encoder                       | LSTM                            | 2.2 K \u001b[0m\n",
      "\u001b[34m12 | lstm_decoder                       | LSTM                            | 2.2 K \u001b[0m\n",
      "\u001b[34m13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \u001b[0m\n",
      "\u001b[34m14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \u001b[0m\n",
      "\u001b[34m15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \u001b[0m\n",
      "\u001b[34m16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \u001b[0m\n",
      "\u001b[34m17 | post_attn_gate_norm                | GateAddNorm                     | 576   \u001b[0m\n",
      "\u001b[34m18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m19 | pre_output_gate_norm               | GateAddNorm                     | 576   \u001b[0m\n",
      "\u001b[34m20 | output_layer                       | Linear                          | 119   \u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m29.7 K    Trainable params\u001b[0m\n",
      "\u001b[34m0         Non-trainable params\u001b[0m\n",
      "\u001b[34m29.7 K    Total params\u001b[0m\n",
      "\u001b[34m0.119     Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34mSanity Checking: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.123 algo-1:302 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.124 algo-1:420 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.124 algo-1:245 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.125 algo-1:361 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.253 algo-1:302 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.254 algo-1:245 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.254 algo-1:420 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.255 algo-1:302 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.255 algo-1:302 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.255 algo-1:245 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.255 algo-1:420 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.255 algo-1:245 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.255 algo-1:302 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.256 algo-1:302 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.256 algo-1:420 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.256 algo-1:245 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.256 algo-1:245 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.256 algo-1:420 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.256 algo-1:420 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.256 algo-1:361 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.258 algo-1:361 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.258 algo-1:361 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.259 algo-1:361 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-04-03 14:51:06.259 algo-1:361 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mSanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]#015Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mSanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]#015Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\u001b[0m\n",
      "\u001b[34mTraining: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34mEpoch 0:   2%|▏         | 1/41 [00:00<00:13,  3.02it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   2%|▏         | 1/41 [00:00<00:13,  3.02it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   2%|▏         | 1/41 [00:00<00:13,  3.01it/s, loss=399, v_num=0, train_loss_step=399.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   5%|▍         | 2/41 [00:00<00:09,  4.00it/s, loss=399, v_num=0, train_loss_step=399.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   5%|▍         | 2/41 [00:00<00:09,  4.00it/s, loss=399, v_num=0, train_loss_step=399.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   5%|▍         | 2/41 [00:00<00:09,  4.00it/s, loss=402, v_num=0, train_loss_step=406.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   7%|▋         | 3/41 [00:00<00:08,  4.49it/s, loss=402, v_num=0, train_loss_step=406.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   7%|▋         | 3/41 [00:00<00:08,  4.49it/s, loss=402, v_num=0, train_loss_step=406.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   7%|▋         | 3/41 [00:00<00:08,  4.49it/s, loss=413, v_num=0, train_loss_step=433.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  10%|▉         | 4/41 [00:00<00:07,  4.78it/s, loss=413, v_num=0, train_loss_step=433.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  10%|▉         | 4/41 [00:00<00:07,  4.78it/s, loss=413, v_num=0, train_loss_step=433.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  10%|▉         | 4/41 [00:00<00:07,  4.78it/s, loss=381, v_num=0, train_loss_step=287.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  12%|█▏        | 5/41 [00:01<00:07,  4.97it/s, loss=381, v_num=0, train_loss_step=287.0]#015Epoch 0:  12%|█▏        | 5/41 [00:01<00:07,  4.97it/s, loss=381, v_num=0, train_loss_step=287.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  12%|█▏        | 5/41 [00:01<00:07,  4.97it/s, loss=395, v_num=0, train_loss_step=451.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  15%|█▍        | 6/41 [00:01<00:06,  5.04it/s, loss=395, v_num=0, train_loss_step=451.0]#015Epoch 0:  15%|█▍        | 6/41 [00:01<00:06,  5.04it/s, loss=395, v_num=0, train_loss_step=451.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  15%|█▍        | 6/41 [00:01<00:06,  5.04it/s, loss=391, v_num=0, train_loss_step=368.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  17%|█▋        | 7/41 [00:01<00:06,  5.13it/s, loss=391, v_num=0, train_loss_step=368.0]#015Epoch 0:  17%|█▋        | 7/41 [00:01<00:06,  5.13it/s, loss=391, v_num=0, train_loss_step=368.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  17%|█▋        | 7/41 [00:01<00:06,  5.13it/s, loss=378, v_num=0, train_loss_step=304.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  20%|█▉        | 8/41 [00:01<00:06,  5.19it/s, loss=378, v_num=0, train_loss_step=304.0]#015Epoch 0:  20%|█▉        | 8/41 [00:01<00:06,  5.19it/s, loss=378, v_num=0, train_loss_step=304.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  20%|█▉        | 8/41 [00:01<00:06,  5.19it/s, loss=374, v_num=0, train_loss_step=344.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  22%|██▏       | 9/41 [00:01<00:06,  5.24it/s, loss=374, v_num=0, train_loss_step=344.0]#015Epoch 0:  22%|██▏       | 9/41 [00:01<00:06,  5.24it/s, loss=374, v_num=0, train_loss_step=344.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  22%|██▏       | 9/41 [00:01<00:06,  5.24it/s, loss=379, v_num=0, train_loss_step=422.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  24%|██▍       | 10/41 [00:01<00:05,  5.28it/s, loss=379, v_num=0, train_loss_step=422.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  24%|██▍       | 10/41 [00:01<00:05,  5.28it/s, loss=379, v_num=0, train_loss_step=422.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  24%|██▍       | 10/41 [00:01<00:05,  5.28it/s, loss=372, v_num=0, train_loss_step=307.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  27%|██▋       | 11/41 [00:02<00:05,  5.32it/s, loss=372, v_num=0, train_loss_step=307.0]#015Epoch 0:  27%|██▋       | 11/41 [00:02<00:05,  5.32it/s, loss=372, v_num=0, train_loss_step=307.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  27%|██▋       | 11/41 [00:02<00:05,  5.32it/s, loss=367, v_num=0, train_loss_step=312.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  29%|██▉       | 12/41 [00:02<00:05,  5.32it/s, loss=367, v_num=0, train_loss_step=312.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  29%|██▉       | 12/41 [00:02<00:05,  5.32it/s, loss=367, v_num=0, train_loss_step=312.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  29%|██▉       | 12/41 [00:02<00:05,  5.32it/s, loss=353, v_num=0, train_loss_step=204.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  32%|███▏      | 13/41 [00:02<00:05,  5.36it/s, loss=353, v_num=0, train_loss_step=204.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  32%|███▏      | 13/41 [00:02<00:05,  5.36it/s, loss=353, v_num=0, train_loss_step=204.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  32%|███▏      | 13/41 [00:02<00:05,  5.35it/s, loss=347, v_num=0, train_loss_step=274.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  34%|███▍      | 14/41 [00:02<00:05,  5.38it/s, loss=347, v_num=0, train_loss_step=274.0]#015Epoch 0:  34%|███▍      | 14/41 [00:02<00:05,  5.38it/s, loss=347, v_num=0, train_loss_step=274.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  34%|███▍      | 14/41 [00:02<00:05,  5.38it/s, loss=344, v_num=0, train_loss_step=303.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  37%|███▋      | 15/41 [00:02<00:04,  5.40it/s, loss=344, v_num=0, train_loss_step=303.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  37%|███▋      | 15/41 [00:02<00:04,  5.40it/s, loss=344, v_num=0, train_loss_step=303.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  37%|███▋      | 15/41 [00:02<00:04,  5.40it/s, loss=335, v_num=0, train_loss_step=212.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  39%|███▉      | 16/41 [00:02<00:04,  5.42it/s, loss=335, v_num=0, train_loss_step=212.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  39%|███▉      | 16/41 [00:02<00:04,  5.42it/s, loss=335, v_num=0, train_loss_step=212.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  39%|███▉      | 16/41 [00:02<00:04,  5.42it/s, loss=329, v_num=0, train_loss_step=243.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  41%|████▏     | 17/41 [00:03<00:04,  5.43it/s, loss=329, v_num=0, train_loss_step=243.0]#015Epoch 0:  41%|████▏     | 17/41 [00:03<00:04,  5.43it/s, loss=329, v_num=0, train_loss_step=243.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  41%|████▏     | 17/41 [00:03<00:04,  5.43it/s, loss=327, v_num=0, train_loss_step=298.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  44%|████▍     | 18/41 [00:03<00:04,  5.41it/s, loss=327, v_num=0, train_loss_step=298.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  44%|████▍     | 18/41 [00:03<00:04,  5.41it/s, loss=327, v_num=0, train_loss_step=298.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  44%|████▍     | 18/41 [00:03<00:04,  5.41it/s, loss=324, v_num=0, train_loss_step=274.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  46%|████▋     | 19/41 [00:03<00:04,  5.39it/s, loss=324, v_num=0, train_loss_step=274.0]#015Epoch 0:  46%|████▋     | 19/41 [00:03<00:04,  5.39it/s, loss=324, v_num=0, train_loss_step=274.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  46%|████▋     | 19/41 [00:03<00:04,  5.39it/s, loss=320, v_num=0, train_loss_step=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  49%|████▉     | 20/41 [00:03<00:03,  5.37it/s, loss=320, v_num=0, train_loss_step=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  49%|████▉     | 20/41 [00:03<00:03,  5.37it/s, loss=320, v_num=0, train_loss_step=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  49%|████▉     | 20/41 [00:03<00:03,  5.37it/s, loss=314, v_num=0, train_loss_step=207.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  51%|█████     | 21/41 [00:03<00:03,  5.33it/s, loss=314, v_num=0, train_loss_step=207.0]#015Epoch 0:  51%|█████     | 21/41 [00:03<00:03,  5.33it/s, loss=314, v_num=0, train_loss_step=207.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  51%|█████     | 21/41 [00:03<00:03,  5.33it/s, loss=304, v_num=0, train_loss_step=207.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  54%|█████▎    | 22/41 [00:04<00:03,  5.31it/s, loss=304, v_num=0, train_loss_step=207.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  54%|█████▎    | 22/41 [00:04<00:03,  5.31it/s, loss=304, v_num=0, train_loss_step=207.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  54%|█████▎    | 22/41 [00:04<00:03,  5.31it/s, loss=295, v_num=0, train_loss_step=211.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  56%|█████▌    | 23/41 [00:04<00:03,  5.29it/s, loss=295, v_num=0, train_loss_step=211.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  56%|█████▌    | 23/41 [00:04<00:03,  5.29it/s, loss=295, v_num=0, train_loss_step=211.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  56%|█████▌    | 23/41 [00:04<00:03,  5.29it/s, loss=285, v_num=0, train_loss_step=237.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  59%|█████▊    | 24/41 [00:04<00:03,  5.26it/s, loss=285, v_num=0, train_loss_step=237.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  59%|█████▊    | 24/41 [00:04<00:03,  5.26it/s, loss=285, v_num=0, train_loss_step=237.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  59%|█████▊    | 24/41 [00:04<00:03,  5.26it/s, loss=282, v_num=0, train_loss_step=239.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  61%|██████    | 25/41 [00:04<00:03,  5.28it/s, loss=282, v_num=0, train_loss_step=239.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  61%|██████    | 25/41 [00:04<00:03,  5.28it/s, loss=282, v_num=0, train_loss_step=239.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  61%|██████    | 25/41 [00:04<00:03,  5.28it/s, loss=273, v_num=0, train_loss_step=265.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  63%|██████▎   | 26/41 [00:04<00:02,  5.29it/s, loss=273, v_num=0, train_loss_step=265.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  63%|██████▎   | 26/41 [00:04<00:02,  5.29it/s, loss=273, v_num=0, train_loss_step=265.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  63%|██████▎   | 26/41 [00:04<00:02,  5.29it/s, loss=265, v_num=0, train_loss_step=200.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  66%|██████▌   | 27/41 [00:05<00:02,  5.28it/s, loss=265, v_num=0, train_loss_step=200.0]#015Epoch 0:  66%|██████▌   | 27/41 [00:05<00:02,  5.28it/s, loss=265, v_num=0, train_loss_step=200.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  66%|██████▌   | 27/41 [00:05<00:02,  5.28it/s, loss=260, v_num=0, train_loss_step=210.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  68%|██████▊   | 28/41 [00:05<00:02,  5.27it/s, loss=260, v_num=0, train_loss_step=210.0]#015Epoch 0:  68%|██████▊   | 28/41 [00:05<00:02,  5.27it/s, loss=260, v_num=0, train_loss_step=210.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  68%|██████▊   | 28/41 [00:05<00:02,  5.27it/s, loss=253, v_num=0, train_loss_step=212.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  71%|███████   | 29/41 [00:05<00:02,  5.27it/s, loss=253, v_num=0, train_loss_step=212.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  71%|███████   | 29/41 [00:05<00:02,  5.27it/s, loss=253, v_num=0, train_loss_step=212.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  71%|███████   | 29/41 [00:05<00:02,  5.27it/s, loss=241, v_num=0, train_loss_step=178.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  73%|███████▎  | 30/41 [00:05<00:02,  5.27it/s, loss=241, v_num=0, train_loss_step=178.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  73%|███████▎  | 30/41 [00:05<00:02,  5.27it/s, loss=241, v_num=0, train_loss_step=178.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  73%|███████▎  | 30/41 [00:05<00:02,  5.27it/s, loss=233, v_num=0, train_loss_step=150.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  76%|███████▌  | 31/41 [00:05<00:01,  5.27it/s, loss=233, v_num=0, train_loss_step=150.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  76%|███████▌  | 31/41 [00:05<00:01,  5.27it/s, loss=233, v_num=0, train_loss_step=150.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  76%|███████▌  | 31/41 [00:05<00:01,  5.27it/s, loss=228, v_num=0, train_loss_step=205.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  78%|███████▊  | 32/41 [00:06<00:01,  5.27it/s, loss=228, v_num=0, train_loss_step=205.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  78%|███████▊  | 32/41 [00:06<00:01,  5.27it/s, loss=228, v_num=0, train_loss_step=205.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  78%|███████▊  | 32/41 [00:06<00:01,  5.27it/s, loss=226, v_num=0, train_loss_step=172.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  80%|████████  | 33/41 [00:06<00:01,  5.27it/s, loss=226, v_num=0, train_loss_step=172.0]#015Epoch 0:  80%|████████  | 33/41 [00:06<00:01,  5.27it/s, loss=226, v_num=0, train_loss_step=172.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  80%|████████  | 33/41 [00:06<00:01,  5.27it/s, loss=222, v_num=0, train_loss_step=180.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  83%|████████▎ | 34/41 [00:06<00:01,  5.27it/s, loss=222, v_num=0, train_loss_step=180.0]#015Epoch 0:  83%|████████▎ | 34/41 [00:06<00:01,  5.27it/s, loss=222, v_num=0, train_loss_step=180.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  83%|████████▎ | 34/41 [00:06<00:01,  5.27it/s, loss=214, v_num=0, train_loss_step=149.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  85%|████████▌ | 35/41 [00:06<00:01,  5.26it/s, loss=214, v_num=0, train_loss_step=149.0]#015Epoch 0:  85%|████████▌ | 35/41 [00:06<00:01,  5.26it/s, loss=214, v_num=0, train_loss_step=149.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  85%|████████▌ | 35/41 [00:06<00:01,  5.26it/s, loss=211, v_num=0, train_loss_step=163.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  88%|████████▊ | 36/41 [00:06<00:00,  5.26it/s, loss=211, v_num=0, train_loss_step=163.0]#015Epoch 0:  88%|████████▊ | 36/41 [00:06<00:00,  5.26it/s, loss=211, v_num=0, train_loss_step=163.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  88%|████████▊ | 36/41 [00:06<00:00,  5.26it/s, loss=209, v_num=0, train_loss_step=186.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  90%|█████████ | 37/41 [00:07<00:00,  5.26it/s, loss=209, v_num=0, train_loss_step=186.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  90%|█████████ | 37/41 [00:07<00:00,  5.26it/s, loss=209, v_num=0, train_loss_step=186.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  90%|█████████ | 37/41 [00:07<00:00,  5.25it/s, loss=200, v_num=0, train_loss_step=132.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  93%|█████████▎| 38/41 [00:07<00:00,  5.25it/s, loss=200, v_num=0, train_loss_step=132.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  93%|█████████▎| 38/41 [00:07<00:00,  5.25it/s, loss=200, v_num=0, train_loss_step=132.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  93%|█████████▎| 38/41 [00:07<00:00,  5.25it/s, loss=196, v_num=0, train_loss_step=185.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  95%|█████████▌| 39/41 [00:07<00:00,  5.25it/s, loss=196, v_num=0, train_loss_step=185.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  95%|█████████▌| 39/41 [00:07<00:00,  5.25it/s, loss=196, v_num=0, train_loss_step=185.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  95%|█████████▌| 39/41 [00:07<00:00,  5.25it/s, loss=194, v_num=0, train_loss_step=184.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  98%|█████████▊| 40/41 [00:07<00:00,  5.25it/s, loss=194, v_num=0, train_loss_step=184.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  98%|█████████▊| 40/41 [00:07<00:00,  5.25it/s, loss=194, v_num=0, train_loss_step=184.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  98%|█████████▊| 40/41 [00:07<00:00,  5.25it/s, loss=190, v_num=0, train_loss_step=147.0]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.49it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.49it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 41/41 [00:07<00:00,  5.18it/s, loss=190, v_num=0, train_loss_step=147.0]\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 41/41 [00:07<00:00,  5.18it/s, loss=190, v_num=0, train_loss_step=147.0]\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 41/41 [00:08<00:00,  4.85it/s, loss=190, v_num=0, train_loss_step=147.0, val_loss=231.0]\u001b[0m\n",
      "\u001b[34m#015                                                                      #033[A\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 41/41 [00:08<00:00,  4.84it/s, loss=190, v_num=0, train_loss_step=147.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   0%|          | 0/41 [00:00<?, ?it/s, loss=190, v_num=0, train_loss_step=147.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   0%|          | 0/41 [00:00<?, ?it/s, loss=190, v_num=0, train_loss_step=147.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   2%|▏         | 1/41 [00:00<00:06,  5.82it/s, loss=190, v_num=0, train_loss_step=147.0, val_loss=231.0, train_loss_epoch=250.0]#015Epoch 1:   2%|▏         | 1/41 [00:00<00:06,  5.82it/s, loss=190, v_num=0, train_loss_step=147.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   2%|▏         | 1/41 [00:00<00:06,  5.80it/s, loss=187, v_num=0, train_loss_step=136.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   5%|▍         | 2/41 [00:00<00:07,  5.55it/s, loss=187, v_num=0, train_loss_step=136.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   5%|▍         | 2/41 [00:00<00:07,  5.55it/s, loss=187, v_num=0, train_loss_step=136.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   5%|▍         | 2/41 [00:00<00:07,  5.54it/s, loss=182, v_num=0, train_loss_step=114.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   7%|▋         | 3/41 [00:00<00:06,  5.53it/s, loss=182, v_num=0, train_loss_step=114.0, val_loss=231.0, train_loss_epoch=250.0]#015Epoch 1:   7%|▋         | 3/41 [00:00<00:06,  5.52it/s, loss=182, v_num=0, train_loss_step=114.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   7%|▋         | 3/41 [00:00<00:06,  5.52it/s, loss=178, v_num=0, train_loss_step=159.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  10%|▉         | 4/41 [00:00<00:06,  5.46it/s, loss=178, v_num=0, train_loss_step=159.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  10%|▉         | 4/41 [00:00<00:06,  5.46it/s, loss=178, v_num=0, train_loss_step=159.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  10%|▉         | 4/41 [00:00<00:06,  5.46it/s, loss=174, v_num=0, train_loss_step=163.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  12%|█▏        | 5/41 [00:00<00:06,  5.41it/s, loss=174, v_num=0, train_loss_step=163.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  12%|█▏        | 5/41 [00:00<00:06,  5.41it/s, loss=174, v_num=0, train_loss_step=163.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  12%|█▏        | 5/41 [00:00<00:06,  5.40it/s, loss=172, v_num=0, train_loss_step=211.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  15%|█▍        | 6/41 [00:01<00:06,  5.37it/s, loss=172, v_num=0, train_loss_step=211.0, val_loss=231.0, train_loss_epoch=250.0]#015Epoch 1:  15%|█▍        | 6/41 [00:01<00:06,  5.37it/s, loss=172, v_num=0, train_loss_step=211.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  15%|█▍        | 6/41 [00:01<00:06,  5.37it/s, loss=168, v_num=0, train_loss_step=133.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 7/41 [00:01<00:06,  5.34it/s, loss=168, v_num=0, train_loss_step=133.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 7/41 [00:01<00:06,  5.34it/s, loss=168, v_num=0, train_loss_step=133.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 7/41 [00:01<00:06,  5.34it/s, loss=164, v_num=0, train_loss_step=115.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  20%|█▉        | 8/41 [00:01<00:06,  5.27it/s, loss=164, v_num=0, train_loss_step=115.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  20%|█▉        | 8/41 [00:01<00:06,  5.27it/s, loss=164, v_num=0, train_loss_step=115.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  20%|█▉        | 8/41 [00:01<00:06,  5.27it/s, loss=160, v_num=0, train_loss_step=137.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  22%|██▏       | 9/41 [00:01<00:06,  5.27it/s, loss=160, v_num=0, train_loss_step=137.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  22%|██▏       | 9/41 [00:01<00:06,  5.27it/s, loss=160, v_num=0, train_loss_step=137.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  22%|██▏       | 9/41 [00:01<00:06,  5.27it/s, loss=159, v_num=0, train_loss_step=165.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  24%|██▍       | 10/41 [00:01<00:05,  5.26it/s, loss=159, v_num=0, train_loss_step=165.0, val_loss=231.0, train_loss_epoch=250.0]#015Epoch 1:  24%|██▍       | 10/41 [00:01<00:05,  5.26it/s, loss=159, v_num=0, train_loss_step=165.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  24%|██▍       | 10/41 [00:01<00:05,  5.26it/s, loss=158, v_num=0, train_loss_step=127.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  27%|██▋       | 11/41 [00:02<00:05,  5.26it/s, loss=158, v_num=0, train_loss_step=127.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  27%|██▋       | 11/41 [00:02<00:05,  5.26it/s, loss=158, v_num=0, train_loss_step=127.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  27%|██▋       | 11/41 [00:02<00:05,  5.26it/s, loss=157, v_num=0, train_loss_step=175.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  29%|██▉       | 12/41 [00:02<00:05,  5.25it/s, loss=157, v_num=0, train_loss_step=175.0, val_loss=231.0, train_loss_epoch=250.0]#015Epoch 1:  29%|██▉       | 12/41 [00:02<00:05,  5.25it/s, loss=157, v_num=0, train_loss_step=175.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  29%|██▉       | 12/41 [00:02<00:05,  5.25it/s, loss=157, v_num=0, train_loss_step=183.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  32%|███▏      | 13/41 [00:02<00:05,  5.27it/s, loss=157, v_num=0, train_loss_step=183.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  32%|███▏      | 13/41 [00:02<00:05,  5.27it/s, loss=157, v_num=0, train_loss_step=183.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  32%|███▏      | 13/41 [00:02<00:05,  5.27it/s, loss=157, v_num=0, train_loss_step=171.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  34%|███▍      | 14/41 [00:02<00:05,  5.27it/s, loss=157, v_num=0, train_loss_step=171.0, val_loss=231.0, train_loss_epoch=250.0]#015Epoch 1:  34%|███▍      | 14/41 [00:02<00:05,  5.27it/s, loss=157, v_num=0, train_loss_step=171.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  34%|███▍      | 14/41 [00:02<00:05,  5.27it/s, loss=156, v_num=0, train_loss_step=131.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  37%|███▋      | 15/41 [00:02<00:04,  5.30it/s, loss=156, v_num=0, train_loss_step=131.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  37%|███▋      | 15/41 [00:02<00:04,  5.30it/s, loss=156, v_num=0, train_loss_step=131.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  37%|███▋      | 15/41 [00:02<00:04,  5.30it/s, loss=153, v_num=0, train_loss_step=108.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  39%|███▉      | 16/41 [00:03<00:04,  5.32it/s, loss=153, v_num=0, train_loss_step=108.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  39%|███▉      | 16/41 [00:03<00:04,  5.32it/s, loss=153, v_num=0, train_loss_step=108.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  39%|███▉      | 16/41 [00:03<00:04,  5.32it/s, loss=152, v_num=0, train_loss_step=161.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  41%|████▏     | 17/41 [00:03<00:04,  5.34it/s, loss=152, v_num=0, train_loss_step=161.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  41%|████▏     | 17/41 [00:03<00:04,  5.34it/s, loss=152, v_num=0, train_loss_step=161.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  41%|████▏     | 17/41 [00:03<00:04,  5.34it/s, loss=153, v_num=0, train_loss_step=149.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  44%|████▍     | 18/41 [00:03<00:04,  5.36it/s, loss=153, v_num=0, train_loss_step=149.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  44%|████▍     | 18/41 [00:03<00:04,  5.36it/s, loss=153, v_num=0, train_loss_step=149.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  44%|████▍     | 18/41 [00:03<00:04,  5.36it/s, loss=151, v_num=0, train_loss_step=144.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  46%|████▋     | 19/41 [00:03<00:04,  5.37it/s, loss=151, v_num=0, train_loss_step=144.0, val_loss=231.0, train_loss_epoch=250.0]#015Epoch 1:  46%|████▋     | 19/41 [00:03<00:04,  5.37it/s, loss=151, v_num=0, train_loss_step=144.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  46%|████▋     | 19/41 [00:03<00:04,  5.37it/s, loss=146, v_num=0, train_loss_step=83.80, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  49%|████▉     | 20/41 [00:03<00:03,  5.37it/s, loss=146, v_num=0, train_loss_step=83.80, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  49%|████▉     | 20/41 [00:03<00:03,  5.37it/s, loss=146, v_num=0, train_loss_step=83.80, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  49%|████▉     | 20/41 [00:03<00:03,  5.37it/s, loss=144, v_num=0, train_loss_step=118.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  51%|█████     | 21/41 [00:03<00:03,  5.39it/s, loss=144, v_num=0, train_loss_step=118.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  51%|█████     | 21/41 [00:03<00:03,  5.39it/s, loss=144, v_num=0, train_loss_step=118.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  51%|█████     | 21/41 [00:03<00:03,  5.39it/s, loss=143, v_num=0, train_loss_step=117.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  54%|█████▎    | 22/41 [00:04<00:03,  5.40it/s, loss=143, v_num=0, train_loss_step=117.0, val_loss=231.0, train_loss_epoch=250.0]#015Epoch 1:  54%|█████▎    | 22/41 [00:04<00:03,  5.40it/s, loss=143, v_num=0, train_loss_step=117.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  54%|█████▎    | 22/41 [00:04<00:03,  5.40it/s, loss=146, v_num=0, train_loss_step=164.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  56%|█████▌    | 23/41 [00:04<00:03,  5.42it/s, loss=146, v_num=0, train_loss_step=164.0, val_loss=231.0, train_loss_epoch=250.0]#015Epoch 1:  56%|█████▌    | 23/41 [00:04<00:03,  5.42it/s, loss=146, v_num=0, train_loss_step=164.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  56%|█████▌    | 23/41 [00:04<00:03,  5.42it/s, loss=145, v_num=0, train_loss_step=138.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  59%|█████▊    | 24/41 [00:04<00:03,  5.43it/s, loss=145, v_num=0, train_loss_step=138.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  59%|█████▊    | 24/41 [00:04<00:03,  5.43it/s, loss=145, v_num=0, train_loss_step=138.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  59%|█████▊    | 24/41 [00:04<00:03,  5.43it/s, loss=143, v_num=0, train_loss_step=127.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  61%|██████    | 25/41 [00:04<00:02,  5.44it/s, loss=143, v_num=0, train_loss_step=127.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  61%|██████    | 25/41 [00:04<00:02,  5.44it/s, loss=143, v_num=0, train_loss_step=127.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  61%|██████    | 25/41 [00:04<00:02,  5.44it/s, loss=143, v_num=0, train_loss_step=217.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  63%|██████▎   | 26/41 [00:04<00:02,  5.44it/s, loss=143, v_num=0, train_loss_step=217.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  63%|██████▎   | 26/41 [00:04<00:02,  5.44it/s, loss=143, v_num=0, train_loss_step=217.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  63%|██████▎   | 26/41 [00:04<00:02,  5.44it/s, loss=144, v_num=0, train_loss_step=142.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  66%|██████▌   | 27/41 [00:04<00:02,  5.45it/s, loss=144, v_num=0, train_loss_step=142.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  66%|██████▌   | 27/41 [00:04<00:02,  5.45it/s, loss=144, v_num=0, train_loss_step=142.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  66%|██████▌   | 27/41 [00:04<00:02,  5.45it/s, loss=145, v_num=0, train_loss_step=135.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  68%|██████▊   | 28/41 [00:05<00:02,  5.46it/s, loss=145, v_num=0, train_loss_step=135.0, val_loss=231.0, train_loss_epoch=250.0]#015Epoch 1:  68%|██████▊   | 28/41 [00:05<00:02,  5.46it/s, loss=145, v_num=0, train_loss_step=135.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  68%|██████▊   | 28/41 [00:05<00:02,  5.46it/s, loss=145, v_num=0, train_loss_step=140.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  71%|███████   | 29/41 [00:05<00:02,  5.47it/s, loss=145, v_num=0, train_loss_step=140.0, val_loss=231.0, train_loss_epoch=250.0]#015Epoch 1:  71%|███████   | 29/41 [00:05<00:02,  5.47it/s, loss=145, v_num=0, train_loss_step=140.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  71%|███████   | 29/41 [00:05<00:02,  5.47it/s, loss=145, v_num=0, train_loss_step=173.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  73%|███████▎  | 30/41 [00:05<00:02,  5.48it/s, loss=145, v_num=0, train_loss_step=173.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  73%|███████▎  | 30/41 [00:05<00:02,  5.48it/s, loss=145, v_num=0, train_loss_step=173.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  73%|███████▎  | 30/41 [00:05<00:02,  5.48it/s, loss=144, v_num=0, train_loss_step=107.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  76%|███████▌  | 31/41 [00:05<00:01,  5.49it/s, loss=144, v_num=0, train_loss_step=107.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  76%|███████▌  | 31/41 [00:05<00:01,  5.49it/s, loss=144, v_num=0, train_loss_step=107.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  76%|███████▌  | 31/41 [00:05<00:01,  5.49it/s, loss=141, v_num=0, train_loss_step=101.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  78%|███████▊  | 32/41 [00:05<00:01,  5.49it/s, loss=141, v_num=0, train_loss_step=101.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  78%|███████▊  | 32/41 [00:05<00:01,  5.49it/s, loss=141, v_num=0, train_loss_step=101.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  78%|███████▊  | 32/41 [00:05<00:01,  5.49it/s, loss=137, v_num=0, train_loss_step=117.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  80%|████████  | 33/41 [00:06<00:01,  5.49it/s, loss=137, v_num=0, train_loss_step=117.0, val_loss=231.0, train_loss_epoch=250.0]#015Epoch 1:  80%|████████  | 33/41 [00:06<00:01,  5.49it/s, loss=137, v_num=0, train_loss_step=117.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  80%|████████  | 33/41 [00:06<00:01,  5.49it/s, loss=136, v_num=0, train_loss_step=153.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  83%|████████▎ | 34/41 [00:06<00:01,  5.50it/s, loss=136, v_num=0, train_loss_step=153.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  83%|████████▎ | 34/41 [00:06<00:01,  5.50it/s, loss=136, v_num=0, train_loss_step=153.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  83%|████████▎ | 34/41 [00:06<00:01,  5.50it/s, loss=137, v_num=0, train_loss_step=134.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  85%|████████▌ | 35/41 [00:06<00:01,  5.51it/s, loss=137, v_num=0, train_loss_step=134.0, val_loss=231.0, train_loss_epoch=250.0]#015Epoch 1:  85%|████████▌ | 35/41 [00:06<00:01,  5.51it/s, loss=137, v_num=0, train_loss_step=134.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  85%|████████▌ | 35/41 [00:06<00:01,  5.51it/s, loss=141, v_num=0, train_loss_step=194.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  88%|████████▊ | 36/41 [00:06<00:00,  5.51it/s, loss=141, v_num=0, train_loss_step=194.0, val_loss=231.0, train_loss_epoch=250.0]#015Epoch 1:  88%|████████▊ | 36/41 [00:06<00:00,  5.51it/s, loss=141, v_num=0, train_loss_step=194.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  88%|████████▊ | 36/41 [00:06<00:00,  5.51it/s, loss=141, v_num=0, train_loss_step=156.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  90%|█████████ | 37/41 [00:06<00:00,  5.52it/s, loss=141, v_num=0, train_loss_step=156.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  90%|█████████ | 37/41 [00:06<00:00,  5.52it/s, loss=141, v_num=0, train_loss_step=156.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  90%|█████████ | 37/41 [00:06<00:00,  5.52it/s, loss=140, v_num=0, train_loss_step=139.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  93%|█████████▎| 38/41 [00:06<00:00,  5.52it/s, loss=140, v_num=0, train_loss_step=139.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  93%|█████████▎| 38/41 [00:06<00:00,  5.52it/s, loss=140, v_num=0, train_loss_step=139.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  93%|█████████▎| 38/41 [00:06<00:00,  5.52it/s, loss=141, v_num=0, train_loss_step=157.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  95%|█████████▌| 39/41 [00:07<00:00,  5.52it/s, loss=141, v_num=0, train_loss_step=157.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  95%|█████████▌| 39/41 [00:07<00:00,  5.52it/s, loss=141, v_num=0, train_loss_step=157.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  95%|█████████▌| 39/41 [00:07<00:00,  5.52it/s, loss=145, v_num=0, train_loss_step=174.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  98%|█████████▊| 40/41 [00:07<00:00,  5.53it/s, loss=145, v_num=0, train_loss_step=174.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  98%|█████████▊| 40/41 [00:07<00:00,  5.53it/s, loss=145, v_num=0, train_loss_step=174.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  98%|█████████▊| 40/41 [00:07<00:00,  5.53it/s, loss=144, v_num=0, train_loss_step=101.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.72it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.72it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 41/41 [00:07<00:00,  5.46it/s, loss=144, v_num=0, train_loss_step=101.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 41/41 [00:07<00:00,  5.46it/s, loss=144, v_num=0, train_loss_step=101.0, val_loss=231.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 41/41 [00:08<00:00,  4.99it/s, loss=144, v_num=0, train_loss_step=101.0, val_loss=197.0, train_loss_epoch=250.0]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 41/41 [00:08<00:00,  4.99it/s, loss=144, v_num=0, train_loss_step=101.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   0%|          | 0/41 [00:00<?, ?it/s, loss=144, v_num=0, train_loss_step=101.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   0%|          | 0/41 [00:00<?, ?it/s, loss=144, v_num=0, train_loss_step=101.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   2%|▏         | 1/41 [00:00<00:07,  5.62it/s, loss=144, v_num=0, train_loss_step=101.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   2%|▏         | 1/41 [00:00<00:07,  5.61it/s, loss=144, v_num=0, train_loss_step=101.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   2%|▏         | 1/41 [00:00<00:07,  5.60it/s, loss=144, v_num=0, train_loss_step=106.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   5%|▍         | 2/41 [00:00<00:09,  4.14it/s, loss=144, v_num=0, train_loss_step=106.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:   5%|▍         | 2/41 [00:00<00:09,  4.14it/s, loss=144, v_num=0, train_loss_step=106.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   5%|▍         | 2/41 [00:00<00:09,  4.14it/s, loss=143, v_num=0, train_loss_step=152.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   7%|▋         | 3/41 [00:00<00:08,  4.55it/s, loss=143, v_num=0, train_loss_step=152.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:   7%|▋         | 3/41 [00:00<00:08,  4.55it/s, loss=143, v_num=0, train_loss_step=152.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   7%|▋         | 3/41 [00:00<00:08,  4.55it/s, loss=143, v_num=0, train_loss_step=140.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  10%|▉         | 4/41 [00:00<00:07,  4.72it/s, loss=143, v_num=0, train_loss_step=140.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  10%|▉         | 4/41 [00:00<00:07,  4.72it/s, loss=143, v_num=0, train_loss_step=140.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  10%|▉         | 4/41 [00:00<00:07,  4.72it/s, loss=143, v_num=0, train_loss_step=116.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  12%|█▏        | 5/41 [00:01<00:07,  4.89it/s, loss=143, v_num=0, train_loss_step=116.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  12%|█▏        | 5/41 [00:01<00:07,  4.89it/s, loss=143, v_num=0, train_loss_step=116.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  12%|█▏        | 5/41 [00:01<00:07,  4.89it/s, loss=137, v_num=0, train_loss_step=107.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  15%|█▍        | 6/41 [00:01<00:07,  4.99it/s, loss=137, v_num=0, train_loss_step=107.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  15%|█▍        | 6/41 [00:01<00:07,  4.99it/s, loss=137, v_num=0, train_loss_step=107.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  15%|█▍        | 6/41 [00:01<00:07,  4.99it/s, loss=138, v_num=0, train_loss_step=162.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  17%|█▋        | 7/41 [00:01<00:06,  5.09it/s, loss=138, v_num=0, train_loss_step=162.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  17%|█▋        | 7/41 [00:01<00:06,  5.09it/s, loss=138, v_num=0, train_loss_step=162.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  17%|█▋        | 7/41 [00:01<00:06,  5.09it/s, loss=139, v_num=0, train_loss_step=144.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  20%|█▉        | 8/41 [00:01<00:06,  5.16it/s, loss=139, v_num=0, train_loss_step=144.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  20%|█▉        | 8/41 [00:01<00:06,  5.16it/s, loss=139, v_num=0, train_loss_step=144.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  20%|█▉        | 8/41 [00:01<00:06,  5.16it/s, loss=137, v_num=0, train_loss_step=114.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  22%|██▏       | 9/41 [00:01<00:06,  5.22it/s, loss=137, v_num=0, train_loss_step=114.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  22%|██▏       | 9/41 [00:01<00:06,  5.22it/s, loss=137, v_num=0, train_loss_step=114.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  22%|██▏       | 9/41 [00:01<00:06,  5.22it/s, loss=135, v_num=0, train_loss_step=131.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  24%|██▍       | 10/41 [00:01<00:05,  5.24it/s, loss=135, v_num=0, train_loss_step=131.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  24%|██▍       | 10/41 [00:01<00:05,  5.23it/s, loss=135, v_num=0, train_loss_step=131.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  24%|██▍       | 10/41 [00:01<00:05,  5.23it/s, loss=136, v_num=0, train_loss_step=121.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  27%|██▋       | 11/41 [00:02<00:05,  5.28it/s, loss=136, v_num=0, train_loss_step=121.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  27%|██▋       | 11/41 [00:02<00:05,  5.28it/s, loss=136, v_num=0, train_loss_step=121.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  27%|██▋       | 11/41 [00:02<00:05,  5.27it/s, loss=139, v_num=0, train_loss_step=153.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  29%|██▉       | 12/41 [00:02<00:05,  5.31it/s, loss=139, v_num=0, train_loss_step=153.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  29%|██▉       | 12/41 [00:02<00:05,  5.31it/s, loss=139, v_num=0, train_loss_step=153.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  29%|██▉       | 12/41 [00:02<00:05,  5.30it/s, loss=139, v_num=0, train_loss_step=129.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  32%|███▏      | 13/41 [00:02<00:05,  5.33it/s, loss=139, v_num=0, train_loss_step=129.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  32%|███▏      | 13/41 [00:02<00:05,  5.33it/s, loss=139, v_num=0, train_loss_step=129.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  32%|███▏      | 13/41 [00:02<00:05,  5.33it/s, loss=139, v_num=0, train_loss_step=144.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  34%|███▍      | 14/41 [00:02<00:05,  5.05it/s, loss=139, v_num=0, train_loss_step=144.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  34%|███▍      | 14/41 [00:02<00:05,  5.05it/s, loss=139, v_num=0, train_loss_step=144.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  34%|███▍      | 14/41 [00:02<00:05,  5.05it/s, loss=141, v_num=0, train_loss_step=183.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  37%|███▋      | 15/41 [00:02<00:05,  5.09it/s, loss=141, v_num=0, train_loss_step=183.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  37%|███▋      | 15/41 [00:02<00:05,  5.09it/s, loss=141, v_num=0, train_loss_step=183.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  37%|███▋      | 15/41 [00:02<00:05,  5.09it/s, loss=138, v_num=0, train_loss_step=131.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  39%|███▉      | 16/41 [00:03<00:04,  5.11it/s, loss=138, v_num=0, train_loss_step=131.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  39%|███▉      | 16/41 [00:03<00:04,  5.11it/s, loss=138, v_num=0, train_loss_step=131.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  39%|███▉      | 16/41 [00:03<00:04,  5.11it/s, loss=135, v_num=0, train_loss_step=97.30, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  41%|████▏     | 17/41 [00:03<00:04,  5.12it/s, loss=135, v_num=0, train_loss_step=97.30, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  41%|████▏     | 17/41 [00:03<00:04,  5.12it/s, loss=135, v_num=0, train_loss_step=97.30, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  41%|████▏     | 17/41 [00:03<00:04,  5.12it/s, loss=133, v_num=0, train_loss_step=92.50, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  44%|████▍     | 18/41 [00:03<00:04,  5.15it/s, loss=133, v_num=0, train_loss_step=92.50, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  44%|████▍     | 18/41 [00:03<00:04,  5.15it/s, loss=133, v_num=0, train_loss_step=92.50, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  44%|████▍     | 18/41 [00:03<00:04,  5.15it/s, loss=132, v_num=0, train_loss_step=135.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  46%|████▋     | 19/41 [00:03<00:04,  5.17it/s, loss=132, v_num=0, train_loss_step=135.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  46%|████▋     | 19/41 [00:03<00:04,  5.17it/s, loss=132, v_num=0, train_loss_step=135.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  46%|████▋     | 19/41 [00:03<00:04,  5.17it/s, loss=130, v_num=0, train_loss_step=134.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  49%|████▉     | 20/41 [00:03<00:04,  5.20it/s, loss=130, v_num=0, train_loss_step=134.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  49%|████▉     | 20/41 [00:03<00:04,  5.20it/s, loss=130, v_num=0, train_loss_step=134.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  49%|████▉     | 20/41 [00:03<00:04,  5.20it/s, loss=131, v_num=0, train_loss_step=128.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  51%|█████     | 21/41 [00:04<00:03,  5.22it/s, loss=131, v_num=0, train_loss_step=128.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  51%|█████     | 21/41 [00:04<00:03,  5.22it/s, loss=131, v_num=0, train_loss_step=128.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  51%|█████     | 21/41 [00:04<00:03,  5.22it/s, loss=133, v_num=0, train_loss_step=150.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  54%|█████▎    | 22/41 [00:04<00:03,  5.23it/s, loss=133, v_num=0, train_loss_step=150.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  54%|█████▎    | 22/41 [00:04<00:03,  5.23it/s, loss=133, v_num=0, train_loss_step=150.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  54%|█████▎    | 22/41 [00:04<00:03,  5.23it/s, loss=131, v_num=0, train_loss_step=113.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  56%|█████▌    | 23/41 [00:04<00:03,  5.25it/s, loss=131, v_num=0, train_loss_step=113.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  56%|█████▌    | 23/41 [00:04<00:03,  5.24it/s, loss=131, v_num=0, train_loss_step=113.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  56%|█████▌    | 23/41 [00:04<00:03,  5.24it/s, loss=131, v_num=0, train_loss_step=136.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  59%|█████▊    | 24/41 [00:04<00:03,  5.26it/s, loss=131, v_num=0, train_loss_step=136.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  59%|█████▊    | 24/41 [00:04<00:03,  5.26it/s, loss=131, v_num=0, train_loss_step=136.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  59%|█████▊    | 24/41 [00:04<00:03,  5.26it/s, loss=135, v_num=0, train_loss_step=185.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  61%|██████    | 25/41 [00:04<00:03,  5.28it/s, loss=135, v_num=0, train_loss_step=185.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  61%|██████    | 25/41 [00:04<00:03,  5.28it/s, loss=135, v_num=0, train_loss_step=185.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  61%|██████    | 25/41 [00:04<00:03,  5.28it/s, loss=133, v_num=0, train_loss_step=75.60, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  63%|██████▎   | 26/41 [00:04<00:02,  5.29it/s, loss=133, v_num=0, train_loss_step=75.60, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  63%|██████▎   | 26/41 [00:04<00:02,  5.29it/s, loss=133, v_num=0, train_loss_step=75.60, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  63%|██████▎   | 26/41 [00:04<00:02,  5.29it/s, loss=130, v_num=0, train_loss_step=111.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  66%|██████▌   | 27/41 [00:05<00:02,  5.30it/s, loss=130, v_num=0, train_loss_step=111.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  66%|██████▌   | 27/41 [00:05<00:02,  5.30it/s, loss=130, v_num=0, train_loss_step=111.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  66%|██████▌   | 27/41 [00:05<00:02,  5.30it/s, loss=129, v_num=0, train_loss_step=117.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  68%|██████▊   | 28/41 [00:05<00:02,  5.30it/s, loss=129, v_num=0, train_loss_step=117.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  68%|██████▊   | 28/41 [00:05<00:02,  5.30it/s, loss=129, v_num=0, train_loss_step=117.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  68%|██████▊   | 28/41 [00:05<00:02,  5.30it/s, loss=131, v_num=0, train_loss_step=144.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  71%|███████   | 29/41 [00:05<00:02,  5.31it/s, loss=131, v_num=0, train_loss_step=144.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  71%|███████   | 29/41 [00:05<00:02,  5.31it/s, loss=131, v_num=0, train_loss_step=144.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  71%|███████   | 29/41 [00:05<00:02,  5.31it/s, loss=131, v_num=0, train_loss_step=137.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  73%|███████▎  | 30/41 [00:05<00:02,  5.32it/s, loss=131, v_num=0, train_loss_step=137.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  73%|███████▎  | 30/41 [00:05<00:02,  5.32it/s, loss=131, v_num=0, train_loss_step=137.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  73%|███████▎  | 30/41 [00:05<00:02,  5.32it/s, loss=133, v_num=0, train_loss_step=155.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  76%|███████▌  | 31/41 [00:05<00:01,  5.34it/s, loss=133, v_num=0, train_loss_step=155.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  76%|███████▌  | 31/41 [00:05<00:01,  5.34it/s, loss=133, v_num=0, train_loss_step=155.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  76%|███████▌  | 31/41 [00:05<00:01,  5.34it/s, loss=131, v_num=0, train_loss_step=115.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  78%|███████▊  | 32/41 [00:05<00:01,  5.35it/s, loss=131, v_num=0, train_loss_step=115.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  78%|███████▊  | 32/41 [00:05<00:01,  5.35it/s, loss=131, v_num=0, train_loss_step=115.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  78%|███████▊  | 32/41 [00:05<00:01,  5.35it/s, loss=131, v_num=0, train_loss_step=126.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  80%|████████  | 33/41 [00:06<00:01,  5.36it/s, loss=131, v_num=0, train_loss_step=126.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  80%|████████  | 33/41 [00:06<00:01,  5.36it/s, loss=131, v_num=0, train_loss_step=126.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  80%|████████  | 33/41 [00:06<00:01,  5.36it/s, loss=130, v_num=0, train_loss_step=123.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  83%|████████▎ | 34/41 [00:06<00:01,  5.36it/s, loss=130, v_num=0, train_loss_step=123.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  83%|████████▎ | 34/41 [00:06<00:01,  5.36it/s, loss=130, v_num=0, train_loss_step=123.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  83%|████████▎ | 34/41 [00:06<00:01,  5.36it/s, loss=127, v_num=0, train_loss_step=133.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  85%|████████▌ | 35/41 [00:06<00:01,  5.37it/s, loss=127, v_num=0, train_loss_step=133.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  85%|████████▌ | 35/41 [00:06<00:01,  5.37it/s, loss=127, v_num=0, train_loss_step=133.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  85%|████████▌ | 35/41 [00:06<00:01,  5.37it/s, loss=128, v_num=0, train_loss_step=145.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  88%|████████▊ | 36/41 [00:06<00:00,  5.37it/s, loss=128, v_num=0, train_loss_step=145.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  88%|████████▊ | 36/41 [00:06<00:00,  5.37it/s, loss=128, v_num=0, train_loss_step=145.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  88%|████████▊ | 36/41 [00:06<00:00,  5.37it/s, loss=130, v_num=0, train_loss_step=138.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  90%|█████████ | 37/41 [00:06<00:00,  5.38it/s, loss=130, v_num=0, train_loss_step=138.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  90%|█████████ | 37/41 [00:06<00:00,  5.38it/s, loss=130, v_num=0, train_loss_step=138.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  90%|█████████ | 37/41 [00:06<00:00,  5.38it/s, loss=130, v_num=0, train_loss_step=97.20, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  93%|█████████▎| 38/41 [00:07<00:00,  5.39it/s, loss=130, v_num=0, train_loss_step=97.20, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  93%|█████████▎| 38/41 [00:07<00:00,  5.39it/s, loss=130, v_num=0, train_loss_step=97.20, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  93%|█████████▎| 38/41 [00:07<00:00,  5.39it/s, loss=130, v_num=0, train_loss_step=134.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  95%|█████████▌| 39/41 [00:07<00:00,  5.40it/s, loss=130, v_num=0, train_loss_step=134.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  95%|█████████▌| 39/41 [00:07<00:00,  5.40it/s, loss=130, v_num=0, train_loss_step=134.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  95%|█████████▌| 39/41 [00:07<00:00,  5.40it/s, loss=129, v_num=0, train_loss_step=122.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  98%|█████████▊| 40/41 [00:07<00:00,  5.40it/s, loss=129, v_num=0, train_loss_step=122.0, val_loss=197.0, train_loss_epoch=146.0]#015Epoch 2:  98%|█████████▊| 40/41 [00:07<00:00,  5.40it/s, loss=129, v_num=0, train_loss_step=122.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  98%|█████████▊| 40/41 [00:07<00:00,  5.40it/s, loss=131, v_num=0, train_loss_step=155.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2: 100%|██████████| 41/41 [00:07<00:00,  5.34it/s, loss=131, v_num=0, train_loss_step=155.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2: 100%|██████████| 41/41 [00:07<00:00,  5.34it/s, loss=131, v_num=0, train_loss_step=155.0, val_loss=197.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 2: 100%|██████████| 41/41 [00:08<00:00,  4.94it/s, loss=131, v_num=0, train_loss_step=155.0, val_loss=181.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2: 100%|██████████| 41/41 [00:08<00:00,  4.94it/s, loss=131, v_num=0, train_loss_step=155.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   0%|          | 0/41 [00:00<?, ?it/s, loss=131, v_num=0, train_loss_step=155.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:   0%|          | 0/41 [00:00<?, ?it/s, loss=131, v_num=0, train_loss_step=155.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:   2%|▏         | 1/41 [00:00<00:06,  5.88it/s, loss=131, v_num=0, train_loss_step=155.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:   2%|▏         | 1/41 [00:00<00:06,  5.87it/s, loss=131, v_num=0, train_loss_step=155.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:   2%|▏         | 1/41 [00:00<00:06,  5.86it/s, loss=130, v_num=0, train_loss_step=130.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:   5%|▍         | 2/41 [00:00<00:06,  5.82it/s, loss=130, v_num=0, train_loss_step=130.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:   5%|▍         | 2/41 [00:00<00:06,  5.82it/s, loss=130, v_num=0, train_loss_step=130.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:   5%|▍         | 2/41 [00:00<00:06,  5.81it/s, loss=131, v_num=0, train_loss_step=139.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:   7%|▋         | 3/41 [00:00<00:06,  5.82it/s, loss=131, v_num=0, train_loss_step=139.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:   7%|▋         | 3/41 [00:00<00:06,  5.81it/s, loss=131, v_num=0, train_loss_step=139.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:   7%|▋         | 3/41 [00:00<00:06,  5.81it/s, loss=130, v_num=0, train_loss_step=127.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  10%|▉         | 4/41 [00:00<00:06,  5.41it/s, loss=130, v_num=0, train_loss_step=127.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  10%|▉         | 4/41 [00:00<00:06,  5.40it/s, loss=130, v_num=0, train_loss_step=127.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  10%|▉         | 4/41 [00:00<00:06,  5.40it/s, loss=128, v_num=0, train_loss_step=145.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  12%|█▏        | 5/41 [00:00<00:06,  5.33it/s, loss=128, v_num=0, train_loss_step=145.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  12%|█▏        | 5/41 [00:00<00:06,  5.32it/s, loss=128, v_num=0, train_loss_step=145.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  12%|█▏        | 5/41 [00:00<00:06,  5.32it/s, loss=130, v_num=0, train_loss_step=106.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  15%|█▍        | 6/41 [00:01<00:06,  5.32it/s, loss=130, v_num=0, train_loss_step=106.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  15%|█▍        | 6/41 [00:01<00:06,  5.32it/s, loss=130, v_num=0, train_loss_step=106.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  15%|█▍        | 6/41 [00:01<00:06,  5.32it/s, loss=130, v_num=0, train_loss_step=120.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  17%|█▋        | 7/41 [00:01<00:06,  5.36it/s, loss=130, v_num=0, train_loss_step=120.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  17%|█▋        | 7/41 [00:01<00:06,  5.36it/s, loss=130, v_num=0, train_loss_step=120.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  17%|█▋        | 7/41 [00:01<00:06,  5.36it/s, loss=132, v_num=0, train_loss_step=145.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  20%|█▉        | 8/41 [00:01<00:06,  5.40it/s, loss=132, v_num=0, train_loss_step=145.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  20%|█▉        | 8/41 [00:01<00:06,  5.40it/s, loss=132, v_num=0, train_loss_step=145.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  20%|█▉        | 8/41 [00:01<00:06,  5.40it/s, loss=129, v_num=0, train_loss_step=95.60, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  22%|██▏       | 9/41 [00:01<00:05,  5.42it/s, loss=129, v_num=0, train_loss_step=95.60, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  22%|██▏       | 9/41 [00:01<00:05,  5.41it/s, loss=129, v_num=0, train_loss_step=95.60, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  22%|██▏       | 9/41 [00:01<00:05,  5.41it/s, loss=129, v_num=0, train_loss_step=128.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  24%|██▍       | 10/41 [00:01<00:05,  5.44it/s, loss=129, v_num=0, train_loss_step=128.0, val_loss=181.0, train_loss_epoch=133.0]#015Epoch 3:  24%|██▍       | 10/41 [00:01<00:05,  5.44it/s, loss=129, v_num=0, train_loss_step=128.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  24%|██▍       | 10/41 [00:01<00:05,  5.44it/s, loss=127, v_num=0, train_loss_step=126.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  27%|██▋       | 11/41 [00:02<00:05,  5.46it/s, loss=127, v_num=0, train_loss_step=126.0, val_loss=181.0, train_loss_epoch=133.0]#015Epoch 3:  27%|██▋       | 11/41 [00:02<00:05,  5.46it/s, loss=127, v_num=0, train_loss_step=126.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  27%|██▋       | 11/41 [00:02<00:05,  5.46it/s, loss=127, v_num=0, train_loss_step=111.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  29%|██▉       | 12/41 [00:02<00:05,  5.44it/s, loss=127, v_num=0, train_loss_step=111.0, val_loss=181.0, train_loss_epoch=133.0]#015Epoch 3:  29%|██▉       | 12/41 [00:02<00:05,  5.44it/s, loss=127, v_num=0, train_loss_step=111.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  29%|██▉       | 12/41 [00:02<00:05,  5.44it/s, loss=127, v_num=0, train_loss_step=120.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  32%|███▏      | 13/41 [00:02<00:05,  5.46it/s, loss=127, v_num=0, train_loss_step=120.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  32%|███▏      | 13/41 [00:02<00:05,  5.46it/s, loss=127, v_num=0, train_loss_step=120.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  32%|███▏      | 13/41 [00:02<00:05,  5.46it/s, loss=126, v_num=0, train_loss_step=102.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  34%|███▍      | 14/41 [00:02<00:04,  5.48it/s, loss=126, v_num=0, train_loss_step=102.0, val_loss=181.0, train_loss_epoch=133.0]#015Epoch 3:  34%|███▍      | 14/41 [00:02<00:04,  5.48it/s, loss=126, v_num=0, train_loss_step=102.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  34%|███▍      | 14/41 [00:02<00:04,  5.48it/s, loss=125, v_num=0, train_loss_step=117.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  37%|███▋      | 15/41 [00:02<00:04,  5.49it/s, loss=125, v_num=0, train_loss_step=117.0, val_loss=181.0, train_loss_epoch=133.0]#015Epoch 3:  37%|███▋      | 15/41 [00:02<00:04,  5.49it/s, loss=125, v_num=0, train_loss_step=117.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  37%|███▋      | 15/41 [00:02<00:04,  5.49it/s, loss=125, v_num=0, train_loss_step=137.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  39%|███▉      | 16/41 [00:02<00:04,  5.50it/s, loss=125, v_num=0, train_loss_step=137.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  39%|███▉      | 16/41 [00:02<00:04,  5.50it/s, loss=125, v_num=0, train_loss_step=137.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  39%|███▉      | 16/41 [00:02<00:04,  5.50it/s, loss=123, v_num=0, train_loss_step=96.40, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  41%|████▏     | 17/41 [00:03<00:04,  5.51it/s, loss=123, v_num=0, train_loss_step=96.40, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  41%|████▏     | 17/41 [00:03<00:04,  5.51it/s, loss=123, v_num=0, train_loss_step=96.40, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  41%|████▏     | 17/41 [00:03<00:04,  5.51it/s, loss=124, v_num=0, train_loss_step=129.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  44%|████▍     | 18/41 [00:03<00:04,  5.50it/s, loss=124, v_num=0, train_loss_step=129.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  44%|████▍     | 18/41 [00:03<00:04,  5.50it/s, loss=124, v_num=0, train_loss_step=129.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  44%|████▍     | 18/41 [00:03<00:04,  5.50it/s, loss=123, v_num=0, train_loss_step=118.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  46%|████▋     | 19/41 [00:03<00:03,  5.51it/s, loss=123, v_num=0, train_loss_step=118.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  46%|████▋     | 19/41 [00:03<00:03,  5.51it/s, loss=123, v_num=0, train_loss_step=118.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  46%|████▋     | 19/41 [00:03<00:03,  5.51it/s, loss=123, v_num=0, train_loss_step=122.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  49%|████▉     | 20/41 [00:03<00:03,  5.51it/s, loss=123, v_num=0, train_loss_step=122.0, val_loss=181.0, train_loss_epoch=133.0]#015Epoch 3:  49%|████▉     | 20/41 [00:03<00:03,  5.51it/s, loss=123, v_num=0, train_loss_step=122.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  49%|████▉     | 20/41 [00:03<00:03,  5.51it/s, loss=121, v_num=0, train_loss_step=102.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  51%|█████     | 21/41 [00:03<00:03,  5.52it/s, loss=121, v_num=0, train_loss_step=102.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  51%|█████     | 21/41 [00:03<00:03,  5.52it/s, loss=121, v_num=0, train_loss_step=102.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  51%|█████     | 21/41 [00:03<00:03,  5.52it/s, loss=120, v_num=0, train_loss_step=109.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  54%|█████▎    | 22/41 [00:03<00:03,  5.53it/s, loss=120, v_num=0, train_loss_step=109.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  54%|█████▎    | 22/41 [00:03<00:03,  5.53it/s, loss=120, v_num=0, train_loss_step=109.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  54%|█████▎    | 22/41 [00:03<00:03,  5.53it/s, loss=119, v_num=0, train_loss_step=127.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  56%|█████▌    | 23/41 [00:04<00:03,  5.53it/s, loss=119, v_num=0, train_loss_step=127.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  56%|█████▌    | 23/41 [00:04<00:03,  5.53it/s, loss=119, v_num=0, train_loss_step=127.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  56%|█████▌    | 23/41 [00:04<00:03,  5.53it/s, loss=120, v_num=0, train_loss_step=138.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  59%|█████▊    | 24/41 [00:04<00:03,  5.53it/s, loss=120, v_num=0, train_loss_step=138.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  59%|█████▊    | 24/41 [00:04<00:03,  5.53it/s, loss=120, v_num=0, train_loss_step=138.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  59%|█████▊    | 24/41 [00:04<00:03,  5.53it/s, loss=119, v_num=0, train_loss_step=138.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  61%|██████    | 25/41 [00:04<00:02,  5.53it/s, loss=119, v_num=0, train_loss_step=138.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  61%|██████    | 25/41 [00:04<00:02,  5.53it/s, loss=119, v_num=0, train_loss_step=138.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  61%|██████    | 25/41 [00:04<00:02,  5.53it/s, loss=122, v_num=0, train_loss_step=153.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  63%|██████▎   | 26/41 [00:04<00:02,  5.54it/s, loss=122, v_num=0, train_loss_step=153.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  63%|██████▎   | 26/41 [00:04<00:02,  5.54it/s, loss=122, v_num=0, train_loss_step=153.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  63%|██████▎   | 26/41 [00:04<00:02,  5.54it/s, loss=122, v_num=0, train_loss_step=127.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  66%|██████▌   | 27/41 [00:04<00:02,  5.55it/s, loss=122, v_num=0, train_loss_step=127.0, val_loss=181.0, train_loss_epoch=133.0]#015Epoch 3:  66%|██████▌   | 27/41 [00:04<00:02,  5.55it/s, loss=122, v_num=0, train_loss_step=127.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  66%|██████▌   | 27/41 [00:04<00:02,  5.55it/s, loss=120, v_num=0, train_loss_step=112.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  68%|██████▊   | 28/41 [00:05<00:02,  5.55it/s, loss=120, v_num=0, train_loss_step=112.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  68%|██████▊   | 28/41 [00:05<00:02,  5.55it/s, loss=120, v_num=0, train_loss_step=112.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  68%|██████▊   | 28/41 [00:05<00:02,  5.55it/s, loss=121, v_num=0, train_loss_step=110.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  71%|███████   | 29/41 [00:05<00:02,  5.55it/s, loss=121, v_num=0, train_loss_step=110.0, val_loss=181.0, train_loss_epoch=133.0]#015Epoch 3:  71%|███████   | 29/41 [00:05<00:02,  5.55it/s, loss=121, v_num=0, train_loss_step=110.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  71%|███████   | 29/41 [00:05<00:02,  5.55it/s, loss=121, v_num=0, train_loss_step=126.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  73%|███████▎  | 30/41 [00:05<00:01,  5.55it/s, loss=121, v_num=0, train_loss_step=126.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  73%|███████▎  | 30/41 [00:05<00:01,  5.55it/s, loss=121, v_num=0, train_loss_step=126.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  73%|███████▎  | 30/41 [00:05<00:01,  5.55it/s, loss=121, v_num=0, train_loss_step=133.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  76%|███████▌  | 31/41 [00:05<00:01,  5.55it/s, loss=121, v_num=0, train_loss_step=133.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  76%|███████▌  | 31/41 [00:05<00:01,  5.55it/s, loss=121, v_num=0, train_loss_step=133.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  76%|███████▌  | 31/41 [00:05<00:01,  5.55it/s, loss=121, v_num=0, train_loss_step=95.20, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  78%|███████▊  | 32/41 [00:05<00:01,  5.56it/s, loss=121, v_num=0, train_loss_step=95.20, val_loss=181.0, train_loss_epoch=133.0]#015Epoch 3:  78%|███████▊  | 32/41 [00:05<00:01,  5.56it/s, loss=121, v_num=0, train_loss_step=95.20, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  78%|███████▊  | 32/41 [00:05<00:01,  5.55it/s, loss=121, v_num=0, train_loss_step=123.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  80%|████████  | 33/41 [00:05<00:01,  5.56it/s, loss=121, v_num=0, train_loss_step=123.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  80%|████████  | 33/41 [00:05<00:01,  5.56it/s, loss=121, v_num=0, train_loss_step=123.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  80%|████████  | 33/41 [00:05<00:01,  5.56it/s, loss=120, v_num=0, train_loss_step=96.60, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  83%|████████▎ | 34/41 [00:06<00:01,  5.56it/s, loss=120, v_num=0, train_loss_step=96.60, val_loss=181.0, train_loss_epoch=133.0]#015Epoch 3:  83%|████████▎ | 34/41 [00:06<00:01,  5.56it/s, loss=120, v_num=0, train_loss_step=96.60, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  83%|████████▎ | 34/41 [00:06<00:01,  5.56it/s, loss=120, v_num=0, train_loss_step=103.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  85%|████████▌ | 35/41 [00:06<00:01,  5.56it/s, loss=120, v_num=0, train_loss_step=103.0, val_loss=181.0, train_loss_epoch=133.0]#015Epoch 3:  85%|████████▌ | 35/41 [00:06<00:01,  5.56it/s, loss=120, v_num=0, train_loss_step=103.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  85%|████████▌ | 35/41 [00:06<00:01,  5.56it/s, loss=117, v_num=0, train_loss_step=77.10, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  88%|████████▊ | 36/41 [00:06<00:00,  5.55it/s, loss=117, v_num=0, train_loss_step=77.10, val_loss=181.0, train_loss_epoch=133.0]#015Epoch 3:  88%|████████▊ | 36/41 [00:06<00:00,  5.55it/s, loss=117, v_num=0, train_loss_step=77.10, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  88%|████████▊ | 36/41 [00:06<00:00,  5.55it/s, loss=117, v_num=0, train_loss_step=105.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  90%|█████████ | 37/41 [00:06<00:00,  5.55it/s, loss=117, v_num=0, train_loss_step=105.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  90%|█████████ | 37/41 [00:06<00:00,  5.55it/s, loss=117, v_num=0, train_loss_step=105.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  90%|█████████ | 37/41 [00:06<00:00,  5.55it/s, loss=114, v_num=0, train_loss_step=68.90, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  93%|█████████▎| 38/41 [00:06<00:00,  5.55it/s, loss=114, v_num=0, train_loss_step=68.90, val_loss=181.0, train_loss_epoch=133.0]#015Epoch 3:  93%|█████████▎| 38/41 [00:06<00:00,  5.55it/s, loss=114, v_num=0, train_loss_step=68.90, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  93%|█████████▎| 38/41 [00:06<00:00,  5.55it/s, loss=113, v_num=0, train_loss_step=101.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  95%|█████████▌| 39/41 [00:07<00:00,  5.56it/s, loss=113, v_num=0, train_loss_step=101.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  95%|█████████▌| 39/41 [00:07<00:00,  5.56it/s, loss=113, v_num=0, train_loss_step=101.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  95%|█████████▌| 39/41 [00:07<00:00,  5.56it/s, loss=113, v_num=0, train_loss_step=109.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  98%|█████████▊| 40/41 [00:07<00:00,  5.56it/s, loss=113, v_num=0, train_loss_step=109.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  98%|█████████▊| 40/41 [00:07<00:00,  5.56it/s, loss=113, v_num=0, train_loss_step=109.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:  98%|█████████▊| 40/41 [00:07<00:00,  5.56it/s, loss=113, v_num=0, train_loss_step=106.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  5.01it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  5.01it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 3: 100%|██████████| 41/41 [00:07<00:00,  5.50it/s, loss=113, v_num=0, train_loss_step=106.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3: 100%|██████████| 41/41 [00:07<00:00,  5.50it/s, loss=113, v_num=0, train_loss_step=106.0, val_loss=181.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34mEpoch 3: 100%|██████████| 41/41 [00:08<00:00,  5.12it/s, loss=113, v_num=0, train_loss_step=106.0, val_loss=154.0, train_loss_epoch=133.0]\u001b[0m\n",
      "\u001b[34m#015                                                                      #033[A\u001b[0m\n",
      "\u001b[34mEpoch 3: 100%|██████████| 41/41 [00:08<00:00,  5.12it/s, loss=113, v_num=0, train_loss_step=106.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 3:   0%|          | 0/41 [00:00<?, ?it/s, loss=113, v_num=0, train_loss_step=106.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:   0%|          | 0/41 [00:00<?, ?it/s, loss=113, v_num=0, train_loss_step=106.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:   2%|▏         | 1/41 [00:00<00:06,  5.86it/s, loss=113, v_num=0, train_loss_step=106.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:   2%|▏         | 1/41 [00:00<00:06,  5.85it/s, loss=113, v_num=0, train_loss_step=106.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:   2%|▏         | 1/41 [00:00<00:06,  5.84it/s, loss=113, v_num=0, train_loss_step=109.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:   5%|▍         | 2/41 [00:00<00:06,  5.63it/s, loss=113, v_num=0, train_loss_step=109.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:   5%|▍         | 2/41 [00:00<00:06,  5.63it/s, loss=113, v_num=0, train_loss_step=109.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:   5%|▍         | 2/41 [00:00<00:06,  5.62it/s, loss=113, v_num=0, train_loss_step=127.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:   7%|▋         | 3/41 [00:00<00:06,  5.66it/s, loss=113, v_num=0, train_loss_step=127.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:   7%|▋         | 3/41 [00:00<00:06,  5.66it/s, loss=113, v_num=0, train_loss_step=127.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:   7%|▋         | 3/41 [00:00<00:06,  5.65it/s, loss=111, v_num=0, train_loss_step=110.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  10%|▉         | 4/41 [00:00<00:06,  5.68it/s, loss=111, v_num=0, train_loss_step=110.0, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  10%|▉         | 4/41 [00:00<00:06,  5.68it/s, loss=111, v_num=0, train_loss_step=110.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  10%|▉         | 4/41 [00:00<00:06,  5.67it/s, loss=110, v_num=0, train_loss_step=116.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  12%|█▏        | 5/41 [00:00<00:06,  5.69it/s, loss=110, v_num=0, train_loss_step=116.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  12%|█▏        | 5/41 [00:00<00:06,  5.69it/s, loss=110, v_num=0, train_loss_step=116.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  12%|█▏        | 5/41 [00:00<00:06,  5.68it/s, loss=108, v_num=0, train_loss_step=106.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  15%|█▍        | 6/41 [00:01<00:06,  5.69it/s, loss=108, v_num=0, train_loss_step=106.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  15%|█▍        | 6/41 [00:01<00:06,  5.69it/s, loss=108, v_num=0, train_loss_step=106.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  15%|█▍        | 6/41 [00:01<00:06,  5.68it/s, loss=107, v_num=0, train_loss_step=110.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  17%|█▋        | 7/41 [00:01<00:05,  5.69it/s, loss=107, v_num=0, train_loss_step=110.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  17%|█▋        | 7/41 [00:01<00:05,  5.69it/s, loss=107, v_num=0, train_loss_step=110.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  17%|█▋        | 7/41 [00:01<00:05,  5.69it/s, loss=107, v_num=0, train_loss_step=109.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  20%|█▉        | 8/41 [00:01<00:05,  5.64it/s, loss=107, v_num=0, train_loss_step=109.0, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  20%|█▉        | 8/41 [00:01<00:05,  5.64it/s, loss=107, v_num=0, train_loss_step=109.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  20%|█▉        | 8/41 [00:01<00:05,  5.63it/s, loss=106, v_num=0, train_loss_step=82.70, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  22%|██▏       | 9/41 [00:01<00:05,  5.66it/s, loss=106, v_num=0, train_loss_step=82.70, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  22%|██▏       | 9/41 [00:01<00:05,  5.66it/s, loss=106, v_num=0, train_loss_step=82.70, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  22%|██▏       | 9/41 [00:01<00:05,  5.66it/s, loss=106, v_num=0, train_loss_step=133.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  24%|██▍       | 10/41 [00:01<00:05,  5.66it/s, loss=106, v_num=0, train_loss_step=133.0, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  24%|██▍       | 10/41 [00:01<00:05,  5.66it/s, loss=106, v_num=0, train_loss_step=133.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  24%|██▍       | 10/41 [00:01<00:05,  5.66it/s, loss=104, v_num=0, train_loss_step=92.80, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  27%|██▋       | 11/41 [00:01<00:05,  5.66it/s, loss=104, v_num=0, train_loss_step=92.80, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  27%|██▋       | 11/41 [00:01<00:05,  5.66it/s, loss=104, v_num=0, train_loss_step=92.80, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  27%|██▋       | 11/41 [00:01<00:05,  5.66it/s, loss=104, v_num=0, train_loss_step=93.50, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  29%|██▉       | 12/41 [00:02<00:05,  5.65it/s, loss=104, v_num=0, train_loss_step=93.50, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  29%|██▉       | 12/41 [00:02<00:05,  5.65it/s, loss=104, v_num=0, train_loss_step=93.50, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  29%|██▉       | 12/41 [00:02<00:05,  5.65it/s, loss=103, v_num=0, train_loss_step=107.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  32%|███▏      | 13/41 [00:02<00:04,  5.65it/s, loss=103, v_num=0, train_loss_step=107.0, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  32%|███▏      | 13/41 [00:02<00:04,  5.65it/s, loss=103, v_num=0, train_loss_step=107.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  32%|███▏      | 13/41 [00:02<00:04,  5.65it/s, loss=103, v_num=0, train_loss_step=101.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  34%|███▍      | 14/41 [00:02<00:04,  5.63it/s, loss=103, v_num=0, train_loss_step=101.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  34%|███▍      | 14/41 [00:02<00:04,  5.63it/s, loss=103, v_num=0, train_loss_step=101.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  34%|███▍      | 14/41 [00:02<00:04,  5.63it/s, loss=104, v_num=0, train_loss_step=118.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  37%|███▋      | 15/41 [00:02<00:04,  5.63it/s, loss=104, v_num=0, train_loss_step=118.0, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  37%|███▋      | 15/41 [00:02<00:04,  5.63it/s, loss=104, v_num=0, train_loss_step=118.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  37%|███▋      | 15/41 [00:02<00:04,  5.63it/s, loss=106, v_num=0, train_loss_step=124.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  39%|███▉      | 16/41 [00:02<00:04,  5.63it/s, loss=106, v_num=0, train_loss_step=124.0, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  39%|███▉      | 16/41 [00:02<00:04,  5.63it/s, loss=106, v_num=0, train_loss_step=124.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  39%|███▉      | 16/41 [00:02<00:04,  5.63it/s, loss=107, v_num=0, train_loss_step=120.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  41%|████▏     | 17/41 [00:03<00:04,  5.63it/s, loss=107, v_num=0, train_loss_step=120.0, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  41%|████▏     | 17/41 [00:03<00:04,  5.63it/s, loss=107, v_num=0, train_loss_step=120.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  41%|████▏     | 17/41 [00:03<00:04,  5.63it/s, loss=108, v_num=0, train_loss_step=93.00, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  44%|████▍     | 18/41 [00:03<00:04,  5.63it/s, loss=108, v_num=0, train_loss_step=93.00, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  44%|████▍     | 18/41 [00:03<00:04,  5.63it/s, loss=108, v_num=0, train_loss_step=93.00, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  44%|████▍     | 18/41 [00:03<00:04,  5.63it/s, loss=108, v_num=0, train_loss_step=103.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  46%|████▋     | 19/41 [00:03<00:03,  5.63it/s, loss=108, v_num=0, train_loss_step=103.0, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  46%|████▋     | 19/41 [00:03<00:03,  5.63it/s, loss=108, v_num=0, train_loss_step=103.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  46%|████▋     | 19/41 [00:03<00:03,  5.63it/s, loss=109, v_num=0, train_loss_step=116.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  49%|████▉     | 20/41 [00:03<00:03,  5.61it/s, loss=109, v_num=0, train_loss_step=116.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  49%|████▉     | 20/41 [00:03<00:03,  5.61it/s, loss=109, v_num=0, train_loss_step=116.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  49%|████▉     | 20/41 [00:03<00:03,  5.61it/s, loss=108, v_num=0, train_loss_step=98.00, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  51%|█████     | 21/41 [00:03<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=98.00, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  51%|█████     | 21/41 [00:03<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=98.00, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  51%|█████     | 21/41 [00:03<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=107.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  54%|█████▎    | 22/41 [00:03<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=107.0, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  54%|█████▎    | 22/41 [00:03<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=107.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  54%|█████▎    | 22/41 [00:03<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=120.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  56%|█████▌    | 23/41 [00:04<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=120.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  56%|█████▌    | 23/41 [00:04<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=120.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  56%|█████▌    | 23/41 [00:04<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=99.80, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  59%|█████▊    | 24/41 [00:04<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=99.80, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  59%|█████▊    | 24/41 [00:04<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=99.80, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  59%|█████▊    | 24/41 [00:04<00:03,  5.62it/s, loss=106, v_num=0, train_loss_step=89.30, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  61%|██████    | 25/41 [00:04<00:02,  5.62it/s, loss=106, v_num=0, train_loss_step=89.30, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  61%|██████    | 25/41 [00:04<00:02,  5.62it/s, loss=106, v_num=0, train_loss_step=89.30, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  61%|██████    | 25/41 [00:04<00:02,  5.62it/s, loss=107, v_num=0, train_loss_step=117.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  63%|██████▎   | 26/41 [00:04<00:02,  5.61it/s, loss=107, v_num=0, train_loss_step=117.0, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  63%|██████▎   | 26/41 [00:04<00:02,  5.61it/s, loss=107, v_num=0, train_loss_step=117.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  63%|██████▎   | 26/41 [00:04<00:02,  5.61it/s, loss=106, v_num=0, train_loss_step=89.60, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  66%|██████▌   | 27/41 [00:04<00:02,  5.62it/s, loss=106, v_num=0, train_loss_step=89.60, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  66%|██████▌   | 27/41 [00:04<00:02,  5.62it/s, loss=106, v_num=0, train_loss_step=89.60, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  66%|██████▌   | 27/41 [00:04<00:02,  5.62it/s, loss=105, v_num=0, train_loss_step=87.00, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  68%|██████▊   | 28/41 [00:04<00:02,  5.62it/s, loss=105, v_num=0, train_loss_step=87.00, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  68%|██████▊   | 28/41 [00:04<00:02,  5.62it/s, loss=105, v_num=0, train_loss_step=87.00, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  68%|██████▊   | 28/41 [00:04<00:02,  5.62it/s, loss=105, v_num=0, train_loss_step=90.10, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  71%|███████   | 29/41 [00:05<00:02,  5.62it/s, loss=105, v_num=0, train_loss_step=90.10, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  71%|███████   | 29/41 [00:05<00:02,  5.62it/s, loss=105, v_num=0, train_loss_step=90.10, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  71%|███████   | 29/41 [00:05<00:02,  5.62it/s, loss=104, v_num=0, train_loss_step=120.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  73%|███████▎  | 30/41 [00:05<00:01,  5.62it/s, loss=104, v_num=0, train_loss_step=120.0, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  73%|███████▎  | 30/41 [00:05<00:01,  5.62it/s, loss=104, v_num=0, train_loss_step=120.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  73%|███████▎  | 30/41 [00:05<00:01,  5.62it/s, loss=104, v_num=0, train_loss_step=90.00, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  76%|███████▌  | 31/41 [00:05<00:01,  5.62it/s, loss=104, v_num=0, train_loss_step=90.00, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  76%|███████▌  | 31/41 [00:05<00:01,  5.62it/s, loss=104, v_num=0, train_loss_step=90.00, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  76%|███████▌  | 31/41 [00:05<00:01,  5.62it/s, loss=105, v_num=0, train_loss_step=117.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  78%|███████▊  | 32/41 [00:05<00:01,  5.61it/s, loss=105, v_num=0, train_loss_step=117.0, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  78%|███████▊  | 32/41 [00:05<00:01,  5.61it/s, loss=105, v_num=0, train_loss_step=117.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  78%|███████▊  | 32/41 [00:05<00:01,  5.61it/s, loss=106, v_num=0, train_loss_step=121.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  80%|████████  | 33/41 [00:05<00:01,  5.62it/s, loss=106, v_num=0, train_loss_step=121.0, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  80%|████████  | 33/41 [00:05<00:01,  5.61it/s, loss=106, v_num=0, train_loss_step=121.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  80%|████████  | 33/41 [00:05<00:01,  5.61it/s, loss=108, v_num=0, train_loss_step=138.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  83%|████████▎ | 34/41 [00:06<00:01,  5.62it/s, loss=108, v_num=0, train_loss_step=138.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  83%|████████▎ | 34/41 [00:06<00:01,  5.62it/s, loss=108, v_num=0, train_loss_step=138.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  83%|████████▎ | 34/41 [00:06<00:01,  5.62it/s, loss=107, v_num=0, train_loss_step=101.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  85%|████████▌ | 35/41 [00:06<00:01,  5.62it/s, loss=107, v_num=0, train_loss_step=101.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  85%|████████▌ | 35/41 [00:06<00:01,  5.62it/s, loss=107, v_num=0, train_loss_step=101.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  85%|████████▌ | 35/41 [00:06<00:01,  5.62it/s, loss=107, v_num=0, train_loss_step=115.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  88%|████████▊ | 36/41 [00:06<00:00,  5.62it/s, loss=107, v_num=0, train_loss_step=115.0, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  88%|████████▊ | 36/41 [00:06<00:00,  5.62it/s, loss=107, v_num=0, train_loss_step=115.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  88%|████████▊ | 36/41 [00:06<00:00,  5.62it/s, loss=107, v_num=0, train_loss_step=122.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  90%|█████████ | 37/41 [00:06<00:00,  5.62it/s, loss=107, v_num=0, train_loss_step=122.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  90%|█████████ | 37/41 [00:06<00:00,  5.62it/s, loss=107, v_num=0, train_loss_step=122.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  90%|█████████ | 37/41 [00:06<00:00,  5.62it/s, loss=109, v_num=0, train_loss_step=130.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  93%|█████████▎| 38/41 [00:06<00:00,  5.61it/s, loss=109, v_num=0, train_loss_step=130.0, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  93%|█████████▎| 38/41 [00:06<00:00,  5.61it/s, loss=109, v_num=0, train_loss_step=130.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  93%|█████████▎| 38/41 [00:06<00:00,  5.61it/s, loss=109, v_num=0, train_loss_step=117.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  95%|█████████▌| 39/41 [00:06<00:00,  5.61it/s, loss=109, v_num=0, train_loss_step=117.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  95%|█████████▌| 39/41 [00:06<00:00,  5.61it/s, loss=109, v_num=0, train_loss_step=117.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  95%|█████████▌| 39/41 [00:06<00:00,  5.61it/s, loss=108, v_num=0, train_loss_step=89.90, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  98%|█████████▊| 40/41 [00:07<00:00,  5.62it/s, loss=108, v_num=0, train_loss_step=89.90, val_loss=154.0, train_loss_epoch=116.0]#015Epoch 4:  98%|█████████▊| 40/41 [00:07<00:00,  5.61it/s, loss=108, v_num=0, train_loss_step=89.90, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:  98%|█████████▊| 40/41 [00:07<00:00,  5.61it/s, loss=108, v_num=0, train_loss_step=102.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.70it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.70it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 4: 100%|██████████| 41/41 [00:07<00:00,  5.54it/s, loss=108, v_num=0, train_loss_step=102.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4: 100%|██████████| 41/41 [00:07<00:00,  5.54it/s, loss=108, v_num=0, train_loss_step=102.0, val_loss=154.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34mEpoch 4: 100%|██████████| 41/41 [00:08<00:00,  5.06it/s, loss=108, v_num=0, train_loss_step=102.0, val_loss=155.0, train_loss_epoch=116.0]\u001b[0m\n",
      "\u001b[34m#015                                                                      #033[A\u001b[0m\n",
      "\u001b[34mEpoch 4: 100%|██████████| 41/41 [00:08<00:00,  5.05it/s, loss=108, v_num=0, train_loss_step=102.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 4:   0%|          | 0/41 [00:00<?, ?it/s, loss=108, v_num=0, train_loss_step=102.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:   0%|          | 0/41 [00:00<?, ?it/s, loss=108, v_num=0, train_loss_step=102.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:   2%|▏         | 1/41 [00:00<00:06,  5.76it/s, loss=108, v_num=0, train_loss_step=102.0, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:   2%|▏         | 1/41 [00:00<00:06,  5.75it/s, loss=108, v_num=0, train_loss_step=102.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:   2%|▏         | 1/41 [00:00<00:06,  5.74it/s, loss=109, v_num=0, train_loss_step=128.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:   5%|▍         | 2/41 [00:00<00:06,  5.76it/s, loss=109, v_num=0, train_loss_step=128.0, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:   5%|▍         | 2/41 [00:00<00:06,  5.76it/s, loss=109, v_num=0, train_loss_step=128.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:   5%|▍         | 2/41 [00:00<00:06,  5.75it/s, loss=109, v_num=0, train_loss_step=122.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:   7%|▋         | 3/41 [00:00<00:06,  5.68it/s, loss=109, v_num=0, train_loss_step=122.0, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:   7%|▋         | 3/41 [00:00<00:06,  5.68it/s, loss=109, v_num=0, train_loss_step=122.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:   7%|▋         | 3/41 [00:00<00:06,  5.68it/s, loss=110, v_num=0, train_loss_step=109.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  10%|▉         | 4/41 [00:00<00:06,  5.59it/s, loss=110, v_num=0, train_loss_step=109.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  10%|▉         | 4/41 [00:00<00:06,  5.59it/s, loss=110, v_num=0, train_loss_step=109.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  10%|▉         | 4/41 [00:00<00:06,  5.59it/s, loss=110, v_num=0, train_loss_step=99.50, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  12%|█▏        | 5/41 [00:00<00:06,  5.60it/s, loss=110, v_num=0, train_loss_step=99.50, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  12%|█▏        | 5/41 [00:00<00:06,  5.60it/s, loss=110, v_num=0, train_loss_step=99.50, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  12%|█▏        | 5/41 [00:00<00:06,  5.59it/s, loss=109, v_num=0, train_loss_step=102.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  15%|█▍        | 6/41 [00:01<00:06,  5.61it/s, loss=109, v_num=0, train_loss_step=102.0, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  15%|█▍        | 6/41 [00:01<00:06,  5.60it/s, loss=109, v_num=0, train_loss_step=102.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  15%|█▍        | 6/41 [00:01<00:06,  5.60it/s, loss=110, v_num=0, train_loss_step=108.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  17%|█▋        | 7/41 [00:01<00:06,  5.61it/s, loss=110, v_num=0, train_loss_step=108.0, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  17%|█▋        | 7/41 [00:01<00:06,  5.61it/s, loss=110, v_num=0, train_loss_step=108.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  17%|█▋        | 7/41 [00:01<00:06,  5.61it/s, loss=113, v_num=0, train_loss_step=138.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  20%|█▉        | 8/41 [00:01<00:05,  5.62it/s, loss=113, v_num=0, train_loss_step=138.0, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  20%|█▉        | 8/41 [00:01<00:05,  5.62it/s, loss=113, v_num=0, train_loss_step=138.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  20%|█▉        | 8/41 [00:01<00:05,  5.62it/s, loss=114, v_num=0, train_loss_step=120.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  22%|██▏       | 9/41 [00:01<00:05,  5.62it/s, loss=114, v_num=0, train_loss_step=120.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  22%|██▏       | 9/41 [00:01<00:05,  5.62it/s, loss=114, v_num=0, train_loss_step=120.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  22%|██▏       | 9/41 [00:01<00:05,  5.62it/s, loss=113, v_num=0, train_loss_step=82.50, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  24%|██▍       | 10/41 [00:01<00:05,  5.60it/s, loss=113, v_num=0, train_loss_step=82.50, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  24%|██▍       | 10/41 [00:01<00:05,  5.60it/s, loss=113, v_num=0, train_loss_step=82.50, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  24%|██▍       | 10/41 [00:01<00:05,  5.60it/s, loss=114, v_num=0, train_loss_step=110.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  27%|██▋       | 11/41 [00:01<00:05,  5.60it/s, loss=114, v_num=0, train_loss_step=110.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  27%|██▋       | 11/41 [00:01<00:05,  5.59it/s, loss=114, v_num=0, train_loss_step=110.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  27%|██▋       | 11/41 [00:01<00:05,  5.59it/s, loss=114, v_num=0, train_loss_step=123.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  29%|██▉       | 12/41 [00:02<00:05,  5.60it/s, loss=114, v_num=0, train_loss_step=123.0, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  29%|██▉       | 12/41 [00:02<00:05,  5.60it/s, loss=114, v_num=0, train_loss_step=123.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  29%|██▉       | 12/41 [00:02<00:05,  5.60it/s, loss=114, v_num=0, train_loss_step=130.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  32%|███▏      | 13/41 [00:02<00:04,  5.61it/s, loss=114, v_num=0, train_loss_step=130.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  32%|███▏      | 13/41 [00:02<00:04,  5.61it/s, loss=114, v_num=0, train_loss_step=130.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  32%|███▏      | 13/41 [00:02<00:04,  5.61it/s, loss=113, v_num=0, train_loss_step=118.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  34%|███▍      | 14/41 [00:02<00:04,  5.62it/s, loss=113, v_num=0, train_loss_step=118.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  34%|███▍      | 14/41 [00:02<00:04,  5.62it/s, loss=113, v_num=0, train_loss_step=118.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  34%|███▍      | 14/41 [00:02<00:04,  5.62it/s, loss=114, v_num=0, train_loss_step=107.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  37%|███▋      | 15/41 [00:02<00:04,  5.62it/s, loss=114, v_num=0, train_loss_step=107.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  37%|███▋      | 15/41 [00:02<00:04,  5.62it/s, loss=114, v_num=0, train_loss_step=107.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  37%|███▋      | 15/41 [00:02<00:04,  5.62it/s, loss=112, v_num=0, train_loss_step=92.90, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  39%|███▉      | 16/41 [00:02<00:04,  5.61it/s, loss=112, v_num=0, train_loss_step=92.90, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  39%|███▉      | 16/41 [00:02<00:04,  5.61it/s, loss=112, v_num=0, train_loss_step=92.90, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  39%|███▉      | 16/41 [00:02<00:04,  5.60it/s, loss=111, v_num=0, train_loss_step=88.50, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  41%|████▏     | 17/41 [00:03<00:04,  5.61it/s, loss=111, v_num=0, train_loss_step=88.50, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  41%|████▏     | 17/41 [00:03<00:04,  5.61it/s, loss=111, v_num=0, train_loss_step=88.50, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  41%|████▏     | 17/41 [00:03<00:04,  5.61it/s, loss=109, v_num=0, train_loss_step=89.00, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  44%|████▍     | 18/41 [00:03<00:04,  5.61it/s, loss=109, v_num=0, train_loss_step=89.00, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  44%|████▍     | 18/41 [00:03<00:04,  5.61it/s, loss=109, v_num=0, train_loss_step=89.00, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  44%|████▍     | 18/41 [00:03<00:04,  5.61it/s, loss=108, v_num=0, train_loss_step=104.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  46%|████▋     | 19/41 [00:03<00:03,  5.61it/s, loss=108, v_num=0, train_loss_step=104.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  46%|████▋     | 19/41 [00:03<00:03,  5.61it/s, loss=108, v_num=0, train_loss_step=104.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  46%|████▋     | 19/41 [00:03<00:03,  5.61it/s, loss=108, v_num=0, train_loss_step=88.30, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  49%|████▉     | 20/41 [00:03<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=88.30, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  49%|████▉     | 20/41 [00:03<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=88.30, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  49%|████▉     | 20/41 [00:03<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=110.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  51%|█████     | 21/41 [00:03<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=110.0, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  51%|█████     | 21/41 [00:03<00:03,  5.62it/s, loss=108, v_num=0, train_loss_step=110.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  51%|█████     | 21/41 [00:03<00:03,  5.62it/s, loss=107, v_num=0, train_loss_step=103.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  54%|█████▎    | 22/41 [00:03<00:03,  5.61it/s, loss=107, v_num=0, train_loss_step=103.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  54%|█████▎    | 22/41 [00:03<00:03,  5.61it/s, loss=107, v_num=0, train_loss_step=103.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  54%|█████▎    | 22/41 [00:03<00:03,  5.61it/s, loss=108, v_num=0, train_loss_step=133.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  56%|█████▌    | 23/41 [00:04<00:03,  5.61it/s, loss=108, v_num=0, train_loss_step=133.0, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  56%|█████▌    | 23/41 [00:04<00:03,  5.61it/s, loss=108, v_num=0, train_loss_step=133.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  56%|█████▌    | 23/41 [00:04<00:03,  5.61it/s, loss=108, v_num=0, train_loss_step=116.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  59%|█████▊    | 24/41 [00:04<00:03,  5.61it/s, loss=108, v_num=0, train_loss_step=116.0, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  59%|█████▊    | 24/41 [00:04<00:03,  5.61it/s, loss=108, v_num=0, train_loss_step=116.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  59%|█████▊    | 24/41 [00:04<00:03,  5.61it/s, loss=108, v_num=0, train_loss_step=94.90, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  61%|██████    | 25/41 [00:04<00:02,  5.61it/s, loss=108, v_num=0, train_loss_step=94.90, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  61%|██████    | 25/41 [00:04<00:02,  5.61it/s, loss=108, v_num=0, train_loss_step=94.90, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  61%|██████    | 25/41 [00:04<00:02,  5.61it/s, loss=108, v_num=0, train_loss_step=110.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  63%|██████▎   | 26/41 [00:04<00:02,  5.61it/s, loss=108, v_num=0, train_loss_step=110.0, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  63%|██████▎   | 26/41 [00:04<00:02,  5.61it/s, loss=108, v_num=0, train_loss_step=110.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  63%|██████▎   | 26/41 [00:04<00:02,  5.61it/s, loss=108, v_num=0, train_loss_step=99.40, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  66%|██████▌   | 27/41 [00:04<00:02,  5.62it/s, loss=108, v_num=0, train_loss_step=99.40, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  66%|██████▌   | 27/41 [00:04<00:02,  5.62it/s, loss=108, v_num=0, train_loss_step=99.40, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  66%|██████▌   | 27/41 [00:04<00:02,  5.62it/s, loss=107, v_num=0, train_loss_step=123.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  68%|██████▊   | 28/41 [00:05<00:02,  5.60it/s, loss=107, v_num=0, train_loss_step=123.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  68%|██████▊   | 28/41 [00:05<00:02,  5.60it/s, loss=107, v_num=0, train_loss_step=123.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  68%|██████▊   | 28/41 [00:05<00:02,  5.60it/s, loss=107, v_num=0, train_loss_step=111.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  71%|███████   | 29/41 [00:05<00:02,  5.59it/s, loss=107, v_num=0, train_loss_step=111.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  71%|███████   | 29/41 [00:05<00:02,  5.58it/s, loss=107, v_num=0, train_loss_step=111.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  71%|███████   | 29/41 [00:05<00:02,  5.58it/s, loss=107, v_num=0, train_loss_step=97.50, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  73%|███████▎  | 30/41 [00:05<00:01,  5.58it/s, loss=107, v_num=0, train_loss_step=97.50, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  73%|███████▎  | 30/41 [00:05<00:01,  5.58it/s, loss=107, v_num=0, train_loss_step=97.50, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  73%|███████▎  | 30/41 [00:05<00:01,  5.58it/s, loss=106, v_num=0, train_loss_step=77.90, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  76%|███████▌  | 31/41 [00:05<00:01,  5.57it/s, loss=106, v_num=0, train_loss_step=77.90, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  76%|███████▌  | 31/41 [00:05<00:01,  5.57it/s, loss=106, v_num=0, train_loss_step=77.90, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  76%|███████▌  | 31/41 [00:05<00:01,  5.57it/s, loss=104, v_num=0, train_loss_step=81.70, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  78%|███████▊  | 32/41 [00:05<00:01,  5.56it/s, loss=104, v_num=0, train_loss_step=81.70, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  78%|███████▊  | 32/41 [00:05<00:01,  5.56it/s, loss=104, v_num=0, train_loss_step=81.70, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  78%|███████▊  | 32/41 [00:05<00:01,  5.56it/s, loss=103, v_num=0, train_loss_step=119.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  80%|████████  | 33/41 [00:05<00:01,  5.55it/s, loss=103, v_num=0, train_loss_step=119.0, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  80%|████████  | 33/41 [00:05<00:01,  5.55it/s, loss=103, v_num=0, train_loss_step=119.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  80%|████████  | 33/41 [00:05<00:01,  5.55it/s, loss=104, v_num=0, train_loss_step=135.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  83%|████████▎ | 34/41 [00:06<00:01,  5.54it/s, loss=104, v_num=0, train_loss_step=135.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  83%|████████▎ | 34/41 [00:06<00:01,  5.54it/s, loss=104, v_num=0, train_loss_step=135.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  83%|████████▎ | 34/41 [00:06<00:01,  5.54it/s, loss=103, v_num=0, train_loss_step=92.50, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  85%|████████▌ | 35/41 [00:06<00:01,  5.54it/s, loss=103, v_num=0, train_loss_step=92.50, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  85%|████████▌ | 35/41 [00:06<00:01,  5.54it/s, loss=103, v_num=0, train_loss_step=92.50, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  85%|████████▌ | 35/41 [00:06<00:01,  5.54it/s, loss=103, v_num=0, train_loss_step=91.10, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  88%|████████▊ | 36/41 [00:06<00:00,  5.54it/s, loss=103, v_num=0, train_loss_step=91.10, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  88%|████████▊ | 36/41 [00:06<00:00,  5.54it/s, loss=103, v_num=0, train_loss_step=91.10, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  88%|████████▊ | 36/41 [00:06<00:00,  5.54it/s, loss=103, v_num=0, train_loss_step=82.90, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  90%|█████████ | 37/41 [00:06<00:00,  5.54it/s, loss=103, v_num=0, train_loss_step=82.90, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  90%|█████████ | 37/41 [00:06<00:00,  5.54it/s, loss=103, v_num=0, train_loss_step=82.90, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  90%|█████████ | 37/41 [00:06<00:00,  5.54it/s, loss=103, v_num=0, train_loss_step=95.00, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  93%|█████████▎| 38/41 [00:06<00:00,  5.53it/s, loss=103, v_num=0, train_loss_step=95.00, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  93%|█████████▎| 38/41 [00:06<00:00,  5.53it/s, loss=103, v_num=0, train_loss_step=95.00, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  93%|█████████▎| 38/41 [00:06<00:00,  5.53it/s, loss=103, v_num=0, train_loss_step=106.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  95%|█████████▌| 39/41 [00:07<00:00,  5.53it/s, loss=103, v_num=0, train_loss_step=106.0, val_loss=155.0, train_loss_epoch=108.0]#015Epoch 5:  95%|█████████▌| 39/41 [00:07<00:00,  5.53it/s, loss=103, v_num=0, train_loss_step=106.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  95%|█████████▌| 39/41 [00:07<00:00,  5.53it/s, loss=106, v_num=0, train_loss_step=135.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  98%|█████████▊| 40/41 [00:07<00:00,  5.52it/s, loss=106, v_num=0, train_loss_step=135.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  98%|█████████▊| 40/41 [00:07<00:00,  5.52it/s, loss=106, v_num=0, train_loss_step=135.0, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:  98%|█████████▊| 40/41 [00:07<00:00,  5.52it/s, loss=105, v_num=0, train_loss_step=88.90, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 5: 100%|██████████| 41/41 [00:07<00:00,  5.39it/s, loss=105, v_num=0, train_loss_step=88.90, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5: 100%|██████████| 41/41 [00:07<00:00,  5.39it/s, loss=105, v_num=0, train_loss_step=88.90, val_loss=155.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34mEpoch 5: 100%|██████████| 41/41 [00:08<00:00,  5.03it/s, loss=105, v_num=0, train_loss_step=88.90, val_loss=152.0, train_loss_epoch=108.0]\u001b[0m\n",
      "\u001b[34m#015                                                                      #033[A\u001b[0m\n",
      "\u001b[34mEpoch 5: 100%|██████████| 41/41 [00:08<00:00,  5.03it/s, loss=105, v_num=0, train_loss_step=88.90, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 5:   0%|          | 0/41 [00:00<?, ?it/s, loss=105, v_num=0, train_loss_step=88.90, val_loss=152.0, train_loss_epoch=105.0]         #015Epoch 6:   0%|          | 0/41 [00:00<?, ?it/s, loss=105, v_num=0, train_loss_step=88.90, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:   2%|▏         | 1/41 [00:00<00:10,  3.68it/s, loss=105, v_num=0, train_loss_step=88.90, val_loss=152.0, train_loss_epoch=105.0]#015Epoch 6:   2%|▏         | 1/41 [00:00<00:10,  3.68it/s, loss=105, v_num=0, train_loss_step=88.90, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:   2%|▏         | 1/41 [00:00<00:10,  3.67it/s, loss=104, v_num=0, train_loss_step=97.10, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:   5%|▍         | 2/41 [00:00<00:09,  4.15it/s, loss=104, v_num=0, train_loss_step=97.10, val_loss=152.0, train_loss_epoch=105.0]#015Epoch 6:   5%|▍         | 2/41 [00:00<00:09,  4.15it/s, loss=104, v_num=0, train_loss_step=97.10, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:   5%|▍         | 2/41 [00:00<00:09,  4.14it/s, loss=103, v_num=0, train_loss_step=98.40, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:   7%|▋         | 3/41 [00:00<00:08,  4.35it/s, loss=103, v_num=0, train_loss_step=98.40, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:   7%|▋         | 3/41 [00:00<00:08,  4.34it/s, loss=103, v_num=0, train_loss_step=98.40, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:   7%|▋         | 3/41 [00:00<00:08,  4.34it/s, loss=101, v_num=0, train_loss_step=85.00, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  10%|▉         | 4/41 [00:00<00:08,  4.46it/s, loss=101, v_num=0, train_loss_step=85.00, val_loss=152.0, train_loss_epoch=105.0]#015Epoch 6:  10%|▉         | 4/41 [00:00<00:08,  4.46it/s, loss=101, v_num=0, train_loss_step=85.00, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  10%|▉         | 4/41 [00:00<00:08,  4.46it/s, loss=101, v_num=0, train_loss_step=87.40, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  12%|█▏        | 5/41 [00:01<00:07,  4.53it/s, loss=101, v_num=0, train_loss_step=87.40, val_loss=152.0, train_loss_epoch=105.0]#015Epoch 6:  12%|█▏        | 5/41 [00:01<00:07,  4.53it/s, loss=101, v_num=0, train_loss_step=87.40, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  12%|█▏        | 5/41 [00:01<00:07,  4.53it/s, loss=98.4, v_num=0, train_loss_step=65.50, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  15%|█▍        | 6/41 [00:01<00:07,  4.52it/s, loss=98.4, v_num=0, train_loss_step=65.50, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  15%|█▍        | 6/41 [00:01<00:07,  4.52it/s, loss=98.4, v_num=0, train_loss_step=65.50, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  15%|█▍        | 6/41 [00:01<00:07,  4.51it/s, loss=99.1, v_num=0, train_loss_step=113.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  17%|█▋        | 7/41 [00:01<00:07,  4.54it/s, loss=99.1, v_num=0, train_loss_step=113.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  17%|█▋        | 7/41 [00:01<00:07,  4.54it/s, loss=99.1, v_num=0, train_loss_step=113.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  17%|█▋        | 7/41 [00:01<00:07,  4.54it/s, loss=97.5, v_num=0, train_loss_step=91.20, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  20%|█▉        | 8/41 [00:01<00:07,  4.56it/s, loss=97.5, v_num=0, train_loss_step=91.20, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  20%|█▉        | 8/41 [00:01<00:07,  4.56it/s, loss=97.5, v_num=0, train_loss_step=91.20, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  20%|█▉        | 8/41 [00:01<00:07,  4.56it/s, loss=97.3, v_num=0, train_loss_step=107.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  22%|██▏       | 9/41 [00:01<00:06,  4.58it/s, loss=97.3, v_num=0, train_loss_step=107.0, val_loss=152.0, train_loss_epoch=105.0]#015Epoch 6:  22%|██▏       | 9/41 [00:01<00:06,  4.58it/s, loss=97.3, v_num=0, train_loss_step=107.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  22%|██▏       | 9/41 [00:01<00:06,  4.58it/s, loss=97.7, v_num=0, train_loss_step=105.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  24%|██▍       | 10/41 [00:02<00:07,  4.35it/s, loss=97.7, v_num=0, train_loss_step=105.0, val_loss=152.0, train_loss_epoch=105.0]#015Epoch 6:  24%|██▍       | 10/41 [00:02<00:07,  4.35it/s, loss=97.7, v_num=0, train_loss_step=105.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  24%|██▍       | 10/41 [00:02<00:07,  4.35it/s, loss=99.1, v_num=0, train_loss_step=107.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  27%|██▋       | 11/41 [00:02<00:06,  4.38it/s, loss=99.1, v_num=0, train_loss_step=107.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  27%|██▋       | 11/41 [00:02<00:06,  4.38it/s, loss=99.1, v_num=0, train_loss_step=107.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  27%|██▋       | 11/41 [00:02<00:06,  4.38it/s, loss=101, v_num=0, train_loss_step=119.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  29%|██▉       | 12/41 [00:02<00:06,  4.39it/s, loss=101, v_num=0, train_loss_step=119.0, val_loss=152.0, train_loss_epoch=105.0]#015Epoch 6:  29%|██▉       | 12/41 [00:02<00:06,  4.39it/s, loss=101, v_num=0, train_loss_step=119.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  29%|██▉       | 12/41 [00:02<00:06,  4.39it/s, loss=99.8, v_num=0, train_loss_step=94.70, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  32%|███▏      | 13/41 [00:02<00:06,  4.42it/s, loss=99.8, v_num=0, train_loss_step=94.70, val_loss=152.0, train_loss_epoch=105.0]#015Epoch 6:  32%|███▏      | 13/41 [00:02<00:06,  4.42it/s, loss=99.8, v_num=0, train_loss_step=94.70, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  32%|███▏      | 13/41 [00:02<00:06,  4.42it/s, loss=98.6, v_num=0, train_loss_step=112.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  34%|███▍      | 14/41 [00:03<00:06,  4.45it/s, loss=98.6, v_num=0, train_loss_step=112.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  34%|███▍      | 14/41 [00:03<00:06,  4.45it/s, loss=98.6, v_num=0, train_loss_step=112.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  34%|███▍      | 14/41 [00:03<00:06,  4.45it/s, loss=100, v_num=0, train_loss_step=120.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  37%|███▋      | 15/41 [00:03<00:05,  4.47it/s, loss=100, v_num=0, train_loss_step=120.0, val_loss=152.0, train_loss_epoch=105.0]#015Epoch 6:  37%|███▋      | 15/41 [00:03<00:05,  4.47it/s, loss=100, v_num=0, train_loss_step=120.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  37%|███▋      | 15/41 [00:03<00:05,  4.47it/s, loss=101, v_num=0, train_loss_step=105.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  39%|███▉      | 16/41 [00:03<00:05,  4.49it/s, loss=101, v_num=0, train_loss_step=105.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  39%|███▉      | 16/41 [00:03<00:05,  4.49it/s, loss=101, v_num=0, train_loss_step=105.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  39%|███▉      | 16/41 [00:03<00:05,  4.49it/s, loss=100, v_num=0, train_loss_step=69.90, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  41%|████▏     | 17/41 [00:03<00:05,  4.51it/s, loss=100, v_num=0, train_loss_step=69.90, val_loss=152.0, train_loss_epoch=105.0]#015Epoch 6:  41%|████▏     | 17/41 [00:03<00:05,  4.51it/s, loss=100, v_num=0, train_loss_step=69.90, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  41%|████▏     | 17/41 [00:03<00:05,  4.51it/s, loss=101, v_num=0, train_loss_step=111.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  44%|████▍     | 18/41 [00:03<00:05,  4.51it/s, loss=101, v_num=0, train_loss_step=111.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  44%|████▍     | 18/41 [00:03<00:05,  4.51it/s, loss=101, v_num=0, train_loss_step=111.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  44%|████▍     | 18/41 [00:03<00:05,  4.51it/s, loss=100, v_num=0, train_loss_step=87.20, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  46%|████▋     | 19/41 [00:04<00:04,  4.51it/s, loss=100, v_num=0, train_loss_step=87.20, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  46%|████▋     | 19/41 [00:04<00:04,  4.51it/s, loss=100, v_num=0, train_loss_step=87.20, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  46%|████▋     | 19/41 [00:04<00:04,  4.51it/s, loss=97.7, v_num=0, train_loss_step=88.80, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  49%|████▉     | 20/41 [00:04<00:04,  4.52it/s, loss=97.7, v_num=0, train_loss_step=88.80, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  49%|████▉     | 20/41 [00:04<00:04,  4.52it/s, loss=97.7, v_num=0, train_loss_step=88.80, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  49%|████▉     | 20/41 [00:04<00:04,  4.52it/s, loss=98.1, v_num=0, train_loss_step=98.30, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  51%|█████     | 21/41 [00:04<00:04,  4.53it/s, loss=98.1, v_num=0, train_loss_step=98.30, val_loss=152.0, train_loss_epoch=105.0]#015Epoch 6:  51%|█████     | 21/41 [00:04<00:04,  4.53it/s, loss=98.1, v_num=0, train_loss_step=98.30, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  51%|█████     | 21/41 [00:04<00:04,  4.53it/s, loss=99.6, v_num=0, train_loss_step=126.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  54%|█████▎    | 22/41 [00:04<00:04,  4.54it/s, loss=99.6, v_num=0, train_loss_step=126.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  54%|█████▎    | 22/41 [00:04<00:04,  4.54it/s, loss=99.6, v_num=0, train_loss_step=126.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  54%|█████▎    | 22/41 [00:04<00:04,  4.54it/s, loss=98.9, v_num=0, train_loss_step=84.60, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  56%|█████▌    | 23/41 [00:05<00:03,  4.55it/s, loss=98.9, v_num=0, train_loss_step=84.60, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  56%|█████▌    | 23/41 [00:05<00:03,  4.55it/s, loss=98.9, v_num=0, train_loss_step=84.60, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  56%|█████▌    | 23/41 [00:05<00:03,  4.55it/s, loss=99.3, v_num=0, train_loss_step=93.80, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  59%|█████▊    | 24/41 [00:05<00:03,  4.54it/s, loss=99.3, v_num=0, train_loss_step=93.80, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  59%|█████▊    | 24/41 [00:05<00:03,  4.53it/s, loss=99.3, v_num=0, train_loss_step=93.80, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  59%|█████▊    | 24/41 [00:05<00:03,  4.53it/s, loss=99.9, v_num=0, train_loss_step=98.30, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  61%|██████    | 25/41 [00:05<00:03,  4.54it/s, loss=99.9, v_num=0, train_loss_step=98.30, val_loss=152.0, train_loss_epoch=105.0]#015Epoch 6:  61%|██████    | 25/41 [00:05<00:03,  4.54it/s, loss=99.9, v_num=0, train_loss_step=98.30, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  61%|██████    | 25/41 [00:05<00:03,  4.54it/s, loss=101, v_num=0, train_loss_step=85.00, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  63%|██████▎   | 26/41 [00:05<00:03,  4.55it/s, loss=101, v_num=0, train_loss_step=85.00, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  63%|██████▎   | 26/41 [00:05<00:03,  4.55it/s, loss=101, v_num=0, train_loss_step=85.00, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  63%|██████▎   | 26/41 [00:05<00:03,  4.55it/s, loss=99, v_num=0, train_loss_step=74.50, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  66%|██████▌   | 27/41 [00:05<00:03,  4.55it/s, loss=99, v_num=0, train_loss_step=74.50, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  66%|██████▌   | 27/41 [00:05<00:03,  4.55it/s, loss=99, v_num=0, train_loss_step=74.50, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  66%|██████▌   | 27/41 [00:05<00:03,  4.55it/s, loss=102, v_num=0, train_loss_step=146.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  68%|██████▊   | 28/41 [00:06<00:02,  4.55it/s, loss=102, v_num=0, train_loss_step=146.0, val_loss=152.0, train_loss_epoch=105.0]#015Epoch 6:  68%|██████▊   | 28/41 [00:06<00:02,  4.55it/s, loss=102, v_num=0, train_loss_step=146.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  68%|██████▊   | 28/41 [00:06<00:02,  4.55it/s, loss=101, v_num=0, train_loss_step=86.50, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  71%|███████   | 29/41 [00:06<00:02,  4.56it/s, loss=101, v_num=0, train_loss_step=86.50, val_loss=152.0, train_loss_epoch=105.0]#015Epoch 6:  71%|███████   | 29/41 [00:06<00:02,  4.56it/s, loss=101, v_num=0, train_loss_step=86.50, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  71%|███████   | 29/41 [00:06<00:02,  4.56it/s, loss=102, v_num=0, train_loss_step=122.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  73%|███████▎  | 30/41 [00:06<00:02,  4.56it/s, loss=102, v_num=0, train_loss_step=122.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  73%|███████▎  | 30/41 [00:06<00:02,  4.56it/s, loss=102, v_num=0, train_loss_step=122.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  73%|███████▎  | 30/41 [00:06<00:02,  4.56it/s, loss=102, v_num=0, train_loss_step=125.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  76%|███████▌  | 31/41 [00:06<00:02,  4.53it/s, loss=102, v_num=0, train_loss_step=125.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  76%|███████▌  | 31/41 [00:06<00:02,  4.53it/s, loss=102, v_num=0, train_loss_step=125.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  76%|███████▌  | 31/41 [00:06<00:02,  4.53it/s, loss=101, v_num=0, train_loss_step=84.50, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  78%|███████▊  | 32/41 [00:07<00:02,  4.49it/s, loss=101, v_num=0, train_loss_step=84.50, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  78%|███████▊  | 32/41 [00:07<00:02,  4.49it/s, loss=101, v_num=0, train_loss_step=84.50, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  78%|███████▊  | 32/41 [00:07<00:02,  4.49it/s, loss=102, v_num=0, train_loss_step=128.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  80%|████████  | 33/41 [00:07<00:01,  4.48it/s, loss=102, v_num=0, train_loss_step=128.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  80%|████████  | 33/41 [00:07<00:01,  4.48it/s, loss=102, v_num=0, train_loss_step=128.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  80%|████████  | 33/41 [00:07<00:01,  4.48it/s, loss=101, v_num=0, train_loss_step=75.60, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  83%|████████▎ | 34/41 [00:07<00:01,  4.49it/s, loss=101, v_num=0, train_loss_step=75.60, val_loss=152.0, train_loss_epoch=105.0]#015Epoch 6:  83%|████████▎ | 34/41 [00:07<00:01,  4.49it/s, loss=101, v_num=0, train_loss_step=75.60, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  83%|████████▎ | 34/41 [00:07<00:01,  4.49it/s, loss=98.2, v_num=0, train_loss_step=72.90, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  85%|████████▌ | 35/41 [00:07<00:01,  4.50it/s, loss=98.2, v_num=0, train_loss_step=72.90, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  85%|████████▌ | 35/41 [00:07<00:01,  4.50it/s, loss=98.2, v_num=0, train_loss_step=72.90, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  85%|████████▌ | 35/41 [00:07<00:01,  4.50it/s, loss=97.5, v_num=0, train_loss_step=91.70, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  88%|████████▊ | 36/41 [00:07<00:01,  4.50it/s, loss=97.5, v_num=0, train_loss_step=91.70, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  88%|████████▊ | 36/41 [00:07<00:01,  4.50it/s, loss=97.5, v_num=0, train_loss_step=91.70, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  88%|████████▊ | 36/41 [00:07<00:01,  4.50it/s, loss=98.7, v_num=0, train_loss_step=94.10, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  90%|█████████ | 37/41 [00:08<00:00,  4.51it/s, loss=98.7, v_num=0, train_loss_step=94.10, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  90%|█████████ | 37/41 [00:08<00:00,  4.51it/s, loss=98.7, v_num=0, train_loss_step=94.10, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  90%|█████████ | 37/41 [00:08<00:00,  4.51it/s, loss=97.5, v_num=0, train_loss_step=88.40, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  93%|█████████▎| 38/41 [00:08<00:00,  4.52it/s, loss=97.5, v_num=0, train_loss_step=88.40, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  93%|█████████▎| 38/41 [00:08<00:00,  4.52it/s, loss=97.5, v_num=0, train_loss_step=88.40, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  93%|█████████▎| 38/41 [00:08<00:00,  4.52it/s, loss=99.2, v_num=0, train_loss_step=120.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  95%|█████████▌| 39/41 [00:08<00:00,  4.53it/s, loss=99.2, v_num=0, train_loss_step=120.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  95%|█████████▌| 39/41 [00:08<00:00,  4.53it/s, loss=99.2, v_num=0, train_loss_step=120.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  95%|█████████▌| 39/41 [00:08<00:00,  4.53it/s, loss=100, v_num=0, train_loss_step=105.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  98%|█████████▊| 40/41 [00:08<00:00,  4.53it/s, loss=100, v_num=0, train_loss_step=105.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  98%|█████████▊| 40/41 [00:08<00:00,  4.53it/s, loss=100, v_num=0, train_loss_step=105.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:  98%|█████████▊| 40/41 [00:08<00:00,  4.53it/s, loss=101, v_num=0, train_loss_step=121.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.69it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.69it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 6: 100%|██████████| 41/41 [00:09<00:00,  4.50it/s, loss=101, v_num=0, train_loss_step=121.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6: 100%|██████████| 41/41 [00:09<00:00,  4.50it/s, loss=101, v_num=0, train_loss_step=121.0, val_loss=152.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34mEpoch 6: 100%|██████████| 41/41 [00:09<00:00,  4.24it/s, loss=101, v_num=0, train_loss_step=121.0, val_loss=145.0, train_loss_epoch=105.0]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 6: 100%|██████████| 41/41 [00:09<00:00,  4.24it/s, loss=101, v_num=0, train_loss_step=121.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 6:   0%|          | 0/41 [00:00<?, ?it/s, loss=101, v_num=0, train_loss_step=121.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:   0%|          | 0/41 [00:00<?, ?it/s, loss=101, v_num=0, train_loss_step=121.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:   2%|▏         | 1/41 [00:00<00:08,  4.93it/s, loss=101, v_num=0, train_loss_step=121.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:   2%|▏         | 1/41 [00:00<00:08,  4.92it/s, loss=101, v_num=0, train_loss_step=121.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:   2%|▏         | 1/41 [00:00<00:08,  4.91it/s, loss=99.8, v_num=0, train_loss_step=99.50, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:   5%|▍         | 2/41 [00:00<00:08,  4.74it/s, loss=99.8, v_num=0, train_loss_step=99.50, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:   5%|▍         | 2/41 [00:00<00:08,  4.74it/s, loss=99.8, v_num=0, train_loss_step=99.50, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:   5%|▍         | 2/41 [00:00<00:08,  4.73it/s, loss=101, v_num=0, train_loss_step=101.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:   7%|▋         | 3/41 [00:00<00:07,  4.78it/s, loss=101, v_num=0, train_loss_step=101.0, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:   7%|▋         | 3/41 [00:00<00:07,  4.78it/s, loss=101, v_num=0, train_loss_step=101.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:   7%|▋         | 3/41 [00:00<00:07,  4.78it/s, loss=102, v_num=0, train_loss_step=114.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  10%|▉         | 4/41 [00:00<00:07,  4.80it/s, loss=102, v_num=0, train_loss_step=114.0, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:  10%|▉         | 4/41 [00:00<00:07,  4.80it/s, loss=102, v_num=0, train_loss_step=114.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  10%|▉         | 4/41 [00:00<00:07,  4.80it/s, loss=100, v_num=0, train_loss_step=74.30, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  12%|█▏        | 5/41 [00:01<00:07,  4.82it/s, loss=100, v_num=0, train_loss_step=74.30, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:  12%|█▏        | 5/41 [00:01<00:07,  4.82it/s, loss=100, v_num=0, train_loss_step=74.30, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  12%|█▏        | 5/41 [00:01<00:07,  4.81it/s, loss=102, v_num=0, train_loss_step=111.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  15%|█▍        | 6/41 [00:01<00:07,  4.82it/s, loss=102, v_num=0, train_loss_step=111.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  15%|█▍        | 6/41 [00:01<00:07,  4.82it/s, loss=102, v_num=0, train_loss_step=111.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  15%|█▍        | 6/41 [00:01<00:07,  4.82it/s, loss=104, v_num=0, train_loss_step=116.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  17%|█▋        | 7/41 [00:01<00:07,  4.82it/s, loss=104, v_num=0, train_loss_step=116.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  17%|█▋        | 7/41 [00:01<00:07,  4.82it/s, loss=104, v_num=0, train_loss_step=116.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  17%|█▋        | 7/41 [00:01<00:07,  4.82it/s, loss=101, v_num=0, train_loss_step=89.40, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  20%|█▉        | 8/41 [00:01<00:06,  4.78it/s, loss=101, v_num=0, train_loss_step=89.40, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  20%|█▉        | 8/41 [00:01<00:06,  4.78it/s, loss=101, v_num=0, train_loss_step=89.40, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  20%|█▉        | 8/41 [00:01<00:06,  4.78it/s, loss=102, v_num=0, train_loss_step=105.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  22%|██▏       | 9/41 [00:01<00:06,  4.79it/s, loss=102, v_num=0, train_loss_step=105.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  22%|██▏       | 9/41 [00:01<00:06,  4.79it/s, loss=102, v_num=0, train_loss_step=105.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  22%|██▏       | 9/41 [00:01<00:06,  4.79it/s, loss=100, v_num=0, train_loss_step=86.10, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  24%|██▍       | 10/41 [00:02<00:06,  4.79it/s, loss=100, v_num=0, train_loss_step=86.10, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:  24%|██▍       | 10/41 [00:02<00:06,  4.79it/s, loss=100, v_num=0, train_loss_step=86.10, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  24%|██▍       | 10/41 [00:02<00:06,  4.79it/s, loss=99, v_num=0, train_loss_step=102.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  27%|██▋       | 11/41 [00:02<00:06,  4.79it/s, loss=99, v_num=0, train_loss_step=102.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  27%|██▋       | 11/41 [00:02<00:06,  4.79it/s, loss=99, v_num=0, train_loss_step=102.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  27%|██▋       | 11/41 [00:02<00:06,  4.79it/s, loss=100, v_num=0, train_loss_step=106.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  29%|██▉       | 12/41 [00:02<00:06,  4.80it/s, loss=100, v_num=0, train_loss_step=106.0, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:  29%|██▉       | 12/41 [00:02<00:06,  4.80it/s, loss=100, v_num=0, train_loss_step=106.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  29%|██▉       | 12/41 [00:02<00:06,  4.80it/s, loss=98.8, v_num=0, train_loss_step=103.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  32%|███▏      | 13/41 [00:02<00:05,  4.79it/s, loss=98.8, v_num=0, train_loss_step=103.0, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:  32%|███▏      | 13/41 [00:02<00:05,  4.79it/s, loss=98.8, v_num=0, train_loss_step=103.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  32%|███▏      | 13/41 [00:02<00:05,  4.79it/s, loss=101, v_num=0, train_loss_step=119.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  34%|███▍      | 14/41 [00:02<00:05,  4.76it/s, loss=101, v_num=0, train_loss_step=119.0, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:  34%|███▍      | 14/41 [00:02<00:05,  4.76it/s, loss=101, v_num=0, train_loss_step=119.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  34%|███▍      | 14/41 [00:02<00:05,  4.76it/s, loss=102, v_num=0, train_loss_step=89.40, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  37%|███▋      | 15/41 [00:03<00:05,  4.76it/s, loss=102, v_num=0, train_loss_step=89.40, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:  37%|███▋      | 15/41 [00:03<00:05,  4.76it/s, loss=102, v_num=0, train_loss_step=89.40, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  37%|███▋      | 15/41 [00:03<00:05,  4.76it/s, loss=101, v_num=0, train_loss_step=72.80, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  39%|███▉      | 16/41 [00:03<00:05,  4.76it/s, loss=101, v_num=0, train_loss_step=72.80, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  39%|███▉      | 16/41 [00:03<00:05,  4.76it/s, loss=101, v_num=0, train_loss_step=72.80, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  39%|███▉      | 16/41 [00:03<00:05,  4.76it/s, loss=101, v_num=0, train_loss_step=93.70, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  41%|████▏     | 17/41 [00:03<00:05,  4.77it/s, loss=101, v_num=0, train_loss_step=93.70, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  41%|████▏     | 17/41 [00:03<00:05,  4.77it/s, loss=101, v_num=0, train_loss_step=93.70, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  41%|████▏     | 17/41 [00:03<00:05,  4.77it/s, loss=99.9, v_num=0, train_loss_step=70.90, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  44%|████▍     | 18/41 [00:03<00:04,  4.77it/s, loss=99.9, v_num=0, train_loss_step=70.90, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  44%|████▍     | 18/41 [00:03<00:04,  4.77it/s, loss=99.9, v_num=0, train_loss_step=70.90, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  44%|████▍     | 18/41 [00:03<00:04,  4.77it/s, loss=99.7, v_num=0, train_loss_step=116.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  46%|████▋     | 19/41 [00:03<00:04,  4.78it/s, loss=99.7, v_num=0, train_loss_step=116.0, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:  46%|████▋     | 19/41 [00:03<00:04,  4.78it/s, loss=99.7, v_num=0, train_loss_step=116.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  46%|████▋     | 19/41 [00:03<00:04,  4.78it/s, loss=100, v_num=0, train_loss_step=117.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  49%|████▉     | 20/41 [00:04<00:04,  4.76it/s, loss=100, v_num=0, train_loss_step=117.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  49%|████▉     | 20/41 [00:04<00:04,  4.76it/s, loss=100, v_num=0, train_loss_step=117.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  49%|████▉     | 20/41 [00:04<00:04,  4.76it/s, loss=98.9, v_num=0, train_loss_step=92.20, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  51%|█████     | 21/41 [00:04<00:04,  4.77it/s, loss=98.9, v_num=0, train_loss_step=92.20, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:  51%|█████     | 21/41 [00:04<00:04,  4.77it/s, loss=98.9, v_num=0, train_loss_step=92.20, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  51%|█████     | 21/41 [00:04<00:04,  4.77it/s, loss=98.7, v_num=0, train_loss_step=95.60, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  54%|█████▎    | 22/41 [00:04<00:03,  4.77it/s, loss=98.7, v_num=0, train_loss_step=95.60, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:  54%|█████▎    | 22/41 [00:04<00:03,  4.77it/s, loss=98.7, v_num=0, train_loss_step=95.60, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  54%|█████▎    | 22/41 [00:04<00:03,  4.77it/s, loss=99, v_num=0, train_loss_step=108.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  56%|█████▌    | 23/41 [00:04<00:03,  4.73it/s, loss=99, v_num=0, train_loss_step=108.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  56%|█████▌    | 23/41 [00:04<00:03,  4.73it/s, loss=99, v_num=0, train_loss_step=108.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  56%|█████▌    | 23/41 [00:04<00:03,  4.72it/s, loss=97.9, v_num=0, train_loss_step=91.20, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  59%|█████▊    | 24/41 [00:05<00:03,  4.70it/s, loss=97.9, v_num=0, train_loss_step=91.20, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:  59%|█████▊    | 24/41 [00:05<00:03,  4.70it/s, loss=97.9, v_num=0, train_loss_step=91.20, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  59%|█████▊    | 24/41 [00:05<00:03,  4.70it/s, loss=98.5, v_num=0, train_loss_step=87.20, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  61%|██████    | 25/41 [00:05<00:03,  4.70it/s, loss=98.5, v_num=0, train_loss_step=87.20, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  61%|██████    | 25/41 [00:05<00:03,  4.70it/s, loss=98.5, v_num=0, train_loss_step=87.20, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  61%|██████    | 25/41 [00:05<00:03,  4.70it/s, loss=98.4, v_num=0, train_loss_step=109.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  63%|██████▎   | 26/41 [00:05<00:03,  4.69it/s, loss=98.4, v_num=0, train_loss_step=109.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  63%|██████▎   | 26/41 [00:05<00:03,  4.69it/s, loss=98.4, v_num=0, train_loss_step=109.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  63%|██████▎   | 26/41 [00:05<00:03,  4.69it/s, loss=98.3, v_num=0, train_loss_step=114.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  66%|██████▌   | 27/41 [00:05<00:02,  4.69it/s, loss=98.3, v_num=0, train_loss_step=114.0, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:  66%|██████▌   | 27/41 [00:05<00:02,  4.69it/s, loss=98.3, v_num=0, train_loss_step=114.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  66%|██████▌   | 27/41 [00:05<00:02,  4.69it/s, loss=98.9, v_num=0, train_loss_step=100.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  68%|██████▊   | 28/41 [00:05<00:02,  4.70it/s, loss=98.9, v_num=0, train_loss_step=100.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  68%|██████▊   | 28/41 [00:05<00:02,  4.70it/s, loss=98.9, v_num=0, train_loss_step=100.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  68%|██████▊   | 28/41 [00:05<00:02,  4.70it/s, loss=97.8, v_num=0, train_loss_step=84.70, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  71%|███████   | 29/41 [00:06<00:02,  4.70it/s, loss=97.8, v_num=0, train_loss_step=84.70, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  71%|███████   | 29/41 [00:06<00:02,  4.70it/s, loss=97.8, v_num=0, train_loss_step=84.70, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  71%|███████   | 29/41 [00:06<00:02,  4.70it/s, loss=98.4, v_num=0, train_loss_step=98.20, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  73%|███████▎  | 30/41 [00:06<00:02,  4.71it/s, loss=98.4, v_num=0, train_loss_step=98.20, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  73%|███████▎  | 30/41 [00:06<00:02,  4.71it/s, loss=98.4, v_num=0, train_loss_step=98.20, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  73%|███████▎  | 30/41 [00:06<00:02,  4.71it/s, loss=98.8, v_num=0, train_loss_step=109.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  76%|███████▌  | 31/41 [00:06<00:02,  4.71it/s, loss=98.8, v_num=0, train_loss_step=109.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  76%|███████▌  | 31/41 [00:06<00:02,  4.71it/s, loss=98.8, v_num=0, train_loss_step=109.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  76%|███████▌  | 31/41 [00:06<00:02,  4.70it/s, loss=98.2, v_num=0, train_loss_step=94.80, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  78%|███████▊  | 32/41 [00:06<00:01,  4.70it/s, loss=98.2, v_num=0, train_loss_step=94.80, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  78%|███████▊  | 32/41 [00:06<00:01,  4.70it/s, loss=98.2, v_num=0, train_loss_step=94.80, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  78%|███████▊  | 32/41 [00:06<00:01,  4.70it/s, loss=98.2, v_num=0, train_loss_step=103.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  80%|████████  | 33/41 [00:07<00:01,  4.70it/s, loss=98.2, v_num=0, train_loss_step=103.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  80%|████████  | 33/41 [00:07<00:01,  4.70it/s, loss=98.2, v_num=0, train_loss_step=103.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  80%|████████  | 33/41 [00:07<00:01,  4.70it/s, loss=97.5, v_num=0, train_loss_step=105.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  83%|████████▎ | 34/41 [00:07<00:01,  4.67it/s, loss=97.5, v_num=0, train_loss_step=105.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  83%|████████▎ | 34/41 [00:07<00:01,  4.67it/s, loss=97.5, v_num=0, train_loss_step=105.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  83%|████████▎ | 34/41 [00:07<00:01,  4.67it/s, loss=96.9, v_num=0, train_loss_step=76.30, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  85%|████████▌ | 35/41 [00:07<00:01,  4.66it/s, loss=96.9, v_num=0, train_loss_step=76.30, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  85%|████████▌ | 35/41 [00:07<00:01,  4.66it/s, loss=96.9, v_num=0, train_loss_step=76.30, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  85%|████████▌ | 35/41 [00:07<00:01,  4.66it/s, loss=98, v_num=0, train_loss_step=96.10, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  88%|████████▊ | 36/41 [00:07<00:01,  4.66it/s, loss=98, v_num=0, train_loss_step=96.10, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  88%|████████▊ | 36/41 [00:07<00:01,  4.66it/s, loss=98, v_num=0, train_loss_step=96.10, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  88%|████████▊ | 36/41 [00:07<00:01,  4.66it/s, loss=98.8, v_num=0, train_loss_step=109.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  90%|█████████ | 37/41 [00:07<00:00,  4.67it/s, loss=98.8, v_num=0, train_loss_step=109.0, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:  90%|█████████ | 37/41 [00:07<00:00,  4.67it/s, loss=98.8, v_num=0, train_loss_step=109.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  90%|█████████ | 37/41 [00:07<00:00,  4.67it/s, loss=100, v_num=0, train_loss_step=103.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  93%|█████████▎| 38/41 [00:08<00:00,  4.66it/s, loss=100, v_num=0, train_loss_step=103.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  93%|█████████▎| 38/41 [00:08<00:00,  4.66it/s, loss=100, v_num=0, train_loss_step=103.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  93%|█████████▎| 38/41 [00:08<00:00,  4.66it/s, loss=99.8, v_num=0, train_loss_step=103.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  95%|█████████▌| 39/41 [00:08<00:00,  4.67it/s, loss=99.8, v_num=0, train_loss_step=103.0, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:  95%|█████████▌| 39/41 [00:08<00:00,  4.67it/s, loss=99.8, v_num=0, train_loss_step=103.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  95%|█████████▌| 39/41 [00:08<00:00,  4.67it/s, loss=98.5, v_num=0, train_loss_step=90.90, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  98%|█████████▊| 40/41 [00:08<00:00,  4.67it/s, loss=98.5, v_num=0, train_loss_step=90.90, val_loss=145.0, train_loss_epoch=101.0]#015Epoch 7:  98%|█████████▊| 40/41 [00:08<00:00,  4.67it/s, loss=98.5, v_num=0, train_loss_step=90.90, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7:  98%|█████████▊| 40/41 [00:08<00:00,  4.67it/s, loss=99.4, v_num=0, train_loss_step=110.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.58it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.58it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 7: 100%|██████████| 41/41 [00:08<00:00,  4.63it/s, loss=99.4, v_num=0, train_loss_step=110.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7: 100%|██████████| 41/41 [00:08<00:00,  4.63it/s, loss=99.4, v_num=0, train_loss_step=110.0, val_loss=145.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34mEpoch 7: 100%|██████████| 41/41 [00:09<00:00,  4.28it/s, loss=99.4, v_num=0, train_loss_step=110.0, val_loss=148.0, train_loss_epoch=101.0]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 7: 100%|██████████| 41/41 [00:09<00:00,  4.28it/s, loss=99.4, v_num=0, train_loss_step=110.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 7:   0%|          | 0/41 [00:00<?, ?it/s, loss=99.4, v_num=0, train_loss_step=110.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:   0%|          | 0/41 [00:00<?, ?it/s, loss=99.4, v_num=0, train_loss_step=110.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:   2%|▏         | 1/41 [00:00<00:08,  4.91it/s, loss=99.4, v_num=0, train_loss_step=110.0, val_loss=148.0, train_loss_epoch=97.60]#015Epoch 8:   2%|▏         | 1/41 [00:00<00:08,  4.90it/s, loss=99.4, v_num=0, train_loss_step=110.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:   2%|▏         | 1/41 [00:00<00:08,  4.89it/s, loss=99.8, v_num=0, train_loss_step=104.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:   5%|▍         | 2/41 [00:00<00:07,  4.89it/s, loss=99.8, v_num=0, train_loss_step=104.0, val_loss=148.0, train_loss_epoch=97.60]#015Epoch 8:   5%|▍         | 2/41 [00:00<00:07,  4.89it/s, loss=99.8, v_num=0, train_loss_step=104.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:   5%|▍         | 2/41 [00:00<00:07,  4.88it/s, loss=100, v_num=0, train_loss_step=120.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:   7%|▋         | 3/41 [00:00<00:07,  4.83it/s, loss=100, v_num=0, train_loss_step=120.0, val_loss=148.0, train_loss_epoch=97.60]#015Epoch 8:   7%|▋         | 3/41 [00:00<00:07,  4.82it/s, loss=100, v_num=0, train_loss_step=120.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:   7%|▋         | 3/41 [00:00<00:07,  4.82it/s, loss=99.2, v_num=0, train_loss_step=66.80, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  10%|▉         | 4/41 [00:00<00:07,  4.74it/s, loss=99.2, v_num=0, train_loss_step=66.80, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  10%|▉         | 4/41 [00:00<00:07,  4.74it/s, loss=99.2, v_num=0, train_loss_step=66.80, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  10%|▉         | 4/41 [00:00<00:07,  4.74it/s, loss=100, v_num=0, train_loss_step=105.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  12%|█▏        | 5/41 [00:01<00:07,  4.76it/s, loss=100, v_num=0, train_loss_step=105.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  12%|█▏        | 5/41 [00:01<00:07,  4.76it/s, loss=100, v_num=0, train_loss_step=105.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  12%|█▏        | 5/41 [00:01<00:07,  4.76it/s, loss=99.7, v_num=0, train_loss_step=101.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  15%|█▍        | 6/41 [00:01<00:07,  4.78it/s, loss=99.7, v_num=0, train_loss_step=101.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  15%|█▍        | 6/41 [00:01<00:07,  4.78it/s, loss=99.7, v_num=0, train_loss_step=101.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  15%|█▍        | 6/41 [00:01<00:07,  4.78it/s, loss=98.5, v_num=0, train_loss_step=90.50, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  17%|█▋        | 7/41 [00:01<00:07,  4.80it/s, loss=98.5, v_num=0, train_loss_step=90.50, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  17%|█▋        | 7/41 [00:01<00:07,  4.80it/s, loss=98.5, v_num=0, train_loss_step=90.50, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  17%|█▋        | 7/41 [00:01<00:07,  4.80it/s, loss=96.9, v_num=0, train_loss_step=68.30, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  20%|█▉        | 8/41 [00:01<00:06,  4.80it/s, loss=96.9, v_num=0, train_loss_step=68.30, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  20%|█▉        | 8/41 [00:01<00:06,  4.80it/s, loss=96.9, v_num=0, train_loss_step=68.30, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  20%|█▉        | 8/41 [00:01<00:06,  4.80it/s, loss=97.1, v_num=0, train_loss_step=88.00, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  22%|██▏       | 9/41 [00:01<00:06,  4.81it/s, loss=97.1, v_num=0, train_loss_step=88.00, val_loss=148.0, train_loss_epoch=97.60]#015Epoch 8:  22%|██▏       | 9/41 [00:01<00:06,  4.81it/s, loss=97.1, v_num=0, train_loss_step=88.00, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  22%|██▏       | 9/41 [00:01<00:06,  4.81it/s, loss=96.8, v_num=0, train_loss_step=91.70, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  24%|██▍       | 10/41 [00:02<00:06,  4.78it/s, loss=96.8, v_num=0, train_loss_step=91.70, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  24%|██▍       | 10/41 [00:02<00:06,  4.78it/s, loss=96.8, v_num=0, train_loss_step=91.70, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  24%|██▍       | 10/41 [00:02<00:06,  4.78it/s, loss=97.1, v_num=0, train_loss_step=115.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  27%|██▋       | 11/41 [00:02<00:06,  4.79it/s, loss=97.1, v_num=0, train_loss_step=115.0, val_loss=148.0, train_loss_epoch=97.60]#015Epoch 8:  27%|██▋       | 11/41 [00:02<00:06,  4.79it/s, loss=97.1, v_num=0, train_loss_step=115.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  27%|██▋       | 11/41 [00:02<00:06,  4.78it/s, loss=97.4, v_num=0, train_loss_step=101.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  29%|██▉       | 12/41 [00:02<00:06,  4.79it/s, loss=97.4, v_num=0, train_loss_step=101.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  29%|██▉       | 12/41 [00:02<00:06,  4.79it/s, loss=97.4, v_num=0, train_loss_step=101.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  29%|██▉       | 12/41 [00:02<00:06,  4.78it/s, loss=97.5, v_num=0, train_loss_step=105.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  32%|███▏      | 13/41 [00:02<00:05,  4.79it/s, loss=97.5, v_num=0, train_loss_step=105.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  32%|███▏      | 13/41 [00:02<00:05,  4.79it/s, loss=97.5, v_num=0, train_loss_step=105.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  32%|███▏      | 13/41 [00:02<00:05,  4.79it/s, loss=96.6, v_num=0, train_loss_step=87.00, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  34%|███▍      | 14/41 [00:02<00:05,  4.79it/s, loss=96.6, v_num=0, train_loss_step=87.00, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  34%|███▍      | 14/41 [00:02<00:05,  4.79it/s, loss=96.6, v_num=0, train_loss_step=87.00, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  34%|███▍      | 14/41 [00:02<00:05,  4.79it/s, loss=97.2, v_num=0, train_loss_step=87.90, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  37%|███▋      | 15/41 [00:03<00:05,  4.80it/s, loss=97.2, v_num=0, train_loss_step=87.90, val_loss=148.0, train_loss_epoch=97.60]#015Epoch 8:  37%|███▋      | 15/41 [00:03<00:05,  4.80it/s, loss=97.2, v_num=0, train_loss_step=87.90, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  37%|███▋      | 15/41 [00:03<00:05,  4.80it/s, loss=96.9, v_num=0, train_loss_step=89.70, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  39%|███▉      | 16/41 [00:03<00:05,  4.78it/s, loss=96.9, v_num=0, train_loss_step=89.70, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  39%|███▉      | 16/41 [00:03<00:05,  4.78it/s, loss=96.9, v_num=0, train_loss_step=89.70, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  39%|███▉      | 16/41 [00:03<00:05,  4.78it/s, loss=96.3, v_num=0, train_loss_step=97.00, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  41%|████▏     | 17/41 [00:03<00:05,  4.79it/s, loss=96.3, v_num=0, train_loss_step=97.00, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  41%|████▏     | 17/41 [00:03<00:05,  4.79it/s, loss=96.3, v_num=0, train_loss_step=97.00, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  41%|████▏     | 17/41 [00:03<00:05,  4.79it/s, loss=96.9, v_num=0, train_loss_step=115.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  44%|████▍     | 18/41 [00:03<00:04,  4.79it/s, loss=96.9, v_num=0, train_loss_step=115.0, val_loss=148.0, train_loss_epoch=97.60]#015Epoch 8:  44%|████▍     | 18/41 [00:03<00:04,  4.79it/s, loss=96.9, v_num=0, train_loss_step=115.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  44%|████▍     | 18/41 [00:03<00:04,  4.79it/s, loss=96, v_num=0, train_loss_step=85.10, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  46%|████▋     | 19/41 [00:03<00:04,  4.80it/s, loss=96, v_num=0, train_loss_step=85.10, val_loss=148.0, train_loss_epoch=97.60]#015Epoch 8:  46%|████▋     | 19/41 [00:03<00:04,  4.79it/s, loss=96, v_num=0, train_loss_step=85.10, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  46%|████▋     | 19/41 [00:03<00:04,  4.79it/s, loss=96.7, v_num=0, train_loss_step=105.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  49%|████▉     | 20/41 [00:04<00:04,  4.80it/s, loss=96.7, v_num=0, train_loss_step=105.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  49%|████▉     | 20/41 [00:04<00:04,  4.80it/s, loss=96.7, v_num=0, train_loss_step=105.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  49%|████▉     | 20/41 [00:04<00:04,  4.80it/s, loss=96, v_num=0, train_loss_step=98.00, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  51%|█████     | 21/41 [00:04<00:04,  4.80it/s, loss=96, v_num=0, train_loss_step=98.00, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  51%|█████     | 21/41 [00:04<00:04,  4.80it/s, loss=96, v_num=0, train_loss_step=98.00, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  51%|█████     | 21/41 [00:04<00:04,  4.80it/s, loss=95.6, v_num=0, train_loss_step=96.30, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  54%|█████▎    | 22/41 [00:04<00:03,  4.79it/s, loss=95.6, v_num=0, train_loss_step=96.30, val_loss=148.0, train_loss_epoch=97.60]#015Epoch 8:  54%|█████▎    | 22/41 [00:04<00:03,  4.79it/s, loss=95.6, v_num=0, train_loss_step=96.30, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  54%|█████▎    | 22/41 [00:04<00:03,  4.79it/s, loss=94.2, v_num=0, train_loss_step=91.80, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  56%|█████▌    | 23/41 [00:04<00:03,  4.79it/s, loss=94.2, v_num=0, train_loss_step=91.80, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  56%|█████▌    | 23/41 [00:04<00:03,  4.79it/s, loss=94.2, v_num=0, train_loss_step=91.80, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  56%|█████▌    | 23/41 [00:04<00:03,  4.79it/s, loss=95.3, v_num=0, train_loss_step=87.50, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  59%|█████▊    | 24/41 [00:05<00:03,  4.80it/s, loss=95.3, v_num=0, train_loss_step=87.50, val_loss=148.0, train_loss_epoch=97.60]#015Epoch 8:  59%|█████▊    | 24/41 [00:05<00:03,  4.80it/s, loss=95.3, v_num=0, train_loss_step=87.50, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  59%|█████▊    | 24/41 [00:05<00:03,  4.80it/s, loss=94.7, v_num=0, train_loss_step=92.70, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  61%|██████    | 25/41 [00:05<00:03,  4.80it/s, loss=94.7, v_num=0, train_loss_step=92.70, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  61%|██████    | 25/41 [00:05<00:03,  4.80it/s, loss=94.7, v_num=0, train_loss_step=92.70, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  61%|██████    | 25/41 [00:05<00:03,  4.80it/s, loss=93.9, v_num=0, train_loss_step=86.10, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  63%|██████▎   | 26/41 [00:05<00:03,  4.79it/s, loss=93.9, v_num=0, train_loss_step=86.10, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  63%|██████▎   | 26/41 [00:05<00:03,  4.79it/s, loss=93.9, v_num=0, train_loss_step=86.10, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  63%|██████▎   | 26/41 [00:05<00:03,  4.79it/s, loss=94.5, v_num=0, train_loss_step=102.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  66%|██████▌   | 27/41 [00:05<00:02,  4.78it/s, loss=94.5, v_num=0, train_loss_step=102.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  66%|██████▌   | 27/41 [00:05<00:02,  4.78it/s, loss=94.5, v_num=0, train_loss_step=102.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  66%|██████▌   | 27/41 [00:05<00:02,  4.78it/s, loss=95.8, v_num=0, train_loss_step=94.30, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  68%|██████▊   | 28/41 [00:05<00:02,  4.73it/s, loss=95.8, v_num=0, train_loss_step=94.30, val_loss=148.0, train_loss_epoch=97.60]#015Epoch 8:  68%|██████▊   | 28/41 [00:05<00:02,  4.73it/s, loss=95.8, v_num=0, train_loss_step=94.30, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  68%|██████▊   | 28/41 [00:05<00:02,  4.73it/s, loss=95.6, v_num=0, train_loss_step=84.80, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  71%|███████   | 29/41 [00:06<00:02,  4.73it/s, loss=95.6, v_num=0, train_loss_step=84.80, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  71%|███████   | 29/41 [00:06<00:02,  4.73it/s, loss=95.6, v_num=0, train_loss_step=84.80, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  71%|███████   | 29/41 [00:06<00:02,  4.73it/s, loss=96, v_num=0, train_loss_step=98.30, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  73%|███████▎  | 30/41 [00:06<00:02,  4.72it/s, loss=96, v_num=0, train_loss_step=98.30, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  73%|███████▎  | 30/41 [00:06<00:02,  4.72it/s, loss=96, v_num=0, train_loss_step=98.30, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  73%|███████▎  | 30/41 [00:06<00:02,  4.72it/s, loss=95.3, v_num=0, train_loss_step=101.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  76%|███████▌  | 31/41 [00:06<00:02,  4.71it/s, loss=95.3, v_num=0, train_loss_step=101.0, val_loss=148.0, train_loss_epoch=97.60]#015Epoch 8:  76%|███████▌  | 31/41 [00:06<00:02,  4.71it/s, loss=95.3, v_num=0, train_loss_step=101.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  76%|███████▌  | 31/41 [00:06<00:02,  4.71it/s, loss=94.6, v_num=0, train_loss_step=87.20, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  78%|███████▊  | 32/41 [00:06<00:01,  4.70it/s, loss=94.6, v_num=0, train_loss_step=87.20, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  78%|███████▊  | 32/41 [00:06<00:01,  4.70it/s, loss=94.6, v_num=0, train_loss_step=87.20, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  78%|███████▊  | 32/41 [00:06<00:01,  4.70it/s, loss=94.1, v_num=0, train_loss_step=96.40, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  80%|████████  | 33/41 [00:07<00:01,  4.70it/s, loss=94.1, v_num=0, train_loss_step=96.40, val_loss=148.0, train_loss_epoch=97.60]#015Epoch 8:  80%|████████  | 33/41 [00:07<00:01,  4.70it/s, loss=94.1, v_num=0, train_loss_step=96.40, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  80%|████████  | 33/41 [00:07<00:01,  4.70it/s, loss=95, v_num=0, train_loss_step=104.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  83%|████████▎ | 34/41 [00:07<00:01,  4.68it/s, loss=95, v_num=0, train_loss_step=104.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  83%|████████▎ | 34/41 [00:07<00:01,  4.68it/s, loss=95, v_num=0, train_loss_step=104.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  83%|████████▎ | 34/41 [00:07<00:01,  4.68it/s, loss=94.8, v_num=0, train_loss_step=83.90, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  85%|████████▌ | 35/41 [00:07<00:01,  4.67it/s, loss=94.8, v_num=0, train_loss_step=83.90, val_loss=148.0, train_loss_epoch=97.60]#015Epoch 8:  85%|████████▌ | 35/41 [00:07<00:01,  4.67it/s, loss=94.8, v_num=0, train_loss_step=83.90, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  85%|████████▌ | 35/41 [00:07<00:01,  4.67it/s, loss=94, v_num=0, train_loss_step=72.90, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  88%|████████▊ | 36/41 [00:07<00:01,  4.67it/s, loss=94, v_num=0, train_loss_step=72.90, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  88%|████████▊ | 36/41 [00:07<00:01,  4.67it/s, loss=94, v_num=0, train_loss_step=72.90, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  88%|████████▊ | 36/41 [00:07<00:01,  4.67it/s, loss=93.7, v_num=0, train_loss_step=91.90, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  90%|█████████ | 37/41 [00:07<00:00,  4.66it/s, loss=93.7, v_num=0, train_loss_step=91.90, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  90%|█████████ | 37/41 [00:07<00:00,  4.66it/s, loss=93.7, v_num=0, train_loss_step=91.90, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  90%|█████████ | 37/41 [00:07<00:00,  4.66it/s, loss=91.8, v_num=0, train_loss_step=76.60, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  93%|█████████▎| 38/41 [00:08<00:00,  4.66it/s, loss=91.8, v_num=0, train_loss_step=76.60, val_loss=148.0, train_loss_epoch=97.60]#015Epoch 8:  93%|█████████▎| 38/41 [00:08<00:00,  4.66it/s, loss=91.8, v_num=0, train_loss_step=76.60, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  93%|█████████▎| 38/41 [00:08<00:00,  4.66it/s, loss=92.9, v_num=0, train_loss_step=107.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  95%|█████████▌| 39/41 [00:08<00:00,  4.66it/s, loss=92.9, v_num=0, train_loss_step=107.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  95%|█████████▌| 39/41 [00:08<00:00,  4.65it/s, loss=92.9, v_num=0, train_loss_step=107.0, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  95%|█████████▌| 39/41 [00:08<00:00,  4.65it/s, loss=92.3, v_num=0, train_loss_step=93.60, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  98%|█████████▊| 40/41 [00:08<00:00,  4.64it/s, loss=92.3, v_num=0, train_loss_step=93.60, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  98%|█████████▊| 40/41 [00:08<00:00,  4.64it/s, loss=92.3, v_num=0, train_loss_step=93.60, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8:  98%|█████████▊| 40/41 [00:08<00:00,  4.64it/s, loss=91.7, v_num=0, train_loss_step=84.70, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.48it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.48it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 8: 100%|██████████| 41/41 [00:08<00:00,  4.60it/s, loss=91.7, v_num=0, train_loss_step=84.70, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8: 100%|██████████| 41/41 [00:08<00:00,  4.60it/s, loss=91.7, v_num=0, train_loss_step=84.70, val_loss=148.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34mEpoch 8: 100%|██████████| 41/41 [00:09<00:00,  4.33it/s, loss=91.7, v_num=0, train_loss_step=84.70, val_loss=146.0, train_loss_epoch=97.60]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 8: 100%|██████████| 41/41 [00:09<00:00,  4.33it/s, loss=91.7, v_num=0, train_loss_step=84.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 8:   0%|          | 0/41 [00:00<?, ?it/s, loss=91.7, v_num=0, train_loss_step=84.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:   0%|          | 0/41 [00:00<?, ?it/s, loss=91.7, v_num=0, train_loss_step=84.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:   2%|▏         | 1/41 [00:00<00:08,  4.88it/s, loss=91.7, v_num=0, train_loss_step=84.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:   2%|▏         | 1/41 [00:00<00:08,  4.87it/s, loss=91.7, v_num=0, train_loss_step=84.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:   2%|▏         | 1/41 [00:00<00:08,  4.87it/s, loss=90.6, v_num=0, train_loss_step=74.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:   5%|▍         | 2/41 [00:00<00:08,  4.79it/s, loss=90.6, v_num=0, train_loss_step=74.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:   5%|▍         | 2/41 [00:00<00:08,  4.79it/s, loss=90.6, v_num=0, train_loss_step=74.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:   5%|▍         | 2/41 [00:00<00:08,  4.78it/s, loss=90.6, v_num=0, train_loss_step=93.10, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:   7%|▋         | 3/41 [00:00<00:07,  4.78it/s, loss=90.6, v_num=0, train_loss_step=93.10, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:   7%|▋         | 3/41 [00:00<00:07,  4.78it/s, loss=90.6, v_num=0, train_loss_step=93.10, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:   7%|▋         | 3/41 [00:00<00:07,  4.77it/s, loss=90.4, v_num=0, train_loss_step=82.30, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  10%|▉         | 4/41 [00:00<00:07,  4.78it/s, loss=90.4, v_num=0, train_loss_step=82.30, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  10%|▉         | 4/41 [00:00<00:07,  4.77it/s, loss=90.4, v_num=0, train_loss_step=82.30, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  10%|▉         | 4/41 [00:00<00:07,  4.77it/s, loss=91.1, v_num=0, train_loss_step=108.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  12%|█▏        | 5/41 [00:01<00:07,  4.60it/s, loss=91.1, v_num=0, train_loss_step=108.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  12%|█▏        | 5/41 [00:01<00:07,  4.60it/s, loss=91.1, v_num=0, train_loss_step=108.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  12%|█▏        | 5/41 [00:01<00:07,  4.60it/s, loss=91.4, v_num=0, train_loss_step=90.40, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  15%|█▍        | 6/41 [00:01<00:07,  4.59it/s, loss=91.4, v_num=0, train_loss_step=90.40, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  15%|█▍        | 6/41 [00:01<00:07,  4.59it/s, loss=91.4, v_num=0, train_loss_step=90.40, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  15%|█▍        | 6/41 [00:01<00:07,  4.59it/s, loss=90.8, v_num=0, train_loss_step=89.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  17%|█▋        | 7/41 [00:01<00:07,  4.62it/s, loss=90.8, v_num=0, train_loss_step=89.70, val_loss=146.0, train_loss_epoch=95.20]#015Epoch 9:  17%|█▋        | 7/41 [00:01<00:07,  4.61it/s, loss=90.8, v_num=0, train_loss_step=89.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  17%|█▋        | 7/41 [00:01<00:07,  4.61it/s, loss=90.9, v_num=0, train_loss_step=96.20, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  20%|█▉        | 8/41 [00:01<00:07,  4.64it/s, loss=90.9, v_num=0, train_loss_step=96.20, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  20%|█▉        | 8/41 [00:01<00:07,  4.64it/s, loss=90.9, v_num=0, train_loss_step=96.20, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  20%|█▉        | 8/41 [00:01<00:07,  4.64it/s, loss=91.2, v_num=0, train_loss_step=92.30, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  22%|██▏       | 9/41 [00:01<00:06,  4.66it/s, loss=91.2, v_num=0, train_loss_step=92.30, val_loss=146.0, train_loss_epoch=95.20]#015Epoch 9:  22%|██▏       | 9/41 [00:01<00:06,  4.66it/s, loss=91.2, v_num=0, train_loss_step=92.30, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  22%|██▏       | 9/41 [00:01<00:06,  4.66it/s, loss=90.3, v_num=0, train_loss_step=78.90, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  24%|██▍       | 10/41 [00:02<00:06,  4.68it/s, loss=90.3, v_num=0, train_loss_step=78.90, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  24%|██▍       | 10/41 [00:02<00:06,  4.68it/s, loss=90.3, v_num=0, train_loss_step=78.90, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  24%|██▍       | 10/41 [00:02<00:06,  4.68it/s, loss=90.2, v_num=0, train_loss_step=99.10, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  27%|██▋       | 11/41 [00:02<00:06,  4.69it/s, loss=90.2, v_num=0, train_loss_step=99.10, val_loss=146.0, train_loss_epoch=95.20]#015Epoch 9:  27%|██▋       | 11/41 [00:02<00:06,  4.69it/s, loss=90.2, v_num=0, train_loss_step=99.10, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  27%|██▋       | 11/41 [00:02<00:06,  4.69it/s, loss=90.6, v_num=0, train_loss_step=95.60, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  29%|██▉       | 12/41 [00:02<00:06,  4.66it/s, loss=90.6, v_num=0, train_loss_step=95.60, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  29%|██▉       | 12/41 [00:02<00:06,  4.66it/s, loss=90.6, v_num=0, train_loss_step=95.60, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  29%|██▉       | 12/41 [00:02<00:06,  4.66it/s, loss=91.5, v_num=0, train_loss_step=114.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  32%|███▏      | 13/41 [00:02<00:05,  4.67it/s, loss=91.5, v_num=0, train_loss_step=114.0, val_loss=146.0, train_loss_epoch=95.20]#015Epoch 9:  32%|███▏      | 13/41 [00:02<00:06,  4.67it/s, loss=91.5, v_num=0, train_loss_step=114.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  32%|███▏      | 13/41 [00:02<00:06,  4.67it/s, loss=90.9, v_num=0, train_loss_step=93.10, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  34%|███▍      | 14/41 [00:02<00:05,  4.68it/s, loss=90.9, v_num=0, train_loss_step=93.10, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  34%|███▍      | 14/41 [00:02<00:05,  4.68it/s, loss=90.9, v_num=0, train_loss_step=93.10, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  34%|███▍      | 14/41 [00:02<00:05,  4.68it/s, loss=91.4, v_num=0, train_loss_step=94.40, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  37%|███▋      | 15/41 [00:03<00:05,  4.68it/s, loss=91.4, v_num=0, train_loss_step=94.40, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  37%|███▋      | 15/41 [00:03<00:05,  4.68it/s, loss=91.4, v_num=0, train_loss_step=94.40, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  37%|███▋      | 15/41 [00:03<00:05,  4.68it/s, loss=93.3, v_num=0, train_loss_step=110.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  39%|███▉      | 16/41 [00:03<00:05,  4.69it/s, loss=93.3, v_num=0, train_loss_step=110.0, val_loss=146.0, train_loss_epoch=95.20]#015Epoch 9:  39%|███▉      | 16/41 [00:03<00:05,  4.69it/s, loss=93.3, v_num=0, train_loss_step=110.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  39%|███▉      | 16/41 [00:03<00:05,  4.69it/s, loss=93.4, v_num=0, train_loss_step=95.00, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  41%|████▏     | 17/41 [00:03<00:05,  4.70it/s, loss=93.4, v_num=0, train_loss_step=95.00, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  41%|████▏     | 17/41 [00:03<00:05,  4.70it/s, loss=93.4, v_num=0, train_loss_step=95.00, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  41%|████▏     | 17/41 [00:03<00:05,  4.70it/s, loss=94.7, v_num=0, train_loss_step=102.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  44%|████▍     | 18/41 [00:03<00:04,  4.69it/s, loss=94.7, v_num=0, train_loss_step=102.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  44%|████▍     | 18/41 [00:03<00:04,  4.69it/s, loss=94.7, v_num=0, train_loss_step=102.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  44%|████▍     | 18/41 [00:03<00:04,  4.69it/s, loss=94.6, v_num=0, train_loss_step=104.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  46%|████▋     | 19/41 [00:04<00:04,  4.70it/s, loss=94.6, v_num=0, train_loss_step=104.0, val_loss=146.0, train_loss_epoch=95.20]#015Epoch 9:  46%|████▋     | 19/41 [00:04<00:04,  4.70it/s, loss=94.6, v_num=0, train_loss_step=104.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  46%|████▋     | 19/41 [00:04<00:04,  4.70it/s, loss=95.3, v_num=0, train_loss_step=109.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  49%|████▉     | 20/41 [00:04<00:04,  4.70it/s, loss=95.3, v_num=0, train_loss_step=109.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  49%|████▉     | 20/41 [00:04<00:04,  4.70it/s, loss=95.3, v_num=0, train_loss_step=109.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  49%|████▉     | 20/41 [00:04<00:04,  4.70it/s, loss=96, v_num=0, train_loss_step=97.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  51%|█████     | 21/41 [00:04<00:04,  4.71it/s, loss=96, v_num=0, train_loss_step=97.70, val_loss=146.0, train_loss_epoch=95.20]#015Epoch 9:  51%|█████     | 21/41 [00:04<00:04,  4.71it/s, loss=96, v_num=0, train_loss_step=97.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  51%|█████     | 21/41 [00:04<00:04,  4.71it/s, loss=98, v_num=0, train_loss_step=115.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  54%|█████▎    | 22/41 [00:04<00:04,  4.71it/s, loss=98, v_num=0, train_loss_step=115.0, val_loss=146.0, train_loss_epoch=95.20]#015Epoch 9:  54%|█████▎    | 22/41 [00:04<00:04,  4.71it/s, loss=98, v_num=0, train_loss_step=115.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  54%|█████▎    | 22/41 [00:04<00:04,  4.71it/s, loss=97, v_num=0, train_loss_step=73.50, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  56%|█████▌    | 23/41 [00:04<00:03,  4.72it/s, loss=97, v_num=0, train_loss_step=73.50, val_loss=146.0, train_loss_epoch=95.20]#015Epoch 9:  56%|█████▌    | 23/41 [00:04<00:03,  4.72it/s, loss=97, v_num=0, train_loss_step=73.50, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  56%|█████▌    | 23/41 [00:04<00:03,  4.72it/s, loss=98, v_num=0, train_loss_step=103.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  59%|█████▊    | 24/41 [00:05<00:03,  4.71it/s, loss=98, v_num=0, train_loss_step=103.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  59%|█████▊    | 24/41 [00:05<00:03,  4.71it/s, loss=98, v_num=0, train_loss_step=103.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  59%|█████▊    | 24/41 [00:05<00:03,  4.71it/s, loss=97.8, v_num=0, train_loss_step=105.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  61%|██████    | 25/41 [00:05<00:03,  4.71it/s, loss=97.8, v_num=0, train_loss_step=105.0, val_loss=146.0, train_loss_epoch=95.20]#015Epoch 9:  61%|██████    | 25/41 [00:05<00:03,  4.71it/s, loss=97.8, v_num=0, train_loss_step=105.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  61%|██████    | 25/41 [00:05<00:03,  4.71it/s, loss=97.7, v_num=0, train_loss_step=87.80, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  63%|██████▎   | 26/41 [00:05<00:03,  4.71it/s, loss=97.7, v_num=0, train_loss_step=87.80, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  63%|██████▎   | 26/41 [00:05<00:03,  4.71it/s, loss=97.7, v_num=0, train_loss_step=87.80, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  63%|██████▎   | 26/41 [00:05<00:03,  4.71it/s, loss=98.4, v_num=0, train_loss_step=104.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  66%|██████▌   | 27/41 [00:05<00:02,  4.72it/s, loss=98.4, v_num=0, train_loss_step=104.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  66%|██████▌   | 27/41 [00:05<00:02,  4.72it/s, loss=98.4, v_num=0, train_loss_step=104.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  66%|██████▌   | 27/41 [00:05<00:02,  4.72it/s, loss=98.2, v_num=0, train_loss_step=91.30, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  68%|██████▊   | 28/41 [00:05<00:02,  4.72it/s, loss=98.2, v_num=0, train_loss_step=91.30, val_loss=146.0, train_loss_epoch=95.20]#015Epoch 9:  68%|██████▊   | 28/41 [00:05<00:02,  4.72it/s, loss=98.2, v_num=0, train_loss_step=91.30, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  68%|██████▊   | 28/41 [00:05<00:02,  4.72it/s, loss=98.2, v_num=0, train_loss_step=92.90, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  71%|███████   | 29/41 [00:06<00:02,  4.73it/s, loss=98.2, v_num=0, train_loss_step=92.90, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  71%|███████   | 29/41 [00:06<00:02,  4.73it/s, loss=98.2, v_num=0, train_loss_step=92.90, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  71%|███████   | 29/41 [00:06<00:02,  4.73it/s, loss=97.7, v_num=0, train_loss_step=67.90, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  73%|███████▎  | 30/41 [00:06<00:02,  4.71it/s, loss=97.7, v_num=0, train_loss_step=67.90, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  73%|███████▎  | 30/41 [00:06<00:02,  4.71it/s, loss=97.7, v_num=0, train_loss_step=67.90, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  73%|███████▎  | 30/41 [00:06<00:02,  4.71it/s, loss=96.2, v_num=0, train_loss_step=69.80, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  76%|███████▌  | 31/41 [00:06<00:02,  4.72it/s, loss=96.2, v_num=0, train_loss_step=69.80, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  76%|███████▌  | 31/41 [00:06<00:02,  4.72it/s, loss=96.2, v_num=0, train_loss_step=69.80, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  76%|███████▌  | 31/41 [00:06<00:02,  4.72it/s, loss=96.4, v_num=0, train_loss_step=98.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  78%|███████▊  | 32/41 [00:06<00:01,  4.69it/s, loss=96.4, v_num=0, train_loss_step=98.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  78%|███████▊  | 32/41 [00:06<00:01,  4.69it/s, loss=96.4, v_num=0, train_loss_step=98.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  78%|███████▊  | 32/41 [00:06<00:01,  4.69it/s, loss=94.8, v_num=0, train_loss_step=82.50, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  80%|████████  | 33/41 [00:07<00:01,  4.69it/s, loss=94.8, v_num=0, train_loss_step=82.50, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  80%|████████  | 33/41 [00:07<00:01,  4.69it/s, loss=94.8, v_num=0, train_loss_step=82.50, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  80%|████████  | 33/41 [00:07<00:01,  4.69it/s, loss=95.5, v_num=0, train_loss_step=107.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  83%|████████▎ | 34/41 [00:07<00:01,  4.70it/s, loss=95.5, v_num=0, train_loss_step=107.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  83%|████████▎ | 34/41 [00:07<00:01,  4.70it/s, loss=95.5, v_num=0, train_loss_step=107.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  83%|████████▎ | 34/41 [00:07<00:01,  4.70it/s, loss=95.1, v_num=0, train_loss_step=86.30, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  85%|████████▌ | 35/41 [00:07<00:01,  4.70it/s, loss=95.1, v_num=0, train_loss_step=86.30, val_loss=146.0, train_loss_epoch=95.20]#015Epoch 9:  85%|████████▌ | 35/41 [00:07<00:01,  4.70it/s, loss=95.1, v_num=0, train_loss_step=86.30, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  85%|████████▌ | 35/41 [00:07<00:01,  4.70it/s, loss=94, v_num=0, train_loss_step=88.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  88%|████████▊ | 36/41 [00:07<00:01,  4.69it/s, loss=94, v_num=0, train_loss_step=88.70, val_loss=146.0, train_loss_epoch=95.20]#015Epoch 9:  88%|████████▊ | 36/41 [00:07<00:01,  4.69it/s, loss=94, v_num=0, train_loss_step=88.70, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  88%|████████▊ | 36/41 [00:07<00:01,  4.68it/s, loss=94.2, v_num=0, train_loss_step=98.20, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  90%|█████████ | 37/41 [00:07<00:00,  4.69it/s, loss=94.2, v_num=0, train_loss_step=98.20, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  90%|█████████ | 37/41 [00:07<00:00,  4.69it/s, loss=94.2, v_num=0, train_loss_step=98.20, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  90%|█████████ | 37/41 [00:07<00:00,  4.69it/s, loss=94.1, v_num=0, train_loss_step=101.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  93%|█████████▎| 38/41 [00:08<00:00,  4.69it/s, loss=94.1, v_num=0, train_loss_step=101.0, val_loss=146.0, train_loss_epoch=95.20]#015Epoch 9:  93%|█████████▎| 38/41 [00:08<00:00,  4.69it/s, loss=94.1, v_num=0, train_loss_step=101.0, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  93%|█████████▎| 38/41 [00:08<00:00,  4.69it/s, loss=93.9, v_num=0, train_loss_step=99.30, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  95%|█████████▌| 39/41 [00:08<00:00,  4.69it/s, loss=93.9, v_num=0, train_loss_step=99.30, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  95%|█████████▌| 39/41 [00:08<00:00,  4.69it/s, loss=93.9, v_num=0, train_loss_step=99.30, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  95%|█████████▌| 39/41 [00:08<00:00,  4.69it/s, loss=92.8, v_num=0, train_loss_step=87.90, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  98%|█████████▊| 40/41 [00:08<00:00,  4.70it/s, loss=92.8, v_num=0, train_loss_step=87.90, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  98%|█████████▊| 40/41 [00:08<00:00,  4.70it/s, loss=92.8, v_num=0, train_loss_step=87.90, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9:  98%|█████████▊| 40/41 [00:08<00:00,  4.70it/s, loss=92.1, v_num=0, train_loss_step=83.00, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.71it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.71it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 9: 100%|██████████| 41/41 [00:08<00:00,  4.66it/s, loss=92.1, v_num=0, train_loss_step=83.00, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9: 100%|██████████| 41/41 [00:08<00:00,  4.66it/s, loss=92.1, v_num=0, train_loss_step=83.00, val_loss=146.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34mEpoch 9: 100%|██████████| 41/41 [00:09<00:00,  4.38it/s, loss=92.1, v_num=0, train_loss_step=83.00, val_loss=148.0, train_loss_epoch=95.20]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 9: 100%|██████████| 41/41 [00:09<00:00,  4.38it/s, loss=92.1, v_num=0, train_loss_step=83.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 9:   0%|          | 0/41 [00:00<?, ?it/s, loss=92.1, v_num=0, train_loss_step=83.00, val_loss=148.0, train_loss_epoch=93.10]         #015Epoch 10:   0%|          | 0/41 [00:00<?, ?it/s, loss=92.1, v_num=0, train_loss_step=83.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:   2%|▏         | 1/41 [00:00<00:08,  4.97it/s, loss=92.1, v_num=0, train_loss_step=83.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:   2%|▏         | 1/41 [00:00<00:08,  4.96it/s, loss=92.1, v_num=0, train_loss_step=83.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:   2%|▏         | 1/41 [00:00<00:08,  4.96it/s, loss=90.6, v_num=0, train_loss_step=84.90, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:   5%|▍         | 2/41 [00:00<00:08,  4.71it/s, loss=90.6, v_num=0, train_loss_step=84.90, val_loss=148.0, train_loss_epoch=93.10]#015Epoch 10:   5%|▍         | 2/41 [00:00<00:08,  4.71it/s, loss=90.6, v_num=0, train_loss_step=84.90, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:   5%|▍         | 2/41 [00:00<00:08,  4.71it/s, loss=90.9, v_num=0, train_loss_step=79.80, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:   7%|▋         | 3/41 [00:00<00:10,  3.76it/s, loss=90.9, v_num=0, train_loss_step=79.80, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:   7%|▋         | 3/41 [00:00<00:10,  3.76it/s, loss=90.9, v_num=0, train_loss_step=79.80, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:   7%|▋         | 3/41 [00:00<00:10,  3.76it/s, loss=89.8, v_num=0, train_loss_step=79.50, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  10%|▉         | 4/41 [00:01<00:09,  4.00it/s, loss=89.8, v_num=0, train_loss_step=79.50, val_loss=148.0, train_loss_epoch=93.10]#015Epoch 10:  10%|▉         | 4/41 [00:01<00:09,  4.00it/s, loss=89.8, v_num=0, train_loss_step=79.50, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  10%|▉         | 4/41 [00:01<00:09,  4.00it/s, loss=89.2, v_num=0, train_loss_step=94.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  12%|█▏        | 5/41 [00:01<00:08,  4.15it/s, loss=89.2, v_num=0, train_loss_step=94.00, val_loss=148.0, train_loss_epoch=93.10]#015Epoch 10:  12%|█▏        | 5/41 [00:01<00:08,  4.15it/s, loss=89.2, v_num=0, train_loss_step=94.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  12%|█▏        | 5/41 [00:01<00:08,  4.15it/s, loss=89.3, v_num=0, train_loss_step=89.30, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  15%|█▍        | 6/41 [00:01<00:08,  4.27it/s, loss=89.3, v_num=0, train_loss_step=89.30, val_loss=148.0, train_loss_epoch=93.10]#015Epoch 10:  15%|█▍        | 6/41 [00:01<00:08,  4.27it/s, loss=89.3, v_num=0, train_loss_step=89.30, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  15%|█▍        | 6/41 [00:01<00:08,  4.27it/s, loss=89.4, v_num=0, train_loss_step=106.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  17%|█▋        | 7/41 [00:01<00:07,  4.35it/s, loss=89.4, v_num=0, train_loss_step=106.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  17%|█▋        | 7/41 [00:01<00:07,  4.35it/s, loss=89.4, v_num=0, train_loss_step=106.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  17%|█▋        | 7/41 [00:01<00:07,  4.35it/s, loss=89.9, v_num=0, train_loss_step=101.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  20%|█▉        | 8/41 [00:01<00:07,  4.38it/s, loss=89.9, v_num=0, train_loss_step=101.0, val_loss=148.0, train_loss_epoch=93.10]#015Epoch 10:  20%|█▉        | 8/41 [00:01<00:07,  4.37it/s, loss=89.9, v_num=0, train_loss_step=101.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  20%|█▉        | 8/41 [00:01<00:07,  4.37it/s, loss=89.8, v_num=0, train_loss_step=89.80, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  22%|██▏       | 9/41 [00:02<00:07,  4.43it/s, loss=89.8, v_num=0, train_loss_step=89.80, val_loss=148.0, train_loss_epoch=93.10]#015Epoch 10:  22%|██▏       | 9/41 [00:02<00:07,  4.43it/s, loss=89.8, v_num=0, train_loss_step=89.80, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  22%|██▏       | 9/41 [00:02<00:07,  4.43it/s, loss=91, v_num=0, train_loss_step=93.80, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  24%|██▍       | 10/41 [00:02<00:06,  4.47it/s, loss=91, v_num=0, train_loss_step=93.80, val_loss=148.0, train_loss_epoch=93.10]#015Epoch 10:  24%|██▍       | 10/41 [00:02<00:06,  4.47it/s, loss=91, v_num=0, train_loss_step=93.80, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  24%|██▍       | 10/41 [00:02<00:06,  4.47it/s, loss=92, v_num=0, train_loss_step=88.30, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  27%|██▋       | 11/41 [00:02<00:06,  4.51it/s, loss=92, v_num=0, train_loss_step=88.30, val_loss=148.0, train_loss_epoch=93.10]#015Epoch 10:  27%|██▋       | 11/41 [00:02<00:06,  4.51it/s, loss=92, v_num=0, train_loss_step=88.30, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  27%|██▋       | 11/41 [00:02<00:06,  4.51it/s, loss=92.5, v_num=0, train_loss_step=109.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  29%|██▉       | 12/41 [00:02<00:06,  4.54it/s, loss=92.5, v_num=0, train_loss_step=109.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  29%|██▉       | 12/41 [00:02<00:06,  4.54it/s, loss=92.5, v_num=0, train_loss_step=109.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  29%|██▉       | 12/41 [00:02<00:06,  4.54it/s, loss=93.2, v_num=0, train_loss_step=97.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  32%|███▏      | 13/41 [00:02<00:06,  4.56it/s, loss=93.2, v_num=0, train_loss_step=97.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  32%|███▏      | 13/41 [00:02<00:06,  4.56it/s, loss=93.2, v_num=0, train_loss_step=97.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  32%|███▏      | 13/41 [00:02<00:06,  4.56it/s, loss=93, v_num=0, train_loss_step=102.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  34%|███▍      | 14/41 [00:03<00:05,  4.56it/s, loss=93, v_num=0, train_loss_step=102.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  34%|███▍      | 14/41 [00:03<00:05,  4.56it/s, loss=93, v_num=0, train_loss_step=102.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  34%|███▍      | 14/41 [00:03<00:05,  4.56it/s, loss=93.6, v_num=0, train_loss_step=97.80, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  37%|███▋      | 15/41 [00:03<00:05,  4.58it/s, loss=93.6, v_num=0, train_loss_step=97.80, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  37%|███▋      | 15/41 [00:03<00:05,  4.58it/s, loss=93.6, v_num=0, train_loss_step=97.80, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  37%|███▋      | 15/41 [00:03<00:05,  4.58it/s, loss=93.9, v_num=0, train_loss_step=94.70, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  39%|███▉      | 16/41 [00:03<00:05,  4.60it/s, loss=93.9, v_num=0, train_loss_step=94.70, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  39%|███▉      | 16/41 [00:03<00:05,  4.60it/s, loss=93.9, v_num=0, train_loss_step=94.70, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  39%|███▉      | 16/41 [00:03<00:05,  4.60it/s, loss=92.9, v_num=0, train_loss_step=79.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  41%|████▏     | 17/41 [00:03<00:05,  4.62it/s, loss=92.9, v_num=0, train_loss_step=79.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  41%|████▏     | 17/41 [00:03<00:05,  4.62it/s, loss=92.9, v_num=0, train_loss_step=79.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  41%|████▏     | 17/41 [00:03<00:05,  4.62it/s, loss=93.2, v_num=0, train_loss_step=106.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  44%|████▍     | 18/41 [00:03<00:04,  4.62it/s, loss=93.2, v_num=0, train_loss_step=106.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  44%|████▍     | 18/41 [00:03<00:04,  4.62it/s, loss=93.2, v_num=0, train_loss_step=106.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  44%|████▍     | 18/41 [00:03<00:04,  4.62it/s, loss=93, v_num=0, train_loss_step=95.50, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  46%|████▋     | 19/41 [00:04<00:04,  4.64it/s, loss=93, v_num=0, train_loss_step=95.50, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  46%|████▋     | 19/41 [00:04<00:04,  4.63it/s, loss=93, v_num=0, train_loss_step=95.50, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  46%|████▋     | 19/41 [00:04<00:04,  4.63it/s, loss=92.9, v_num=0, train_loss_step=86.30, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  49%|████▉     | 20/41 [00:04<00:04,  4.63it/s, loss=92.9, v_num=0, train_loss_step=86.30, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  49%|████▉     | 20/41 [00:04<00:04,  4.63it/s, loss=92.9, v_num=0, train_loss_step=86.30, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  49%|████▉     | 20/41 [00:04<00:04,  4.63it/s, loss=92.6, v_num=0, train_loss_step=77.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  51%|█████     | 21/41 [00:04<00:04,  4.64it/s, loss=92.6, v_num=0, train_loss_step=77.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  51%|█████     | 21/41 [00:04<00:04,  4.64it/s, loss=92.6, v_num=0, train_loss_step=77.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  51%|█████     | 21/41 [00:04<00:04,  4.64it/s, loss=92.2, v_num=0, train_loss_step=77.60, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  54%|█████▎    | 22/41 [00:04<00:04,  4.66it/s, loss=92.2, v_num=0, train_loss_step=77.60, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  54%|█████▎    | 22/41 [00:04<00:04,  4.66it/s, loss=92.2, v_num=0, train_loss_step=77.60, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  54%|█████▎    | 22/41 [00:04<00:04,  4.65it/s, loss=93.1, v_num=0, train_loss_step=97.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  56%|█████▌    | 23/41 [00:04<00:03,  4.67it/s, loss=93.1, v_num=0, train_loss_step=97.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  56%|█████▌    | 23/41 [00:04<00:03,  4.66it/s, loss=93.1, v_num=0, train_loss_step=97.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  56%|█████▌    | 23/41 [00:04<00:03,  4.66it/s, loss=93.7, v_num=0, train_loss_step=92.40, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  59%|█████▊    | 24/41 [00:05<00:03,  4.67it/s, loss=93.7, v_num=0, train_loss_step=92.40, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  59%|█████▊    | 24/41 [00:05<00:03,  4.67it/s, loss=93.7, v_num=0, train_loss_step=92.40, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  59%|█████▊    | 24/41 [00:05<00:03,  4.67it/s, loss=94.1, v_num=0, train_loss_step=102.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  61%|██████    | 25/41 [00:05<00:03,  4.70it/s, loss=94.1, v_num=0, train_loss_step=102.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  61%|██████    | 25/41 [00:05<00:03,  4.70it/s, loss=94.1, v_num=0, train_loss_step=102.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  61%|██████    | 25/41 [00:05<00:03,  4.70it/s, loss=93.4, v_num=0, train_loss_step=73.50, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  63%|██████▎   | 26/41 [00:05<00:03,  4.73it/s, loss=93.4, v_num=0, train_loss_step=73.50, val_loss=148.0, train_loss_epoch=93.10]#015Epoch 10:  63%|██████▎   | 26/41 [00:05<00:03,  4.73it/s, loss=93.4, v_num=0, train_loss_step=73.50, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  63%|██████▎   | 26/41 [00:05<00:03,  4.73it/s, loss=92.4, v_num=0, train_loss_step=86.50, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  66%|██████▌   | 27/41 [00:05<00:02,  4.76it/s, loss=92.4, v_num=0, train_loss_step=86.50, val_loss=148.0, train_loss_epoch=93.10]#015Epoch 10:  66%|██████▌   | 27/41 [00:05<00:02,  4.76it/s, loss=92.4, v_num=0, train_loss_step=86.50, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  66%|██████▌   | 27/41 [00:05<00:02,  4.76it/s, loss=91.9, v_num=0, train_loss_step=92.90, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  68%|██████▊   | 28/41 [00:05<00:02,  4.78it/s, loss=91.9, v_num=0, train_loss_step=92.90, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  68%|██████▊   | 28/41 [00:05<00:02,  4.78it/s, loss=91.9, v_num=0, train_loss_step=92.90, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  68%|██████▊   | 28/41 [00:05<00:02,  4.78it/s, loss=92, v_num=0, train_loss_step=92.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  71%|███████   | 29/41 [00:06<00:02,  4.80it/s, loss=92, v_num=0, train_loss_step=92.00, val_loss=148.0, train_loss_epoch=93.10]#015Epoch 10:  71%|███████   | 29/41 [00:06<00:02,  4.80it/s, loss=92, v_num=0, train_loss_step=92.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  71%|███████   | 29/41 [00:06<00:02,  4.80it/s, loss=91.5, v_num=0, train_loss_step=83.60, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  73%|███████▎  | 30/41 [00:06<00:02,  4.83it/s, loss=91.5, v_num=0, train_loss_step=83.60, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  73%|███████▎  | 30/41 [00:06<00:02,  4.83it/s, loss=91.5, v_num=0, train_loss_step=83.60, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  73%|███████▎  | 30/41 [00:06<00:02,  4.83it/s, loss=91.7, v_num=0, train_loss_step=91.10, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  76%|███████▌  | 31/41 [00:06<00:02,  4.85it/s, loss=91.7, v_num=0, train_loss_step=91.10, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  76%|███████▌  | 31/41 [00:06<00:02,  4.85it/s, loss=91.7, v_num=0, train_loss_step=91.10, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  76%|███████▌  | 31/41 [00:06<00:02,  4.85it/s, loss=91.5, v_num=0, train_loss_step=106.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  78%|███████▊  | 32/41 [00:06<00:01,  4.87it/s, loss=91.5, v_num=0, train_loss_step=106.0, val_loss=148.0, train_loss_epoch=93.10]#015Epoch 10:  78%|███████▊  | 32/41 [00:06<00:01,  4.87it/s, loss=91.5, v_num=0, train_loss_step=106.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  78%|███████▊  | 32/41 [00:06<00:01,  4.87it/s, loss=91.6, v_num=0, train_loss_step=98.70, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  80%|████████  | 33/41 [00:06<00:01,  4.89it/s, loss=91.6, v_num=0, train_loss_step=98.70, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  80%|████████  | 33/41 [00:06<00:01,  4.89it/s, loss=91.6, v_num=0, train_loss_step=98.70, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  80%|████████  | 33/41 [00:06<00:01,  4.89it/s, loss=91.4, v_num=0, train_loss_step=99.20, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  83%|████████▎ | 34/41 [00:06<00:01,  4.90it/s, loss=91.4, v_num=0, train_loss_step=99.20, val_loss=148.0, train_loss_epoch=93.10]#015Epoch 10:  83%|████████▎ | 34/41 [00:06<00:01,  4.90it/s, loss=91.4, v_num=0, train_loss_step=99.20, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  83%|████████▎ | 34/41 [00:06<00:01,  4.90it/s, loss=91, v_num=0, train_loss_step=88.50, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  85%|████████▌ | 35/41 [00:07<00:01,  4.76it/s, loss=91, v_num=0, train_loss_step=88.50, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  85%|████████▌ | 35/41 [00:07<00:01,  4.76it/s, loss=91, v_num=0, train_loss_step=88.50, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  85%|████████▌ | 35/41 [00:07<00:01,  4.76it/s, loss=90.1, v_num=0, train_loss_step=77.90, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  88%|████████▊ | 36/41 [00:07<00:01,  4.78it/s, loss=90.1, v_num=0, train_loss_step=77.90, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  88%|████████▊ | 36/41 [00:07<00:01,  4.78it/s, loss=90.1, v_num=0, train_loss_step=77.90, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  88%|████████▊ | 36/41 [00:07<00:01,  4.78it/s, loss=90.1, v_num=0, train_loss_step=77.40, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  90%|█████████ | 37/41 [00:07<00:00,  4.80it/s, loss=90.1, v_num=0, train_loss_step=77.40, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  90%|█████████ | 37/41 [00:07<00:00,  4.80it/s, loss=90.1, v_num=0, train_loss_step=77.40, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  90%|█████████ | 37/41 [00:07<00:00,  4.80it/s, loss=89.1, v_num=0, train_loss_step=88.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  93%|█████████▎| 38/41 [00:07<00:00,  4.81it/s, loss=89.1, v_num=0, train_loss_step=88.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  93%|█████████▎| 38/41 [00:07<00:00,  4.81it/s, loss=89.1, v_num=0, train_loss_step=88.00, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  93%|█████████▎| 38/41 [00:07<00:00,  4.81it/s, loss=88.3, v_num=0, train_loss_step=79.20, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  95%|█████████▌| 39/41 [00:08<00:00,  4.83it/s, loss=88.3, v_num=0, train_loss_step=79.20, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  95%|█████████▌| 39/41 [00:08<00:00,  4.83it/s, loss=88.3, v_num=0, train_loss_step=79.20, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  95%|█████████▌| 39/41 [00:08<00:00,  4.83it/s, loss=89.1, v_num=0, train_loss_step=102.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  98%|█████████▊| 40/41 [00:08<00:00,  4.85it/s, loss=89.1, v_num=0, train_loss_step=102.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  98%|█████████▊| 40/41 [00:08<00:00,  4.85it/s, loss=89.1, v_num=0, train_loss_step=102.0, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10:  98%|█████████▊| 40/41 [00:08<00:00,  4.85it/s, loss=89.5, v_num=0, train_loss_step=84.20, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.73it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.73it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 10: 100%|██████████| 41/41 [00:08<00:00,  4.81it/s, loss=89.5, v_num=0, train_loss_step=84.20, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10: 100%|██████████| 41/41 [00:08<00:00,  4.81it/s, loss=89.5, v_num=0, train_loss_step=84.20, val_loss=148.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34mEpoch 10: 100%|██████████| 41/41 [00:09<00:00,  4.52it/s, loss=89.5, v_num=0, train_loss_step=84.20, val_loss=151.0, train_loss_epoch=93.10]\u001b[0m\n",
      "\u001b[34m#015                                                                      #033[A\u001b[0m\n",
      "\u001b[34mEpoch 10: 100%|██████████| 41/41 [00:09<00:00,  4.51it/s, loss=89.5, v_num=0, train_loss_step=84.20, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 10:   0%|          | 0/41 [00:00<?, ?it/s, loss=89.5, v_num=0, train_loss_step=84.20, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:   0%|          | 0/41 [00:00<?, ?it/s, loss=89.5, v_num=0, train_loss_step=84.20, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:   2%|▏         | 1/41 [00:00<00:06,  5.86it/s, loss=89.5, v_num=0, train_loss_step=84.20, val_loss=151.0, train_loss_epoch=91.50]#015Epoch 11:   2%|▏         | 1/41 [00:00<00:06,  5.85it/s, loss=89.5, v_num=0, train_loss_step=84.20, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:   2%|▏         | 1/41 [00:00<00:06,  5.84it/s, loss=90.3, v_num=0, train_loss_step=93.40, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:   5%|▍         | 2/41 [00:00<00:06,  5.75it/s, loss=90.3, v_num=0, train_loss_step=93.40, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:   5%|▍         | 2/41 [00:00<00:06,  5.75it/s, loss=90.3, v_num=0, train_loss_step=93.40, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:   5%|▍         | 2/41 [00:00<00:06,  5.74it/s, loss=90.3, v_num=0, train_loss_step=96.90, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:   7%|▋         | 3/41 [00:00<00:06,  5.71it/s, loss=90.3, v_num=0, train_loss_step=96.90, val_loss=151.0, train_loss_epoch=91.50]#015Epoch 11:   7%|▋         | 3/41 [00:00<00:06,  5.71it/s, loss=90.3, v_num=0, train_loss_step=96.90, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:   7%|▋         | 3/41 [00:00<00:06,  5.70it/s, loss=90, v_num=0, train_loss_step=87.00, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  10%|▉         | 4/41 [00:00<00:06,  5.60it/s, loss=90, v_num=0, train_loss_step=87.00, val_loss=151.0, train_loss_epoch=91.50]#015Epoch 11:  10%|▉         | 4/41 [00:00<00:06,  5.60it/s, loss=90, v_num=0, train_loss_step=87.00, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  10%|▉         | 4/41 [00:00<00:06,  5.60it/s, loss=89.2, v_num=0, train_loss_step=85.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  12%|█▏        | 5/41 [00:00<00:06,  5.63it/s, loss=89.2, v_num=0, train_loss_step=85.50, val_loss=151.0, train_loss_epoch=91.50]#015Epoch 11:  12%|█▏        | 5/41 [00:00<00:06,  5.62it/s, loss=89.2, v_num=0, train_loss_step=85.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  12%|█▏        | 5/41 [00:00<00:06,  5.62it/s, loss=90.9, v_num=0, train_loss_step=109.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  15%|█▍        | 6/41 [00:01<00:06,  5.63it/s, loss=90.9, v_num=0, train_loss_step=109.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  15%|█▍        | 6/41 [00:01<00:06,  5.63it/s, loss=90.9, v_num=0, train_loss_step=109.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  15%|█▍        | 6/41 [00:01<00:06,  5.62it/s, loss=91.2, v_num=0, train_loss_step=92.60, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  17%|█▋        | 7/41 [00:01<00:06,  5.62it/s, loss=91.2, v_num=0, train_loss_step=92.60, val_loss=151.0, train_loss_epoch=91.50]#015Epoch 11:  17%|█▋        | 7/41 [00:01<00:06,  5.62it/s, loss=91.2, v_num=0, train_loss_step=92.60, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  17%|█▋        | 7/41 [00:01<00:06,  5.61it/s, loss=90, v_num=0, train_loss_step=68.80, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  20%|█▉        | 8/41 [00:01<00:05,  5.62it/s, loss=90, v_num=0, train_loss_step=68.80, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  20%|█▉        | 8/41 [00:01<00:05,  5.62it/s, loss=90, v_num=0, train_loss_step=68.80, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  20%|█▉        | 8/41 [00:01<00:05,  5.62it/s, loss=89.5, v_num=0, train_loss_step=82.10, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  22%|██▏       | 9/41 [00:01<00:05,  5.62it/s, loss=89.5, v_num=0, train_loss_step=82.10, val_loss=151.0, train_loss_epoch=91.50]#015Epoch 11:  22%|██▏       | 9/41 [00:01<00:05,  5.62it/s, loss=89.5, v_num=0, train_loss_step=82.10, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  22%|██▏       | 9/41 [00:01<00:05,  5.62it/s, loss=89.9, v_num=0, train_loss_step=90.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  24%|██▍       | 10/41 [00:01<00:05,  5.59it/s, loss=89.9, v_num=0, train_loss_step=90.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  24%|██▍       | 10/41 [00:01<00:05,  5.59it/s, loss=89.9, v_num=0, train_loss_step=90.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  24%|██▍       | 10/41 [00:01<00:05,  5.59it/s, loss=90.8, v_num=0, train_loss_step=109.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  27%|██▋       | 11/41 [00:01<00:05,  5.60it/s, loss=90.8, v_num=0, train_loss_step=109.0, val_loss=151.0, train_loss_epoch=91.50]#015Epoch 11:  27%|██▋       | 11/41 [00:01<00:05,  5.60it/s, loss=90.8, v_num=0, train_loss_step=109.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  27%|██▋       | 11/41 [00:01<00:05,  5.60it/s, loss=90.8, v_num=0, train_loss_step=106.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  29%|██▉       | 12/41 [00:02<00:05,  5.61it/s, loss=90.8, v_num=0, train_loss_step=106.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  29%|██▉       | 12/41 [00:02<00:05,  5.61it/s, loss=90.8, v_num=0, train_loss_step=106.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  29%|██▉       | 12/41 [00:02<00:05,  5.61it/s, loss=89.9, v_num=0, train_loss_step=81.40, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  32%|███▏      | 13/41 [00:02<00:04,  5.62it/s, loss=89.9, v_num=0, train_loss_step=81.40, val_loss=151.0, train_loss_epoch=91.50]#015Epoch 11:  32%|███▏      | 13/41 [00:02<00:04,  5.62it/s, loss=89.9, v_num=0, train_loss_step=81.40, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  32%|███▏      | 13/41 [00:02<00:04,  5.62it/s, loss=90.3, v_num=0, train_loss_step=107.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  34%|███▍      | 14/41 [00:02<00:04,  5.62it/s, loss=90.3, v_num=0, train_loss_step=107.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  34%|███▍      | 14/41 [00:02<00:04,  5.62it/s, loss=90.3, v_num=0, train_loss_step=107.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  34%|███▍      | 14/41 [00:02<00:04,  5.62it/s, loss=89.9, v_num=0, train_loss_step=79.60, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  37%|███▋      | 15/41 [00:02<00:04,  5.62it/s, loss=89.9, v_num=0, train_loss_step=79.60, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  37%|███▋      | 15/41 [00:02<00:04,  5.62it/s, loss=89.9, v_num=0, train_loss_step=79.60, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  37%|███▋      | 15/41 [00:02<00:04,  5.62it/s, loss=90.6, v_num=0, train_loss_step=92.10, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  39%|███▉      | 16/41 [00:02<00:04,  5.60it/s, loss=90.6, v_num=0, train_loss_step=92.10, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  39%|███▉      | 16/41 [00:02<00:04,  5.60it/s, loss=90.6, v_num=0, train_loss_step=92.10, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  39%|███▉      | 16/41 [00:02<00:04,  5.60it/s, loss=91, v_num=0, train_loss_step=85.20, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  41%|████▏     | 17/41 [00:03<00:04,  5.60it/s, loss=91, v_num=0, train_loss_step=85.20, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  41%|████▏     | 17/41 [00:03<00:04,  5.60it/s, loss=91, v_num=0, train_loss_step=85.20, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  41%|████▏     | 17/41 [00:03<00:04,  5.60it/s, loss=89.9, v_num=0, train_loss_step=66.30, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  44%|████▍     | 18/41 [00:03<00:04,  5.61it/s, loss=89.9, v_num=0, train_loss_step=66.30, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  44%|████▍     | 18/41 [00:03<00:04,  5.60it/s, loss=89.9, v_num=0, train_loss_step=66.30, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  44%|████▍     | 18/41 [00:03<00:04,  5.60it/s, loss=89, v_num=0, train_loss_step=61.70, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  46%|████▋     | 19/41 [00:03<00:03,  5.61it/s, loss=89, v_num=0, train_loss_step=61.70, val_loss=151.0, train_loss_epoch=91.50]#015Epoch 11:  46%|████▋     | 19/41 [00:03<00:03,  5.61it/s, loss=89, v_num=0, train_loss_step=61.70, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  46%|████▋     | 19/41 [00:03<00:03,  5.61it/s, loss=86.8, v_num=0, train_loss_step=57.20, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  49%|████▉     | 20/41 [00:03<00:03,  5.61it/s, loss=86.8, v_num=0, train_loss_step=57.20, val_loss=151.0, train_loss_epoch=91.50]#015Epoch 11:  49%|████▉     | 20/41 [00:03<00:03,  5.61it/s, loss=86.8, v_num=0, train_loss_step=57.20, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  49%|████▉     | 20/41 [00:03<00:03,  5.61it/s, loss=87, v_num=0, train_loss_step=88.60, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  51%|█████     | 21/41 [00:03<00:03,  5.61it/s, loss=87, v_num=0, train_loss_step=88.60, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  51%|█████     | 21/41 [00:03<00:03,  5.61it/s, loss=87, v_num=0, train_loss_step=88.60, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  51%|█████     | 21/41 [00:03<00:03,  5.61it/s, loss=86, v_num=0, train_loss_step=73.90, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  54%|█████▎    | 22/41 [00:03<00:03,  5.60it/s, loss=86, v_num=0, train_loss_step=73.90, val_loss=151.0, train_loss_epoch=91.50]#015Epoch 11:  54%|█████▎    | 22/41 [00:03<00:03,  5.60it/s, loss=86, v_num=0, train_loss_step=73.90, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  54%|█████▎    | 22/41 [00:03<00:03,  5.60it/s, loss=84.6, v_num=0, train_loss_step=68.70, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  56%|█████▌    | 23/41 [00:04<00:03,  5.60it/s, loss=84.6, v_num=0, train_loss_step=68.70, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  56%|█████▌    | 23/41 [00:04<00:03,  5.60it/s, loss=84.6, v_num=0, train_loss_step=68.70, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  56%|█████▌    | 23/41 [00:04<00:03,  5.60it/s, loss=83.9, v_num=0, train_loss_step=72.70, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  59%|█████▊    | 24/41 [00:04<00:03,  5.60it/s, loss=83.9, v_num=0, train_loss_step=72.70, val_loss=151.0, train_loss_epoch=91.50]#015Epoch 11:  59%|█████▊    | 24/41 [00:04<00:03,  5.60it/s, loss=83.9, v_num=0, train_loss_step=72.70, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  59%|█████▊    | 24/41 [00:04<00:03,  5.60it/s, loss=83.6, v_num=0, train_loss_step=78.90, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  61%|██████    | 25/41 [00:04<00:02,  5.59it/s, loss=83.6, v_num=0, train_loss_step=78.90, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  61%|██████    | 25/41 [00:04<00:02,  5.59it/s, loss=83.6, v_num=0, train_loss_step=78.90, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  61%|██████    | 25/41 [00:04<00:02,  5.59it/s, loss=82.3, v_num=0, train_loss_step=82.30, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  63%|██████▎   | 26/41 [00:04<00:02,  5.56it/s, loss=82.3, v_num=0, train_loss_step=82.30, val_loss=151.0, train_loss_epoch=91.50]#015Epoch 11:  63%|██████▎   | 26/41 [00:04<00:02,  5.56it/s, loss=82.3, v_num=0, train_loss_step=82.30, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  63%|██████▎   | 26/41 [00:04<00:02,  5.56it/s, loss=82, v_num=0, train_loss_step=88.20, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  66%|██████▌   | 27/41 [00:04<00:02,  5.53it/s, loss=82, v_num=0, train_loss_step=88.20, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  66%|██████▌   | 27/41 [00:04<00:02,  5.53it/s, loss=82, v_num=0, train_loss_step=88.20, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  66%|██████▌   | 27/41 [00:04<00:02,  5.53it/s, loss=82.9, v_num=0, train_loss_step=85.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  68%|██████▊   | 28/41 [00:05<00:02,  5.49it/s, loss=82.9, v_num=0, train_loss_step=85.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  68%|██████▊   | 28/41 [00:05<00:02,  5.49it/s, loss=82.9, v_num=0, train_loss_step=85.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  68%|██████▊   | 28/41 [00:05<00:02,  5.49it/s, loss=83.4, v_num=0, train_loss_step=93.40, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  71%|███████   | 29/41 [00:05<00:02,  5.46it/s, loss=83.4, v_num=0, train_loss_step=93.40, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  71%|███████   | 29/41 [00:05<00:02,  5.46it/s, loss=83.4, v_num=0, train_loss_step=93.40, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  71%|███████   | 29/41 [00:05<00:02,  5.45it/s, loss=83, v_num=0, train_loss_step=81.80, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  73%|███████▎  | 30/41 [00:05<00:02,  5.43it/s, loss=83, v_num=0, train_loss_step=81.80, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  73%|███████▎  | 30/41 [00:05<00:02,  5.43it/s, loss=83, v_num=0, train_loss_step=81.80, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  73%|███████▎  | 30/41 [00:05<00:02,  5.43it/s, loss=82, v_num=0, train_loss_step=88.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  76%|███████▌  | 31/41 [00:05<00:01,  5.41it/s, loss=82, v_num=0, train_loss_step=88.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  76%|███████▌  | 31/41 [00:05<00:01,  5.41it/s, loss=82, v_num=0, train_loss_step=88.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  76%|███████▌  | 31/41 [00:05<00:01,  5.41it/s, loss=81.7, v_num=0, train_loss_step=100.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  78%|███████▊  | 32/41 [00:05<00:01,  5.39it/s, loss=81.7, v_num=0, train_loss_step=100.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  78%|███████▊  | 32/41 [00:05<00:01,  5.39it/s, loss=81.7, v_num=0, train_loss_step=100.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  78%|███████▊  | 32/41 [00:05<00:01,  5.39it/s, loss=82.6, v_num=0, train_loss_step=99.30, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  80%|████████  | 33/41 [00:06<00:01,  5.37it/s, loss=82.6, v_num=0, train_loss_step=99.30, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  80%|████████  | 33/41 [00:06<00:01,  5.37it/s, loss=82.6, v_num=0, train_loss_step=99.30, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  80%|████████  | 33/41 [00:06<00:01,  5.37it/s, loss=82.2, v_num=0, train_loss_step=99.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  83%|████████▎ | 34/41 [00:06<00:01,  5.34it/s, loss=82.2, v_num=0, train_loss_step=99.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  83%|████████▎ | 34/41 [00:06<00:01,  5.34it/s, loss=82.2, v_num=0, train_loss_step=99.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  83%|████████▎ | 34/41 [00:06<00:01,  5.34it/s, loss=83.4, v_num=0, train_loss_step=103.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  85%|████████▌ | 35/41 [00:06<00:01,  5.32it/s, loss=83.4, v_num=0, train_loss_step=103.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  85%|████████▌ | 35/41 [00:06<00:01,  5.32it/s, loss=83.4, v_num=0, train_loss_step=103.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  85%|████████▌ | 35/41 [00:06<00:01,  5.32it/s, loss=83.7, v_num=0, train_loss_step=98.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  88%|████████▊ | 36/41 [00:06<00:00,  5.30it/s, loss=83.7, v_num=0, train_loss_step=98.50, val_loss=151.0, train_loss_epoch=91.50]#015Epoch 11:  88%|████████▊ | 36/41 [00:06<00:00,  5.30it/s, loss=83.7, v_num=0, train_loss_step=98.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  88%|████████▊ | 36/41 [00:06<00:00,  5.30it/s, loss=83.4, v_num=0, train_loss_step=79.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  90%|█████████ | 37/41 [00:06<00:00,  5.29it/s, loss=83.4, v_num=0, train_loss_step=79.50, val_loss=151.0, train_loss_epoch=91.50]#015Epoch 11:  90%|█████████ | 37/41 [00:06<00:00,  5.29it/s, loss=83.4, v_num=0, train_loss_step=79.50, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  90%|█████████ | 37/41 [00:06<00:00,  5.29it/s, loss=84.8, v_num=0, train_loss_step=93.30, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  93%|█████████▎| 38/41 [00:07<00:00,  5.27it/s, loss=84.8, v_num=0, train_loss_step=93.30, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  93%|█████████▎| 38/41 [00:07<00:00,  5.27it/s, loss=84.8, v_num=0, train_loss_step=93.30, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  93%|█████████▎| 38/41 [00:07<00:00,  5.27it/s, loss=86.8, v_num=0, train_loss_step=102.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  95%|█████████▌| 39/41 [00:07<00:00,  5.26it/s, loss=86.8, v_num=0, train_loss_step=102.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  95%|█████████▌| 39/41 [00:07<00:00,  5.26it/s, loss=86.8, v_num=0, train_loss_step=102.0, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  95%|█████████▌| 39/41 [00:07<00:00,  5.26it/s, loss=88, v_num=0, train_loss_step=81.30, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  98%|█████████▊| 40/41 [00:07<00:00,  5.24it/s, loss=88, v_num=0, train_loss_step=81.30, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  98%|█████████▊| 40/41 [00:07<00:00,  5.24it/s, loss=88, v_num=0, train_loss_step=81.30, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11:  98%|█████████▊| 40/41 [00:07<00:00,  5.24it/s, loss=88, v_num=0, train_loss_step=88.40, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.33it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.33it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 11: 100%|██████████| 41/41 [00:07<00:00,  5.17it/s, loss=88, v_num=0, train_loss_step=88.40, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11: 100%|██████████| 41/41 [00:07<00:00,  5.17it/s, loss=88, v_num=0, train_loss_step=88.40, val_loss=151.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34mEpoch 11: 100%|██████████| 41/41 [00:08<00:00,  4.84it/s, loss=88, v_num=0, train_loss_step=88.40, val_loss=146.0, train_loss_epoch=91.50]\u001b[0m\n",
      "\u001b[34m#015                                                                      #033[A\u001b[0m\n",
      "\u001b[34mEpoch 11: 100%|██████████| 41/41 [00:08<00:00,  4.84it/s, loss=88, v_num=0, train_loss_step=88.40, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 11:   0%|          | 0/41 [00:00<?, ?it/s, loss=88, v_num=0, train_loss_step=88.40, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:   0%|          | 0/41 [00:00<?, ?it/s, loss=88, v_num=0, train_loss_step=88.40, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:   2%|▏         | 1/41 [00:00<00:07,  5.34it/s, loss=88, v_num=0, train_loss_step=88.40, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:   2%|▏         | 1/41 [00:00<00:07,  5.33it/s, loss=88, v_num=0, train_loss_step=88.40, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:   2%|▏         | 1/41 [00:00<00:07,  5.32it/s, loss=88, v_num=0, train_loss_step=74.10, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:   5%|▍         | 2/41 [00:00<00:07,  5.50it/s, loss=88, v_num=0, train_loss_step=74.10, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:   5%|▍         | 2/41 [00:00<00:07,  5.50it/s, loss=88, v_num=0, train_loss_step=74.10, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:   5%|▍         | 2/41 [00:00<00:07,  5.49it/s, loss=89.5, v_num=0, train_loss_step=98.60, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:   7%|▋         | 3/41 [00:00<00:07,  5.22it/s, loss=89.5, v_num=0, train_loss_step=98.60, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:   7%|▋         | 3/41 [00:00<00:07,  5.22it/s, loss=89.5, v_num=0, train_loss_step=98.60, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:   7%|▋         | 3/41 [00:00<00:07,  5.21it/s, loss=91.1, v_num=0, train_loss_step=105.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  10%|▉         | 4/41 [00:00<00:07,  5.10it/s, loss=91.1, v_num=0, train_loss_step=105.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  10%|▉         | 4/41 [00:00<00:07,  5.10it/s, loss=91.1, v_num=0, train_loss_step=105.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  10%|▉         | 4/41 [00:00<00:07,  5.10it/s, loss=92, v_num=0, train_loss_step=96.20, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  12%|█▏        | 5/41 [00:00<00:07,  5.03it/s, loss=92, v_num=0, train_loss_step=96.20, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  12%|█▏        | 5/41 [00:00<00:07,  5.03it/s, loss=92, v_num=0, train_loss_step=96.20, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  12%|█▏        | 5/41 [00:00<00:07,  5.03it/s, loss=92.3, v_num=0, train_loss_step=88.80, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  15%|█▍        | 6/41 [00:01<00:07,  4.95it/s, loss=92.3, v_num=0, train_loss_step=88.80, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  15%|█▍        | 6/41 [00:01<00:07,  4.95it/s, loss=92.3, v_num=0, train_loss_step=88.80, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  15%|█▍        | 6/41 [00:01<00:07,  4.95it/s, loss=92.4, v_num=0, train_loss_step=89.40, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  17%|█▋        | 7/41 [00:01<00:06,  4.91it/s, loss=92.4, v_num=0, train_loss_step=89.40, val_loss=146.0, train_loss_epoch=89.10]#015Epoch 12:  17%|█▋        | 7/41 [00:01<00:06,  4.91it/s, loss=92.4, v_num=0, train_loss_step=89.40, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  17%|█▋        | 7/41 [00:01<00:06,  4.90it/s, loss=94.2, v_num=0, train_loss_step=122.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  20%|█▉        | 8/41 [00:01<00:06,  4.89it/s, loss=94.2, v_num=0, train_loss_step=122.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  20%|█▉        | 8/41 [00:01<00:06,  4.89it/s, loss=94.2, v_num=0, train_loss_step=122.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  20%|█▉        | 8/41 [00:01<00:06,  4.89it/s, loss=93.3, v_num=0, train_loss_step=75.80, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  22%|██▏       | 9/41 [00:01<00:06,  4.88it/s, loss=93.3, v_num=0, train_loss_step=75.80, val_loss=146.0, train_loss_epoch=89.10]#015Epoch 12:  22%|██▏       | 9/41 [00:01<00:06,  4.88it/s, loss=93.3, v_num=0, train_loss_step=75.80, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  22%|██▏       | 9/41 [00:01<00:06,  4.88it/s, loss=93.2, v_num=0, train_loss_step=79.80, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  24%|██▍       | 10/41 [00:02<00:06,  4.87it/s, loss=93.2, v_num=0, train_loss_step=79.80, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  24%|██▍       | 10/41 [00:02<00:06,  4.87it/s, loss=93.2, v_num=0, train_loss_step=79.80, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  24%|██▍       | 10/41 [00:02<00:06,  4.87it/s, loss=92.8, v_num=0, train_loss_step=79.30, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  27%|██▋       | 11/41 [00:02<00:06,  4.87it/s, loss=92.8, v_num=0, train_loss_step=79.30, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  27%|██▋       | 11/41 [00:02<00:06,  4.87it/s, loss=92.8, v_num=0, train_loss_step=79.30, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  27%|██▋       | 11/41 [00:02<00:06,  4.87it/s, loss=93.2, v_num=0, train_loss_step=109.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  29%|██▉       | 12/41 [00:02<00:05,  4.84it/s, loss=93.2, v_num=0, train_loss_step=109.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  29%|██▉       | 12/41 [00:02<00:05,  4.84it/s, loss=93.2, v_num=0, train_loss_step=109.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  29%|██▉       | 12/41 [00:02<00:05,  4.84it/s, loss=92.2, v_num=0, train_loss_step=80.00, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  32%|███▏      | 13/41 [00:02<00:05,  4.83it/s, loss=92.2, v_num=0, train_loss_step=80.00, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  32%|███▏      | 13/41 [00:02<00:05,  4.82it/s, loss=92.2, v_num=0, train_loss_step=80.00, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  32%|███▏      | 13/41 [00:02<00:05,  4.82it/s, loss=91.3, v_num=0, train_loss_step=80.30, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  34%|███▍      | 14/41 [00:02<00:05,  4.83it/s, loss=91.3, v_num=0, train_loss_step=80.30, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  34%|███▍      | 14/41 [00:02<00:05,  4.82it/s, loss=91.3, v_num=0, train_loss_step=80.30, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  34%|███▍      | 14/41 [00:02<00:05,  4.82it/s, loss=90.1, v_num=0, train_loss_step=79.80, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  37%|███▋      | 15/41 [00:03<00:05,  4.83it/s, loss=90.1, v_num=0, train_loss_step=79.80, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  37%|███▋      | 15/41 [00:03<00:05,  4.83it/s, loss=90.1, v_num=0, train_loss_step=79.80, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  37%|███▋      | 15/41 [00:03<00:05,  4.82it/s, loss=89.7, v_num=0, train_loss_step=91.70, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  39%|███▉      | 16/41 [00:03<00:05,  4.82it/s, loss=89.7, v_num=0, train_loss_step=91.70, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  39%|███▉      | 16/41 [00:03<00:05,  4.82it/s, loss=89.7, v_num=0, train_loss_step=91.70, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  39%|███▉      | 16/41 [00:03<00:05,  4.82it/s, loss=91.2, v_num=0, train_loss_step=109.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  41%|████▏     | 17/41 [00:03<00:04,  4.82it/s, loss=91.2, v_num=0, train_loss_step=109.0, val_loss=146.0, train_loss_epoch=89.10]#015Epoch 12:  41%|████▏     | 17/41 [00:03<00:04,  4.82it/s, loss=91.2, v_num=0, train_loss_step=109.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  41%|████▏     | 17/41 [00:03<00:04,  4.82it/s, loss=92.1, v_num=0, train_loss_step=110.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  44%|████▍     | 18/41 [00:03<00:04,  4.81it/s, loss=92.1, v_num=0, train_loss_step=110.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  44%|████▍     | 18/41 [00:03<00:04,  4.81it/s, loss=92.1, v_num=0, train_loss_step=110.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  44%|████▍     | 18/41 [00:03<00:04,  4.81it/s, loss=91.4, v_num=0, train_loss_step=88.70, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  46%|████▋     | 19/41 [00:03<00:04,  4.80it/s, loss=91.4, v_num=0, train_loss_step=88.70, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  46%|████▋     | 19/41 [00:03<00:04,  4.80it/s, loss=91.4, v_num=0, train_loss_step=88.70, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  46%|████▋     | 19/41 [00:03<00:04,  4.80it/s, loss=91.3, v_num=0, train_loss_step=80.00, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  49%|████▉     | 20/41 [00:04<00:04,  4.81it/s, loss=91.3, v_num=0, train_loss_step=80.00, val_loss=146.0, train_loss_epoch=89.10]#015Epoch 12:  49%|████▉     | 20/41 [00:04<00:04,  4.81it/s, loss=91.3, v_num=0, train_loss_step=80.00, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  49%|████▉     | 20/41 [00:04<00:04,  4.80it/s, loss=91.8, v_num=0, train_loss_step=97.00, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  51%|█████     | 21/41 [00:04<00:04,  4.81it/s, loss=91.8, v_num=0, train_loss_step=97.00, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  51%|█████     | 21/41 [00:04<00:04,  4.81it/s, loss=91.8, v_num=0, train_loss_step=97.00, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  51%|█████     | 21/41 [00:04<00:04,  4.81it/s, loss=92.6, v_num=0, train_loss_step=91.20, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  54%|█████▎    | 22/41 [00:04<00:03,  4.81it/s, loss=92.6, v_num=0, train_loss_step=91.20, val_loss=146.0, train_loss_epoch=89.10]#015Epoch 12:  54%|█████▎    | 22/41 [00:04<00:03,  4.81it/s, loss=92.6, v_num=0, train_loss_step=91.20, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  54%|█████▎    | 22/41 [00:04<00:03,  4.81it/s, loss=93, v_num=0, train_loss_step=106.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  56%|█████▌    | 23/41 [00:04<00:03,  4.81it/s, loss=93, v_num=0, train_loss_step=106.0, val_loss=146.0, train_loss_epoch=89.10]#015Epoch 12:  56%|█████▌    | 23/41 [00:04<00:03,  4.81it/s, loss=93, v_num=0, train_loss_step=106.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  56%|█████▌    | 23/41 [00:04<00:03,  4.80it/s, loss=92.8, v_num=0, train_loss_step=102.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  59%|█████▊    | 24/41 [00:05<00:03,  4.79it/s, loss=92.8, v_num=0, train_loss_step=102.0, val_loss=146.0, train_loss_epoch=89.10]#015Epoch 12:  59%|█████▊    | 24/41 [00:05<00:03,  4.79it/s, loss=92.8, v_num=0, train_loss_step=102.0, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  59%|█████▊    | 24/41 [00:05<00:03,  4.79it/s, loss=92.2, v_num=0, train_loss_step=82.60, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  61%|██████    | 25/41 [00:05<00:03,  4.79it/s, loss=92.2, v_num=0, train_loss_step=82.60, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  61%|██████    | 25/41 [00:05<00:03,  4.79it/s, loss=92.2, v_num=0, train_loss_step=82.60, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  61%|██████    | 25/41 [00:05<00:03,  4.79it/s, loss=91.1, v_num=0, train_loss_step=68.50, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  63%|██████▎   | 26/41 [00:05<00:03,  4.79it/s, loss=91.1, v_num=0, train_loss_step=68.50, val_loss=146.0, train_loss_epoch=89.10]#015Epoch 12:  63%|██████▎   | 26/41 [00:05<00:03,  4.79it/s, loss=91.1, v_num=0, train_loss_step=68.50, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  63%|██████▎   | 26/41 [00:05<00:03,  4.79it/s, loss=90.6, v_num=0, train_loss_step=78.70, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  66%|██████▌   | 27/41 [00:05<00:02,  4.79it/s, loss=90.6, v_num=0, train_loss_step=78.70, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  66%|██████▌   | 27/41 [00:05<00:02,  4.79it/s, loss=90.6, v_num=0, train_loss_step=78.70, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  66%|██████▌   | 27/41 [00:05<00:02,  4.79it/s, loss=88.8, v_num=0, train_loss_step=87.30, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  68%|██████▊   | 28/41 [00:05<00:02,  4.82it/s, loss=88.8, v_num=0, train_loss_step=87.30, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  68%|██████▊   | 28/41 [00:05<00:02,  4.82it/s, loss=88.8, v_num=0, train_loss_step=87.30, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  68%|██████▊   | 28/41 [00:05<00:02,  4.82it/s, loss=89.2, v_num=0, train_loss_step=83.50, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  71%|███████   | 29/41 [00:05<00:02,  4.84it/s, loss=89.2, v_num=0, train_loss_step=83.50, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  71%|███████   | 29/41 [00:05<00:02,  4.84it/s, loss=89.2, v_num=0, train_loss_step=83.50, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  71%|███████   | 29/41 [00:05<00:02,  4.84it/s, loss=88.8, v_num=0, train_loss_step=72.10, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  73%|███████▎  | 30/41 [00:06<00:02,  4.86it/s, loss=88.8, v_num=0, train_loss_step=72.10, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  73%|███████▎  | 30/41 [00:06<00:02,  4.86it/s, loss=88.8, v_num=0, train_loss_step=72.10, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  73%|███████▎  | 30/41 [00:06<00:02,  4.86it/s, loss=88.1, v_num=0, train_loss_step=65.00, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  76%|███████▌  | 31/41 [00:06<00:02,  4.89it/s, loss=88.1, v_num=0, train_loss_step=65.00, val_loss=146.0, train_loss_epoch=89.10]#015Epoch 12:  76%|███████▌  | 31/41 [00:06<00:02,  4.88it/s, loss=88.1, v_num=0, train_loss_step=65.00, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  76%|███████▌  | 31/41 [00:06<00:02,  4.88it/s, loss=86.3, v_num=0, train_loss_step=71.80, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  78%|███████▊  | 32/41 [00:06<00:01,  4.91it/s, loss=86.3, v_num=0, train_loss_step=71.80, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  78%|███████▊  | 32/41 [00:06<00:01,  4.91it/s, loss=86.3, v_num=0, train_loss_step=71.80, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  78%|███████▊  | 32/41 [00:06<00:01,  4.91it/s, loss=85.9, v_num=0, train_loss_step=72.60, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  80%|████████  | 33/41 [00:06<00:01,  4.93it/s, loss=85.9, v_num=0, train_loss_step=72.60, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  80%|████████  | 33/41 [00:06<00:01,  4.93it/s, loss=85.9, v_num=0, train_loss_step=72.60, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  80%|████████  | 33/41 [00:06<00:01,  4.93it/s, loss=86.1, v_num=0, train_loss_step=83.60, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  83%|████████▎ | 34/41 [00:06<00:01,  4.95it/s, loss=86.1, v_num=0, train_loss_step=83.60, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  83%|████████▎ | 34/41 [00:06<00:01,  4.95it/s, loss=86.1, v_num=0, train_loss_step=83.60, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  83%|████████▎ | 34/41 [00:06<00:01,  4.95it/s, loss=86.9, v_num=0, train_loss_step=97.00, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  85%|████████▌ | 35/41 [00:07<00:01,  4.97it/s, loss=86.9, v_num=0, train_loss_step=97.00, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  85%|████████▌ | 35/41 [00:07<00:01,  4.97it/s, loss=86.9, v_num=0, train_loss_step=97.00, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  85%|████████▌ | 35/41 [00:07<00:01,  4.97it/s, loss=86.6, v_num=0, train_loss_step=86.00, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  88%|████████▊ | 36/41 [00:07<00:01,  4.98it/s, loss=86.6, v_num=0, train_loss_step=86.00, val_loss=146.0, train_loss_epoch=89.10]#015Epoch 12:  88%|████████▊ | 36/41 [00:07<00:01,  4.98it/s, loss=86.6, v_num=0, train_loss_step=86.00, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  88%|████████▊ | 36/41 [00:07<00:01,  4.98it/s, loss=84.3, v_num=0, train_loss_step=62.70, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  90%|█████████ | 37/41 [00:07<00:00,  5.00it/s, loss=84.3, v_num=0, train_loss_step=62.70, val_loss=146.0, train_loss_epoch=89.10]#015Epoch 12:  90%|█████████ | 37/41 [00:07<00:00,  5.00it/s, loss=84.3, v_num=0, train_loss_step=62.70, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  90%|█████████ | 37/41 [00:07<00:00,  5.00it/s, loss=83.5, v_num=0, train_loss_step=93.90, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  93%|█████████▎| 38/41 [00:07<00:00,  5.01it/s, loss=83.5, v_num=0, train_loss_step=93.90, val_loss=146.0, train_loss_epoch=89.10]#015Epoch 12:  93%|█████████▎| 38/41 [00:07<00:00,  5.01it/s, loss=83.5, v_num=0, train_loss_step=93.90, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  93%|█████████▎| 38/41 [00:07<00:00,  5.01it/s, loss=82.6, v_num=0, train_loss_step=70.10, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  95%|█████████▌| 39/41 [00:07<00:00,  5.03it/s, loss=82.6, v_num=0, train_loss_step=70.10, val_loss=146.0, train_loss_epoch=89.10]#015Epoch 12:  95%|█████████▌| 39/41 [00:07<00:00,  5.03it/s, loss=82.6, v_num=0, train_loss_step=70.10, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  95%|█████████▌| 39/41 [00:07<00:00,  5.03it/s, loss=83.2, v_num=0, train_loss_step=92.30, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  98%|█████████▊| 40/41 [00:07<00:00,  5.04it/s, loss=83.2, v_num=0, train_loss_step=92.30, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  98%|█████████▊| 40/41 [00:07<00:00,  5.04it/s, loss=83.2, v_num=0, train_loss_step=92.30, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12:  98%|█████████▊| 40/41 [00:07<00:00,  5.04it/s, loss=82.5, v_num=0, train_loss_step=83.50, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.37it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 12: 100%|██████████| 41/41 [00:08<00:00,  4.99it/s, loss=82.5, v_num=0, train_loss_step=83.50, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12: 100%|██████████| 41/41 [00:08<00:00,  4.99it/s, loss=82.5, v_num=0, train_loss_step=83.50, val_loss=146.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34mEpoch 12: 100%|██████████| 41/41 [00:08<00:00,  4.59it/s, loss=82.5, v_num=0, train_loss_step=83.50, val_loss=143.0, train_loss_epoch=89.10]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 12: 100%|██████████| 41/41 [00:08<00:00,  4.58it/s, loss=82.5, v_num=0, train_loss_step=83.50, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 12:   0%|          | 0/41 [00:00<?, ?it/s, loss=82.5, v_num=0, train_loss_step=83.50, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:   0%|          | 0/41 [00:00<?, ?it/s, loss=82.5, v_num=0, train_loss_step=83.50, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:   2%|▏         | 1/41 [00:00<00:07,  5.49it/s, loss=82.5, v_num=0, train_loss_step=83.50, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:   2%|▏         | 1/41 [00:00<00:07,  5.48it/s, loss=82.5, v_num=0, train_loss_step=83.50, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:   2%|▏         | 1/41 [00:00<00:07,  5.47it/s, loss=82.6, v_num=0, train_loss_step=92.40, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:   5%|▍         | 2/41 [00:00<00:07,  5.31it/s, loss=82.6, v_num=0, train_loss_step=92.40, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:   5%|▍         | 2/41 [00:00<00:07,  5.30it/s, loss=82.6, v_num=0, train_loss_step=92.40, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:   5%|▍         | 2/41 [00:00<00:07,  5.30it/s, loss=82.6, v_num=0, train_loss_step=108.0, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:   7%|▋         | 3/41 [00:00<00:07,  5.35it/s, loss=82.6, v_num=0, train_loss_step=108.0, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:   7%|▋         | 3/41 [00:00<00:07,  5.35it/s, loss=82.6, v_num=0, train_loss_step=108.0, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:   7%|▋         | 3/41 [00:00<00:07,  5.34it/s, loss=81.7, v_num=0, train_loss_step=83.50, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  10%|▉         | 4/41 [00:00<00:06,  5.40it/s, loss=81.7, v_num=0, train_loss_step=83.50, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  10%|▉         | 4/41 [00:00<00:06,  5.39it/s, loss=81.7, v_num=0, train_loss_step=83.50, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  10%|▉         | 4/41 [00:00<00:06,  5.39it/s, loss=81.1, v_num=0, train_loss_step=70.10, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  12%|█▏        | 5/41 [00:00<00:06,  5.32it/s, loss=81.1, v_num=0, train_loss_step=70.10, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  12%|█▏        | 5/41 [00:00<00:06,  5.32it/s, loss=81.1, v_num=0, train_loss_step=70.10, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  12%|█▏        | 5/41 [00:00<00:06,  5.32it/s, loss=82.1, v_num=0, train_loss_step=87.80, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  15%|█▍        | 6/41 [00:01<00:06,  5.35it/s, loss=82.1, v_num=0, train_loss_step=87.80, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  15%|█▍        | 6/41 [00:01<00:06,  5.35it/s, loss=82.1, v_num=0, train_loss_step=87.80, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  15%|█▍        | 6/41 [00:01<00:06,  5.34it/s, loss=81.8, v_num=0, train_loss_step=73.90, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  17%|█▋        | 7/41 [00:01<00:06,  5.38it/s, loss=81.8, v_num=0, train_loss_step=73.90, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  17%|█▋        | 7/41 [00:01<00:06,  5.38it/s, loss=81.8, v_num=0, train_loss_step=73.90, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  17%|█▋        | 7/41 [00:01<00:06,  5.37it/s, loss=81.6, v_num=0, train_loss_step=82.20, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  20%|█▉        | 8/41 [00:01<00:06,  5.35it/s, loss=81.6, v_num=0, train_loss_step=82.20, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  20%|█▉        | 8/41 [00:01<00:06,  5.35it/s, loss=81.6, v_num=0, train_loss_step=82.20, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  20%|█▉        | 8/41 [00:01<00:06,  5.35it/s, loss=81.2, v_num=0, train_loss_step=76.90, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  22%|██▏       | 9/41 [00:01<00:05,  5.39it/s, loss=81.2, v_num=0, train_loss_step=76.90, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  22%|██▏       | 9/41 [00:01<00:05,  5.39it/s, loss=81.2, v_num=0, train_loss_step=76.90, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  22%|██▏       | 9/41 [00:01<00:05,  5.39it/s, loss=82.1, v_num=0, train_loss_step=88.80, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  24%|██▍       | 10/41 [00:01<00:05,  5.43it/s, loss=82.1, v_num=0, train_loss_step=88.80, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  24%|██▍       | 10/41 [00:01<00:05,  5.43it/s, loss=82.1, v_num=0, train_loss_step=88.80, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  24%|██▍       | 10/41 [00:01<00:05,  5.43it/s, loss=84, v_num=0, train_loss_step=104.0, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  27%|██▋       | 11/41 [00:02<00:05,  5.46it/s, loss=84, v_num=0, train_loss_step=104.0, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  27%|██▋       | 11/41 [00:02<00:05,  5.45it/s, loss=84, v_num=0, train_loss_step=104.0, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  27%|██▋       | 11/41 [00:02<00:05,  5.45it/s, loss=84.3, v_num=0, train_loss_step=77.10, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  29%|██▉       | 12/41 [00:02<00:05,  5.47it/s, loss=84.3, v_num=0, train_loss_step=77.10, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  29%|██▉       | 12/41 [00:02<00:05,  5.47it/s, loss=84.3, v_num=0, train_loss_step=77.10, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  29%|██▉       | 12/41 [00:02<00:05,  5.47it/s, loss=85.3, v_num=0, train_loss_step=92.10, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  32%|███▏      | 13/41 [00:02<00:05,  5.47it/s, loss=85.3, v_num=0, train_loss_step=92.10, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  32%|███▏      | 13/41 [00:02<00:05,  5.47it/s, loss=85.3, v_num=0, train_loss_step=92.10, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  32%|███▏      | 13/41 [00:02<00:05,  5.47it/s, loss=85.3, v_num=0, train_loss_step=84.40, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  34%|███▍      | 14/41 [00:02<00:04,  5.46it/s, loss=85.3, v_num=0, train_loss_step=84.40, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  34%|███▍      | 14/41 [00:02<00:04,  5.46it/s, loss=85.3, v_num=0, train_loss_step=84.40, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  34%|███▍      | 14/41 [00:02<00:04,  5.46it/s, loss=84.4, v_num=0, train_loss_step=79.40, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  37%|███▋      | 15/41 [00:02<00:04,  5.47it/s, loss=84.4, v_num=0, train_loss_step=79.40, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  37%|███▋      | 15/41 [00:02<00:04,  5.47it/s, loss=84.4, v_num=0, train_loss_step=79.40, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  37%|███▋      | 15/41 [00:02<00:04,  5.47it/s, loss=83.8, v_num=0, train_loss_step=74.20, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  39%|███▉      | 16/41 [00:02<00:04,  5.48it/s, loss=83.8, v_num=0, train_loss_step=74.20, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  39%|███▉      | 16/41 [00:02<00:04,  5.48it/s, loss=83.8, v_num=0, train_loss_step=74.20, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  39%|███▉      | 16/41 [00:02<00:04,  5.48it/s, loss=84.5, v_num=0, train_loss_step=75.50, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  41%|████▏     | 17/41 [00:03<00:04,  5.49it/s, loss=84.5, v_num=0, train_loss_step=75.50, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  41%|████▏     | 17/41 [00:03<00:04,  5.49it/s, loss=84.5, v_num=0, train_loss_step=75.50, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  41%|████▏     | 17/41 [00:03<00:04,  5.49it/s, loss=85, v_num=0, train_loss_step=104.0, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  44%|████▍     | 18/41 [00:03<00:04,  5.51it/s, loss=85, v_num=0, train_loss_step=104.0, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  44%|████▍     | 18/41 [00:03<00:04,  5.51it/s, loss=85, v_num=0, train_loss_step=104.0, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  44%|████▍     | 18/41 [00:03<00:04,  5.51it/s, loss=85.2, v_num=0, train_loss_step=74.60, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  46%|████▋     | 19/41 [00:03<00:03,  5.52it/s, loss=85.2, v_num=0, train_loss_step=74.60, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  46%|████▋     | 19/41 [00:03<00:03,  5.52it/s, loss=85.2, v_num=0, train_loss_step=74.60, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  46%|████▋     | 19/41 [00:03<00:03,  5.52it/s, loss=85.1, v_num=0, train_loss_step=90.90, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  49%|████▉     | 20/41 [00:03<00:03,  5.52it/s, loss=85.1, v_num=0, train_loss_step=90.90, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  49%|████▉     | 20/41 [00:03<00:03,  5.51it/s, loss=85.1, v_num=0, train_loss_step=90.90, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  49%|████▉     | 20/41 [00:03<00:03,  5.51it/s, loss=86.1, v_num=0, train_loss_step=102.0, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  51%|█████     | 21/41 [00:03<00:03,  5.52it/s, loss=86.1, v_num=0, train_loss_step=102.0, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  51%|█████     | 21/41 [00:03<00:03,  5.52it/s, loss=86.1, v_num=0, train_loss_step=102.0, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  51%|█████     | 21/41 [00:03<00:03,  5.52it/s, loss=85, v_num=0, train_loss_step=71.80, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  54%|█████▎    | 22/41 [00:03<00:03,  5.53it/s, loss=85, v_num=0, train_loss_step=71.80, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  54%|█████▎    | 22/41 [00:03<00:03,  5.53it/s, loss=85, v_num=0, train_loss_step=71.80, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  54%|█████▎    | 22/41 [00:03<00:03,  5.53it/s, loss=82.9, v_num=0, train_loss_step=65.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  56%|█████▌    | 23/41 [00:04<00:03,  5.54it/s, loss=82.9, v_num=0, train_loss_step=65.00, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  56%|█████▌    | 23/41 [00:04<00:03,  5.54it/s, loss=82.9, v_num=0, train_loss_step=65.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  56%|█████▌    | 23/41 [00:04<00:03,  5.54it/s, loss=83, v_num=0, train_loss_step=86.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  59%|█████▊    | 24/41 [00:04<00:03,  5.55it/s, loss=83, v_num=0, train_loss_step=86.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  59%|█████▊    | 24/41 [00:04<00:03,  5.55it/s, loss=83, v_num=0, train_loss_step=86.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  59%|█████▊    | 24/41 [00:04<00:03,  5.55it/s, loss=84.4, v_num=0, train_loss_step=97.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  61%|██████    | 25/41 [00:04<00:02,  5.55it/s, loss=84.4, v_num=0, train_loss_step=97.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  61%|██████    | 25/41 [00:04<00:02,  5.55it/s, loss=84.4, v_num=0, train_loss_step=97.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  61%|██████    | 25/41 [00:04<00:02,  5.55it/s, loss=83.8, v_num=0, train_loss_step=76.90, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  63%|██████▎   | 26/41 [00:04<00:02,  5.55it/s, loss=83.8, v_num=0, train_loss_step=76.90, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  63%|██████▎   | 26/41 [00:04<00:02,  5.55it/s, loss=83.8, v_num=0, train_loss_step=76.90, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  63%|██████▎   | 26/41 [00:04<00:02,  5.55it/s, loss=85.5, v_num=0, train_loss_step=107.0, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  66%|██████▌   | 27/41 [00:04<00:02,  5.55it/s, loss=85.5, v_num=0, train_loss_step=107.0, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  66%|██████▌   | 27/41 [00:04<00:02,  5.55it/s, loss=85.5, v_num=0, train_loss_step=107.0, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  66%|██████▌   | 27/41 [00:04<00:02,  5.55it/s, loss=86.1, v_num=0, train_loss_step=95.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  68%|██████▊   | 28/41 [00:05<00:02,  5.55it/s, loss=86.1, v_num=0, train_loss_step=95.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  68%|██████▊   | 28/41 [00:05<00:02,  5.55it/s, loss=86.1, v_num=0, train_loss_step=95.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  68%|██████▊   | 28/41 [00:05<00:02,  5.55it/s, loss=86, v_num=0, train_loss_step=73.90, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  71%|███████   | 29/41 [00:05<00:02,  5.56it/s, loss=86, v_num=0, train_loss_step=73.90, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  71%|███████   | 29/41 [00:05<00:02,  5.56it/s, loss=86, v_num=0, train_loss_step=73.90, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  71%|███████   | 29/41 [00:05<00:02,  5.56it/s, loss=86.1, v_num=0, train_loss_step=91.20, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  73%|███████▎  | 30/41 [00:05<00:01,  5.56it/s, loss=86.1, v_num=0, train_loss_step=91.20, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  73%|███████▎  | 30/41 [00:05<00:01,  5.56it/s, loss=86.1, v_num=0, train_loss_step=91.20, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  73%|███████▎  | 30/41 [00:05<00:01,  5.56it/s, loss=85, v_num=0, train_loss_step=81.60, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  76%|███████▌  | 31/41 [00:05<00:01,  5.57it/s, loss=85, v_num=0, train_loss_step=81.60, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  76%|███████▌  | 31/41 [00:05<00:01,  5.57it/s, loss=85, v_num=0, train_loss_step=81.60, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  76%|███████▌  | 31/41 [00:05<00:01,  5.57it/s, loss=86.1, v_num=0, train_loss_step=98.60, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  78%|███████▊  | 32/41 [00:05<00:01,  5.56it/s, loss=86.1, v_num=0, train_loss_step=98.60, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  78%|███████▊  | 32/41 [00:05<00:01,  5.56it/s, loss=86.1, v_num=0, train_loss_step=98.60, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  78%|███████▊  | 32/41 [00:05<00:01,  5.56it/s, loss=84.7, v_num=0, train_loss_step=65.30, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  80%|████████  | 33/41 [00:05<00:01,  5.57it/s, loss=84.7, v_num=0, train_loss_step=65.30, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  80%|████████  | 33/41 [00:05<00:01,  5.57it/s, loss=84.7, v_num=0, train_loss_step=65.30, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  80%|████████  | 33/41 [00:05<00:01,  5.57it/s, loss=83.8, v_num=0, train_loss_step=66.40, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  83%|████████▎ | 34/41 [00:06<00:01,  5.57it/s, loss=83.8, v_num=0, train_loss_step=66.40, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  83%|████████▎ | 34/41 [00:06<00:01,  5.57it/s, loss=83.8, v_num=0, train_loss_step=66.40, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  83%|████████▎ | 34/41 [00:06<00:01,  5.57it/s, loss=84, v_num=0, train_loss_step=82.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  85%|████████▌ | 35/41 [00:06<00:01,  5.57it/s, loss=84, v_num=0, train_loss_step=82.00, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  85%|████████▌ | 35/41 [00:06<00:01,  5.57it/s, loss=84, v_num=0, train_loss_step=82.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  85%|████████▌ | 35/41 [00:06<00:01,  5.57it/s, loss=85.6, v_num=0, train_loss_step=107.0, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  88%|████████▊ | 36/41 [00:06<00:00,  5.58it/s, loss=85.6, v_num=0, train_loss_step=107.0, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  88%|████████▊ | 36/41 [00:06<00:00,  5.58it/s, loss=85.6, v_num=0, train_loss_step=107.0, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  88%|████████▊ | 36/41 [00:06<00:00,  5.58it/s, loss=86.3, v_num=0, train_loss_step=89.30, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  90%|█████████ | 37/41 [00:06<00:00,  5.58it/s, loss=86.3, v_num=0, train_loss_step=89.30, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  90%|█████████ | 37/41 [00:06<00:00,  5.58it/s, loss=86.3, v_num=0, train_loss_step=89.30, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  90%|█████████ | 37/41 [00:06<00:00,  5.58it/s, loss=85.2, v_num=0, train_loss_step=83.40, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  93%|█████████▎| 38/41 [00:06<00:00,  5.57it/s, loss=85.2, v_num=0, train_loss_step=83.40, val_loss=143.0, train_loss_epoch=86.50]#015Epoch 13:  93%|█████████▎| 38/41 [00:06<00:00,  5.57it/s, loss=85.2, v_num=0, train_loss_step=83.40, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  93%|█████████▎| 38/41 [00:06<00:00,  5.57it/s, loss=85.7, v_num=0, train_loss_step=83.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  95%|█████████▌| 39/41 [00:06<00:00,  5.58it/s, loss=85.7, v_num=0, train_loss_step=83.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  95%|█████████▌| 39/41 [00:06<00:00,  5.58it/s, loss=85.7, v_num=0, train_loss_step=83.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  95%|█████████▌| 39/41 [00:06<00:00,  5.58it/s, loss=85.1, v_num=0, train_loss_step=80.50, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  98%|█████████▊| 40/41 [00:07<00:00,  5.58it/s, loss=85.1, v_num=0, train_loss_step=80.50, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  98%|█████████▊| 40/41 [00:07<00:00,  5.58it/s, loss=85.1, v_num=0, train_loss_step=80.50, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13:  98%|█████████▊| 40/41 [00:07<00:00,  5.58it/s, loss=83.6, v_num=0, train_loss_step=72.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 13: 100%|██████████| 41/41 [00:07<00:00,  5.47it/s, loss=83.6, v_num=0, train_loss_step=72.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13: 100%|██████████| 41/41 [00:07<00:00,  5.47it/s, loss=83.6, v_num=0, train_loss_step=72.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34mEpoch 13: 100%|██████████| 41/41 [00:08<00:00,  5.10it/s, loss=83.6, v_num=0, train_loss_step=72.00, val_loss=143.0, train_loss_epoch=86.50]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 13: 100%|██████████| 41/41 [00:08<00:00,  5.10it/s, loss=83.6, v_num=0, train_loss_step=72.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 13:   0%|          | 0/41 [00:00<?, ?it/s, loss=83.6, v_num=0, train_loss_step=72.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:   0%|          | 0/41 [00:00<?, ?it/s, loss=83.6, v_num=0, train_loss_step=72.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:   2%|▏         | 1/41 [00:00<00:06,  5.81it/s, loss=83.6, v_num=0, train_loss_step=72.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:   2%|▏         | 1/41 [00:00<00:06,  5.81it/s, loss=83.6, v_num=0, train_loss_step=72.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:   2%|▏         | 1/41 [00:00<00:06,  5.80it/s, loss=84.7, v_num=0, train_loss_step=93.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:   5%|▍         | 2/41 [00:00<00:06,  5.78it/s, loss=84.7, v_num=0, train_loss_step=93.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:   5%|▍         | 2/41 [00:00<00:06,  5.78it/s, loss=84.7, v_num=0, train_loss_step=93.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:   5%|▍         | 2/41 [00:00<00:06,  5.77it/s, loss=85.8, v_num=0, train_loss_step=86.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:   7%|▋         | 3/41 [00:00<00:06,  5.74it/s, loss=85.8, v_num=0, train_loss_step=86.70, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:   7%|▋         | 3/41 [00:00<00:06,  5.73it/s, loss=85.8, v_num=0, train_loss_step=86.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:   7%|▋         | 3/41 [00:00<00:06,  5.73it/s, loss=86.2, v_num=0, train_loss_step=94.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  10%|▉         | 4/41 [00:00<00:06,  5.65it/s, loss=86.2, v_num=0, train_loss_step=94.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  10%|▉         | 4/41 [00:00<00:06,  5.65it/s, loss=86.2, v_num=0, train_loss_step=94.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  10%|▉         | 4/41 [00:00<00:06,  5.64it/s, loss=85.1, v_num=0, train_loss_step=74.30, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  12%|█▏        | 5/41 [00:00<00:06,  5.66it/s, loss=85.1, v_num=0, train_loss_step=74.30, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  12%|█▏        | 5/41 [00:00<00:06,  5.66it/s, loss=85.1, v_num=0, train_loss_step=74.30, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  12%|█▏        | 5/41 [00:00<00:06,  5.66it/s, loss=85.8, v_num=0, train_loss_step=90.50, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  15%|█▍        | 6/41 [00:01<00:06,  5.67it/s, loss=85.8, v_num=0, train_loss_step=90.50, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  15%|█▍        | 6/41 [00:01<00:06,  5.67it/s, loss=85.8, v_num=0, train_loss_step=90.50, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  15%|█▍        | 6/41 [00:01<00:06,  5.67it/s, loss=84.9, v_num=0, train_loss_step=89.20, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  17%|█▋        | 7/41 [00:01<00:06,  5.67it/s, loss=84.9, v_num=0, train_loss_step=89.20, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  17%|█▋        | 7/41 [00:01<00:06,  5.67it/s, loss=84.9, v_num=0, train_loss_step=89.20, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  17%|█▋        | 7/41 [00:01<00:06,  5.66it/s, loss=83.6, v_num=0, train_loss_step=70.30, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  20%|█▉        | 8/41 [00:01<00:05,  5.69it/s, loss=83.6, v_num=0, train_loss_step=70.30, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  20%|█▉        | 8/41 [00:01<00:05,  5.69it/s, loss=83.6, v_num=0, train_loss_step=70.30, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  20%|█▉        | 8/41 [00:01<00:05,  5.69it/s, loss=83.4, v_num=0, train_loss_step=68.40, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  22%|██▏       | 9/41 [00:01<00:05,  5.68it/s, loss=83.4, v_num=0, train_loss_step=68.40, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  22%|██▏       | 9/41 [00:01<00:05,  5.68it/s, loss=83.4, v_num=0, train_loss_step=68.40, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  22%|██▏       | 9/41 [00:01<00:05,  5.68it/s, loss=82.5, v_num=0, train_loss_step=74.90, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  24%|██▍       | 10/41 [00:01<00:05,  5.65it/s, loss=82.5, v_num=0, train_loss_step=74.90, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  24%|██▍       | 10/41 [00:01<00:05,  5.65it/s, loss=82.5, v_num=0, train_loss_step=74.90, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  24%|██▍       | 10/41 [00:01<00:05,  5.65it/s, loss=82.8, v_num=0, train_loss_step=87.30, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  27%|██▋       | 11/41 [00:01<00:05,  5.66it/s, loss=82.8, v_num=0, train_loss_step=87.30, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  27%|██▋       | 11/41 [00:01<00:05,  5.66it/s, loss=82.8, v_num=0, train_loss_step=87.30, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  27%|██▋       | 11/41 [00:01<00:05,  5.66it/s, loss=81.6, v_num=0, train_loss_step=74.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  29%|██▉       | 12/41 [00:02<00:05,  5.67it/s, loss=81.6, v_num=0, train_loss_step=74.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  29%|██▉       | 12/41 [00:02<00:05,  5.66it/s, loss=81.6, v_num=0, train_loss_step=74.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  29%|██▉       | 12/41 [00:02<00:05,  5.66it/s, loss=83, v_num=0, train_loss_step=93.40, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  32%|███▏      | 13/41 [00:02<00:04,  5.66it/s, loss=83, v_num=0, train_loss_step=93.40, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  32%|███▏      | 13/41 [00:02<00:04,  5.66it/s, loss=83, v_num=0, train_loss_step=93.40, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  32%|███▏      | 13/41 [00:02<00:04,  5.66it/s, loss=84, v_num=0, train_loss_step=87.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  34%|███▍      | 14/41 [00:02<00:04,  5.67it/s, loss=84, v_num=0, train_loss_step=87.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  34%|███▍      | 14/41 [00:02<00:04,  5.67it/s, loss=84, v_num=0, train_loss_step=87.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  34%|███▍      | 14/41 [00:02<00:04,  5.67it/s, loss=83.9, v_num=0, train_loss_step=78.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  37%|███▋      | 15/41 [00:02<00:04,  5.67it/s, loss=83.9, v_num=0, train_loss_step=78.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  37%|███▋      | 15/41 [00:02<00:04,  5.67it/s, loss=83.9, v_num=0, train_loss_step=78.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  37%|███▋      | 15/41 [00:02<00:04,  5.67it/s, loss=82.6, v_num=0, train_loss_step=80.90, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  39%|███▉      | 16/41 [00:02<00:04,  5.65it/s, loss=82.6, v_num=0, train_loss_step=80.90, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  39%|███▉      | 16/41 [00:02<00:04,  5.65it/s, loss=82.6, v_num=0, train_loss_step=80.90, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  39%|███▉      | 16/41 [00:02<00:04,  5.65it/s, loss=82.8, v_num=0, train_loss_step=94.10, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  41%|████▏     | 17/41 [00:03<00:04,  5.65it/s, loss=82.8, v_num=0, train_loss_step=94.10, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  41%|████▏     | 17/41 [00:03<00:04,  5.65it/s, loss=82.8, v_num=0, train_loss_step=94.10, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  41%|████▏     | 17/41 [00:03<00:04,  5.65it/s, loss=82.3, v_num=0, train_loss_step=72.80, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  44%|████▍     | 18/41 [00:03<00:04,  5.66it/s, loss=82.3, v_num=0, train_loss_step=72.80, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  44%|████▍     | 18/41 [00:03<00:04,  5.65it/s, loss=82.3, v_num=0, train_loss_step=72.80, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  44%|████▍     | 18/41 [00:03<00:04,  5.65it/s, loss=81.7, v_num=0, train_loss_step=70.90, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  46%|████▋     | 19/41 [00:03<00:03,  5.65it/s, loss=81.7, v_num=0, train_loss_step=70.90, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  46%|████▋     | 19/41 [00:03<00:03,  5.65it/s, loss=81.7, v_num=0, train_loss_step=70.90, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  46%|████▋     | 19/41 [00:03<00:03,  5.64it/s, loss=81.4, v_num=0, train_loss_step=75.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  49%|████▉     | 20/41 [00:03<00:03,  5.65it/s, loss=81.4, v_num=0, train_loss_step=75.70, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  49%|████▉     | 20/41 [00:03<00:03,  5.65it/s, loss=81.4, v_num=0, train_loss_step=75.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  49%|████▉     | 20/41 [00:03<00:03,  5.64it/s, loss=83.1, v_num=0, train_loss_step=105.0, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  51%|█████     | 21/41 [00:03<00:03,  5.65it/s, loss=83.1, v_num=0, train_loss_step=105.0, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  51%|█████     | 21/41 [00:03<00:03,  5.65it/s, loss=83.1, v_num=0, train_loss_step=105.0, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  51%|█████     | 21/41 [00:03<00:03,  5.64it/s, loss=82.1, v_num=0, train_loss_step=73.50, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  54%|█████▎    | 22/41 [00:03<00:03,  5.63it/s, loss=82.1, v_num=0, train_loss_step=73.50, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  54%|█████▎    | 22/41 [00:03<00:03,  5.63it/s, loss=82.1, v_num=0, train_loss_step=73.50, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  54%|█████▎    | 22/41 [00:03<00:03,  5.63it/s, loss=83.2, v_num=0, train_loss_step=108.0, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  56%|█████▌    | 23/41 [00:04<00:03,  5.63it/s, loss=83.2, v_num=0, train_loss_step=108.0, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  56%|█████▌    | 23/41 [00:04<00:03,  5.63it/s, loss=83.2, v_num=0, train_loss_step=108.0, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  56%|█████▌    | 23/41 [00:04<00:03,  5.63it/s, loss=82.7, v_num=0, train_loss_step=84.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  59%|█████▊    | 24/41 [00:04<00:03,  5.63it/s, loss=82.7, v_num=0, train_loss_step=84.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  59%|█████▊    | 24/41 [00:04<00:03,  5.63it/s, loss=82.7, v_num=0, train_loss_step=84.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  59%|█████▊    | 24/41 [00:04<00:03,  5.63it/s, loss=83.1, v_num=0, train_loss_step=82.90, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  61%|██████    | 25/41 [00:04<00:02,  5.64it/s, loss=83.1, v_num=0, train_loss_step=82.90, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  61%|██████    | 25/41 [00:04<00:02,  5.63it/s, loss=83.1, v_num=0, train_loss_step=82.90, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  61%|██████    | 25/41 [00:04<00:02,  5.63it/s, loss=83.8, v_num=0, train_loss_step=105.0, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  63%|██████▎   | 26/41 [00:04<00:02,  5.63it/s, loss=83.8, v_num=0, train_loss_step=105.0, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  63%|██████▎   | 26/41 [00:04<00:02,  5.63it/s, loss=83.8, v_num=0, train_loss_step=105.0, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  63%|██████▎   | 26/41 [00:04<00:02,  5.63it/s, loss=83.2, v_num=0, train_loss_step=76.30, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  66%|██████▌   | 27/41 [00:04<00:02,  5.63it/s, loss=83.2, v_num=0, train_loss_step=76.30, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  66%|██████▌   | 27/41 [00:04<00:02,  5.63it/s, loss=83.2, v_num=0, train_loss_step=76.30, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  66%|██████▌   | 27/41 [00:04<00:02,  5.63it/s, loss=84.3, v_num=0, train_loss_step=92.80, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  68%|██████▊   | 28/41 [00:04<00:02,  5.62it/s, loss=84.3, v_num=0, train_loss_step=92.80, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  68%|██████▊   | 28/41 [00:04<00:02,  5.62it/s, loss=84.3, v_num=0, train_loss_step=92.80, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  68%|██████▊   | 28/41 [00:04<00:02,  5.62it/s, loss=84.8, v_num=0, train_loss_step=78.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  71%|███████   | 29/41 [00:05<00:02,  5.62it/s, loss=84.8, v_num=0, train_loss_step=78.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  71%|███████   | 29/41 [00:05<00:02,  5.62it/s, loss=84.8, v_num=0, train_loss_step=78.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  71%|███████   | 29/41 [00:05<00:02,  5.62it/s, loss=84.8, v_num=0, train_loss_step=74.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  73%|███████▎  | 30/41 [00:05<00:01,  5.59it/s, loss=84.8, v_num=0, train_loss_step=74.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  73%|███████▎  | 30/41 [00:05<00:01,  5.59it/s, loss=84.8, v_num=0, train_loss_step=74.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  73%|███████▎  | 30/41 [00:05<00:01,  5.59it/s, loss=84, v_num=0, train_loss_step=72.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  76%|███████▌  | 31/41 [00:05<00:01,  5.56it/s, loss=84, v_num=0, train_loss_step=72.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  76%|███████▌  | 31/41 [00:05<00:01,  5.56it/s, loss=84, v_num=0, train_loss_step=72.00, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  76%|███████▌  | 31/41 [00:05<00:01,  5.56it/s, loss=84.1, v_num=0, train_loss_step=76.60, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  78%|███████▊  | 32/41 [00:05<00:01,  5.53it/s, loss=84.1, v_num=0, train_loss_step=76.60, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  78%|███████▊  | 32/41 [00:05<00:01,  5.53it/s, loss=84.1, v_num=0, train_loss_step=76.60, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  78%|███████▊  | 32/41 [00:05<00:01,  5.53it/s, loss=83.4, v_num=0, train_loss_step=77.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  80%|████████  | 33/41 [00:05<00:01,  5.51it/s, loss=83.4, v_num=0, train_loss_step=77.70, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  80%|████████  | 33/41 [00:05<00:01,  5.51it/s, loss=83.4, v_num=0, train_loss_step=77.70, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  80%|████████  | 33/41 [00:05<00:01,  5.51it/s, loss=83, v_num=0, train_loss_step=78.80, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  83%|████████▎ | 34/41 [00:06<00:01,  5.47it/s, loss=83, v_num=0, train_loss_step=78.80, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  83%|████████▎ | 34/41 [00:06<00:01,  5.47it/s, loss=83, v_num=0, train_loss_step=78.80, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  83%|████████▎ | 34/41 [00:06<00:01,  5.47it/s, loss=83.2, v_num=0, train_loss_step=83.90, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  85%|████████▌ | 35/41 [00:06<00:01,  5.45it/s, loss=83.2, v_num=0, train_loss_step=83.90, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  85%|████████▌ | 35/41 [00:06<00:01,  5.45it/s, loss=83.2, v_num=0, train_loss_step=83.90, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  85%|████████▌ | 35/41 [00:06<00:01,  5.45it/s, loss=84.3, v_num=0, train_loss_step=103.0, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  88%|████████▊ | 36/41 [00:06<00:00,  5.43it/s, loss=84.3, v_num=0, train_loss_step=103.0, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  88%|████████▊ | 36/41 [00:06<00:00,  5.43it/s, loss=84.3, v_num=0, train_loss_step=103.0, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  88%|████████▊ | 36/41 [00:06<00:00,  5.43it/s, loss=83.8, v_num=0, train_loss_step=84.90, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  90%|█████████ | 37/41 [00:06<00:00,  5.41it/s, loss=83.8, v_num=0, train_loss_step=84.90, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  90%|█████████ | 37/41 [00:06<00:00,  5.41it/s, loss=83.8, v_num=0, train_loss_step=84.90, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  90%|█████████ | 37/41 [00:06<00:00,  5.41it/s, loss=84.3, v_num=0, train_loss_step=81.10, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  93%|█████████▎| 38/41 [00:07<00:00,  5.39it/s, loss=84.3, v_num=0, train_loss_step=81.10, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  93%|█████████▎| 38/41 [00:07<00:00,  5.39it/s, loss=84.3, v_num=0, train_loss_step=81.10, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  93%|█████████▎| 38/41 [00:07<00:00,  5.39it/s, loss=84.6, v_num=0, train_loss_step=78.50, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  95%|█████████▌| 39/41 [00:07<00:00,  5.37it/s, loss=84.6, v_num=0, train_loss_step=78.50, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  95%|█████████▌| 39/41 [00:07<00:00,  5.37it/s, loss=84.6, v_num=0, train_loss_step=78.50, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  95%|█████████▌| 39/41 [00:07<00:00,  5.37it/s, loss=84.3, v_num=0, train_loss_step=69.60, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  98%|█████████▊| 40/41 [00:07<00:00,  5.35it/s, loss=84.3, v_num=0, train_loss_step=69.60, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14:  98%|█████████▊| 40/41 [00:07<00:00,  5.35it/s, loss=84.3, v_num=0, train_loss_step=69.60, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14:  98%|█████████▊| 40/41 [00:07<00:00,  5.35it/s, loss=82.8, v_num=0, train_loss_step=74.20, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.47it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 14: 100%|██████████| 41/41 [00:07<00:00,  5.28it/s, loss=82.8, v_num=0, train_loss_step=74.20, val_loss=143.0, train_loss_epoch=84.80]#015Epoch 14: 100%|██████████| 41/41 [00:07<00:00,  5.28it/s, loss=82.8, v_num=0, train_loss_step=74.20, val_loss=143.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34mEpoch 14: 100%|██████████| 41/41 [00:08<00:00,  4.93it/s, loss=82.8, v_num=0, train_loss_step=74.20, val_loss=145.0, train_loss_epoch=84.80]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 14: 100%|██████████| 41/41 [00:08<00:00,  4.93it/s, loss=82.8, v_num=0, train_loss_step=74.20, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 14:   0%|          | 0/41 [00:00<?, ?it/s, loss=82.8, v_num=0, train_loss_step=74.20, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:   0%|          | 0/41 [00:00<?, ?it/s, loss=82.8, v_num=0, train_loss_step=74.20, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:   2%|▏         | 1/41 [00:00<00:06,  5.79it/s, loss=82.8, v_num=0, train_loss_step=74.20, val_loss=145.0, train_loss_epoch=83.60]#015Epoch 15:   2%|▏         | 1/41 [00:00<00:06,  5.78it/s, loss=82.8, v_num=0, train_loss_step=74.20, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:   2%|▏         | 1/41 [00:00<00:06,  5.77it/s, loss=82.8, v_num=0, train_loss_step=72.60, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:   5%|▍         | 2/41 [00:00<00:06,  5.75it/s, loss=82.8, v_num=0, train_loss_step=72.60, val_loss=145.0, train_loss_epoch=83.60]#015Epoch 15:   5%|▍         | 2/41 [00:00<00:06,  5.75it/s, loss=82.8, v_num=0, train_loss_step=72.60, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:   5%|▍         | 2/41 [00:00<00:06,  5.74it/s, loss=81.6, v_num=0, train_loss_step=83.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:   7%|▋         | 3/41 [00:00<00:07,  5.40it/s, loss=81.6, v_num=0, train_loss_step=83.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:   7%|▋         | 3/41 [00:00<00:07,  5.40it/s, loss=81.6, v_num=0, train_loss_step=83.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:   7%|▋         | 3/41 [00:00<00:07,  5.40it/s, loss=80.8, v_num=0, train_loss_step=69.50, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  10%|▉         | 4/41 [00:00<00:07,  5.25it/s, loss=80.8, v_num=0, train_loss_step=69.50, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  10%|▉         | 4/41 [00:00<00:07,  5.25it/s, loss=80.8, v_num=0, train_loss_step=69.50, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  10%|▉         | 4/41 [00:00<00:07,  5.25it/s, loss=81.3, v_num=0, train_loss_step=93.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  12%|█▏        | 5/41 [00:00<00:06,  5.17it/s, loss=81.3, v_num=0, train_loss_step=93.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  12%|█▏        | 5/41 [00:00<00:06,  5.16it/s, loss=81.3, v_num=0, train_loss_step=93.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  12%|█▏        | 5/41 [00:00<00:06,  5.16it/s, loss=80, v_num=0, train_loss_step=79.10, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  15%|█▍        | 6/41 [00:01<00:06,  5.06it/s, loss=80, v_num=0, train_loss_step=79.10, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  15%|█▍        | 6/41 [00:01<00:06,  5.06it/s, loss=80, v_num=0, train_loss_step=79.10, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  15%|█▍        | 6/41 [00:01<00:06,  5.06it/s, loss=80.9, v_num=0, train_loss_step=93.70, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  17%|█▋        | 7/41 [00:01<00:06,  5.01it/s, loss=80.9, v_num=0, train_loss_step=93.70, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  17%|█▋        | 7/41 [00:01<00:06,  5.01it/s, loss=80.9, v_num=0, train_loss_step=93.70, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  17%|█▋        | 7/41 [00:01<00:06,  5.01it/s, loss=79.7, v_num=0, train_loss_step=69.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  20%|█▉        | 8/41 [00:01<00:06,  4.99it/s, loss=79.7, v_num=0, train_loss_step=69.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  20%|█▉        | 8/41 [00:01<00:06,  4.99it/s, loss=79.7, v_num=0, train_loss_step=69.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  20%|█▉        | 8/41 [00:01<00:06,  4.99it/s, loss=79.6, v_num=0, train_loss_step=76.50, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  22%|██▏       | 9/41 [00:01<00:06,  4.97it/s, loss=79.6, v_num=0, train_loss_step=76.50, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  22%|██▏       | 9/41 [00:01<00:06,  4.97it/s, loss=79.6, v_num=0, train_loss_step=76.50, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  22%|██▏       | 9/41 [00:01<00:06,  4.97it/s, loss=80.2, v_num=0, train_loss_step=85.70, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  24%|██▍       | 10/41 [00:02<00:06,  4.96it/s, loss=80.2, v_num=0, train_loss_step=85.70, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  24%|██▍       | 10/41 [00:02<00:06,  4.96it/s, loss=80.2, v_num=0, train_loss_step=85.70, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  24%|██▍       | 10/41 [00:02<00:06,  4.96it/s, loss=79.9, v_num=0, train_loss_step=65.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  27%|██▋       | 11/41 [00:02<00:06,  4.95it/s, loss=79.9, v_num=0, train_loss_step=65.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  27%|██▋       | 11/41 [00:02<00:06,  4.95it/s, loss=79.9, v_num=0, train_loss_step=65.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  27%|██▋       | 11/41 [00:02<00:06,  4.95it/s, loss=81.2, v_num=0, train_loss_step=103.0, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  29%|██▉       | 12/41 [00:02<00:05,  4.92it/s, loss=81.2, v_num=0, train_loss_step=103.0, val_loss=145.0, train_loss_epoch=83.60]#015Epoch 15:  29%|██▉       | 12/41 [00:02<00:05,  4.92it/s, loss=81.2, v_num=0, train_loss_step=103.0, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  29%|██▉       | 12/41 [00:02<00:05,  4.92it/s, loss=80.9, v_num=0, train_loss_step=73.20, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  32%|███▏      | 13/41 [00:02<00:05,  4.91it/s, loss=80.9, v_num=0, train_loss_step=73.20, val_loss=145.0, train_loss_epoch=83.60]#015Epoch 15:  32%|███▏      | 13/41 [00:02<00:05,  4.91it/s, loss=80.9, v_num=0, train_loss_step=73.20, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  32%|███▏      | 13/41 [00:02<00:05,  4.91it/s, loss=79.8, v_num=0, train_loss_step=57.10, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  34%|███▍      | 14/41 [00:02<00:05,  4.91it/s, loss=79.8, v_num=0, train_loss_step=57.10, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  34%|███▍      | 14/41 [00:02<00:05,  4.91it/s, loss=79.8, v_num=0, train_loss_step=57.10, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  34%|███▍      | 14/41 [00:02<00:05,  4.91it/s, loss=79.9, v_num=0, train_loss_step=84.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  37%|███▋      | 15/41 [00:03<00:05,  4.90it/s, loss=79.9, v_num=0, train_loss_step=84.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  37%|███▋      | 15/41 [00:03<00:05,  4.90it/s, loss=79.9, v_num=0, train_loss_step=84.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  37%|███▋      | 15/41 [00:03<00:05,  4.90it/s, loss=78.6, v_num=0, train_loss_step=77.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  39%|███▉      | 16/41 [00:03<00:05,  4.91it/s, loss=78.6, v_num=0, train_loss_step=77.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  39%|███▉      | 16/41 [00:03<00:05,  4.91it/s, loss=78.6, v_num=0, train_loss_step=77.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  39%|███▉      | 16/41 [00:03<00:05,  4.91it/s, loss=77.6, v_num=0, train_loss_step=65.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  41%|████▏     | 17/41 [00:03<00:04,  4.95it/s, loss=77.6, v_num=0, train_loss_step=65.40, val_loss=145.0, train_loss_epoch=83.60]#015Epoch 15:  41%|████▏     | 17/41 [00:03<00:04,  4.95it/s, loss=77.6, v_num=0, train_loss_step=65.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  41%|████▏     | 17/41 [00:03<00:04,  4.95it/s, loss=77.3, v_num=0, train_loss_step=75.60, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  44%|████▍     | 18/41 [00:03<00:04,  4.96it/s, loss=77.3, v_num=0, train_loss_step=75.60, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  44%|████▍     | 18/41 [00:03<00:04,  4.96it/s, loss=77.3, v_num=0, train_loss_step=75.60, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  44%|████▍     | 18/41 [00:03<00:04,  4.96it/s, loss=77.9, v_num=0, train_loss_step=90.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  46%|████▋     | 19/41 [00:03<00:04,  5.00it/s, loss=77.9, v_num=0, train_loss_step=90.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  46%|████▋     | 19/41 [00:03<00:04,  5.00it/s, loss=77.9, v_num=0, train_loss_step=90.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  46%|████▋     | 19/41 [00:03<00:04,  5.00it/s, loss=78.7, v_num=0, train_loss_step=84.30, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  49%|████▉     | 20/41 [00:03<00:04,  5.03it/s, loss=78.7, v_num=0, train_loss_step=84.30, val_loss=145.0, train_loss_epoch=83.60]#015Epoch 15:  49%|████▉     | 20/41 [00:03<00:04,  5.03it/s, loss=78.7, v_num=0, train_loss_step=84.30, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  49%|████▉     | 20/41 [00:03<00:04,  5.03it/s, loss=78.8, v_num=0, train_loss_step=76.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  51%|█████     | 21/41 [00:04<00:03,  5.06it/s, loss=78.8, v_num=0, train_loss_step=76.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  51%|█████     | 21/41 [00:04<00:03,  5.06it/s, loss=78.8, v_num=0, train_loss_step=76.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  51%|█████     | 21/41 [00:04<00:03,  5.06it/s, loss=80.1, v_num=0, train_loss_step=100.0, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  54%|█████▎    | 22/41 [00:04<00:03,  5.08it/s, loss=80.1, v_num=0, train_loss_step=100.0, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  54%|█████▎    | 22/41 [00:04<00:03,  5.08it/s, loss=80.1, v_num=0, train_loss_step=100.0, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  54%|█████▎    | 22/41 [00:04<00:03,  5.08it/s, loss=80.3, v_num=0, train_loss_step=87.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  56%|█████▌    | 23/41 [00:04<00:03,  5.11it/s, loss=80.3, v_num=0, train_loss_step=87.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  56%|█████▌    | 23/41 [00:04<00:03,  5.11it/s, loss=80.3, v_num=0, train_loss_step=87.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  56%|█████▌    | 23/41 [00:04<00:03,  5.11it/s, loss=80.3, v_num=0, train_loss_step=67.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  59%|█████▊    | 24/41 [00:04<00:03,  5.12it/s, loss=80.3, v_num=0, train_loss_step=67.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  59%|█████▊    | 24/41 [00:04<00:03,  5.12it/s, loss=80.3, v_num=0, train_loss_step=67.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  59%|█████▊    | 24/41 [00:04<00:03,  5.12it/s, loss=80.7, v_num=0, train_loss_step=102.0, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  61%|██████    | 25/41 [00:04<00:03,  5.14it/s, loss=80.7, v_num=0, train_loss_step=102.0, val_loss=145.0, train_loss_epoch=83.60]#015Epoch 15:  61%|██████    | 25/41 [00:04<00:03,  5.14it/s, loss=80.7, v_num=0, train_loss_step=102.0, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  61%|██████    | 25/41 [00:04<00:03,  5.13it/s, loss=81.3, v_num=0, train_loss_step=90.30, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  63%|██████▎   | 26/41 [00:05<00:02,  5.15it/s, loss=81.3, v_num=0, train_loss_step=90.30, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  63%|██████▎   | 26/41 [00:05<00:02,  5.15it/s, loss=81.3, v_num=0, train_loss_step=90.30, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  63%|██████▎   | 26/41 [00:05<00:02,  5.15it/s, loss=79.8, v_num=0, train_loss_step=65.20, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  66%|██████▌   | 27/41 [00:05<00:02,  5.17it/s, loss=79.8, v_num=0, train_loss_step=65.20, val_loss=145.0, train_loss_epoch=83.60]#015Epoch 15:  66%|██████▌   | 27/41 [00:05<00:02,  5.17it/s, loss=79.8, v_num=0, train_loss_step=65.20, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  66%|██████▌   | 27/41 [00:05<00:02,  5.17it/s, loss=80.1, v_num=0, train_loss_step=75.70, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  68%|██████▊   | 28/41 [00:05<00:02,  5.19it/s, loss=80.1, v_num=0, train_loss_step=75.70, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  68%|██████▊   | 28/41 [00:05<00:02,  5.19it/s, loss=80.1, v_num=0, train_loss_step=75.70, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  68%|██████▊   | 28/41 [00:05<00:02,  5.19it/s, loss=81.4, v_num=0, train_loss_step=102.0, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  71%|███████   | 29/41 [00:05<00:02,  5.21it/s, loss=81.4, v_num=0, train_loss_step=102.0, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  71%|███████   | 29/41 [00:05<00:02,  5.21it/s, loss=81.4, v_num=0, train_loss_step=102.0, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  71%|███████   | 29/41 [00:05<00:02,  5.21it/s, loss=81.6, v_num=0, train_loss_step=90.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  73%|███████▎  | 30/41 [00:05<00:02,  5.22it/s, loss=81.6, v_num=0, train_loss_step=90.00, val_loss=145.0, train_loss_epoch=83.60]#015Epoch 15:  73%|███████▎  | 30/41 [00:05<00:02,  5.22it/s, loss=81.6, v_num=0, train_loss_step=90.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  73%|███████▎  | 30/41 [00:05<00:02,  5.22it/s, loss=83.1, v_num=0, train_loss_step=94.30, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  76%|███████▌  | 31/41 [00:05<00:01,  5.23it/s, loss=83.1, v_num=0, train_loss_step=94.30, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  76%|███████▌  | 31/41 [00:05<00:01,  5.23it/s, loss=83.1, v_num=0, train_loss_step=94.30, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  76%|███████▌  | 31/41 [00:05<00:01,  5.23it/s, loss=82.9, v_num=0, train_loss_step=99.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  78%|███████▊  | 32/41 [00:06<00:01,  5.25it/s, loss=82.9, v_num=0, train_loss_step=99.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  78%|███████▊  | 32/41 [00:06<00:01,  5.25it/s, loss=82.9, v_num=0, train_loss_step=99.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  78%|███████▊  | 32/41 [00:06<00:01,  5.25it/s, loss=82.3, v_num=0, train_loss_step=60.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  80%|████████  | 33/41 [00:06<00:01,  5.26it/s, loss=82.3, v_num=0, train_loss_step=60.90, val_loss=145.0, train_loss_epoch=83.60]#015Epoch 15:  80%|████████  | 33/41 [00:06<00:01,  5.26it/s, loss=82.3, v_num=0, train_loss_step=60.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  80%|████████  | 33/41 [00:06<00:01,  5.26it/s, loss=83.7, v_num=0, train_loss_step=84.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  83%|████████▎ | 34/41 [00:06<00:01,  5.27it/s, loss=83.7, v_num=0, train_loss_step=84.40, val_loss=145.0, train_loss_epoch=83.60]#015Epoch 15:  83%|████████▎ | 34/41 [00:06<00:01,  5.27it/s, loss=83.7, v_num=0, train_loss_step=84.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  83%|████████▎ | 34/41 [00:06<00:01,  5.27it/s, loss=83.9, v_num=0, train_loss_step=88.80, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  85%|████████▌ | 35/41 [00:06<00:01,  5.28it/s, loss=83.9, v_num=0, train_loss_step=88.80, val_loss=145.0, train_loss_epoch=83.60]#015Epoch 15:  85%|████████▌ | 35/41 [00:06<00:01,  5.28it/s, loss=83.9, v_num=0, train_loss_step=88.80, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  85%|████████▌ | 35/41 [00:06<00:01,  5.28it/s, loss=83, v_num=0, train_loss_step=58.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  88%|████████▊ | 36/41 [00:06<00:00,  5.29it/s, loss=83, v_num=0, train_loss_step=58.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  88%|████████▊ | 36/41 [00:06<00:00,  5.29it/s, loss=83, v_num=0, train_loss_step=58.40, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  88%|████████▊ | 36/41 [00:06<00:00,  5.29it/s, loss=83.4, v_num=0, train_loss_step=73.80, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  90%|█████████ | 37/41 [00:06<00:00,  5.30it/s, loss=83.4, v_num=0, train_loss_step=73.80, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  90%|█████████ | 37/41 [00:06<00:00,  5.30it/s, loss=83.4, v_num=0, train_loss_step=73.80, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  90%|█████████ | 37/41 [00:06<00:00,  5.30it/s, loss=83.4, v_num=0, train_loss_step=75.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  93%|█████████▎| 38/41 [00:07<00:00,  5.31it/s, loss=83.4, v_num=0, train_loss_step=75.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  93%|█████████▎| 38/41 [00:07<00:00,  5.31it/s, loss=83.4, v_num=0, train_loss_step=75.90, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  93%|█████████▎| 38/41 [00:07<00:00,  5.31it/s, loss=83.1, v_num=0, train_loss_step=85.10, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  95%|█████████▌| 39/41 [00:07<00:00,  5.32it/s, loss=83.1, v_num=0, train_loss_step=85.10, val_loss=145.0, train_loss_epoch=83.60]#015Epoch 15:  95%|█████████▌| 39/41 [00:07<00:00,  5.32it/s, loss=83.1, v_num=0, train_loss_step=85.10, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  95%|█████████▌| 39/41 [00:07<00:00,  5.32it/s, loss=82.7, v_num=0, train_loss_step=74.70, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  98%|█████████▊| 40/41 [00:07<00:00,  5.21it/s, loss=82.7, v_num=0, train_loss_step=74.70, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  98%|█████████▊| 40/41 [00:07<00:00,  5.21it/s, loss=82.7, v_num=0, train_loss_step=74.70, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15:  98%|█████████▊| 40/41 [00:07<00:00,  5.21it/s, loss=83.8, v_num=0, train_loss_step=99.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.48it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.48it/s]#033[A#015Epoch 15: 100%|██████████| 41/41 [00:07<00:00,  5.15it/s, loss=83.8, v_num=0, train_loss_step=99.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15: 100%|██████████| 41/41 [00:07<00:00,  5.15it/s, loss=83.8, v_num=0, train_loss_step=99.00, val_loss=145.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34mEpoch 15: 100%|██████████| 41/41 [00:08<00:00,  4.82it/s, loss=83.8, v_num=0, train_loss_step=99.00, val_loss=148.0, train_loss_epoch=83.60]\u001b[0m\n",
      "\u001b[34m#015                                                                      #033[A\u001b[0m\n",
      "\u001b[34mEpoch 15: 100%|██████████| 41/41 [00:08<00:00,  4.81it/s, loss=83.8, v_num=0, train_loss_step=99.00, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 15:   0%|          | 0/41 [00:00<?, ?it/s, loss=83.8, v_num=0, train_loss_step=99.00, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:   0%|          | 0/41 [00:00<?, ?it/s, loss=83.8, v_num=0, train_loss_step=99.00, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:   2%|▏         | 1/41 [00:00<00:14,  2.70it/s, loss=83.8, v_num=0, train_loss_step=99.00, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:   2%|▏         | 1/41 [00:00<00:14,  2.70it/s, loss=83.8, v_num=0, train_loss_step=99.00, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:   2%|▏         | 1/41 [00:00<00:14,  2.69it/s, loss=82.4, v_num=0, train_loss_step=71.60, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:   5%|▍         | 2/41 [00:00<00:16,  2.40it/s, loss=82.4, v_num=0, train_loss_step=71.60, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:   5%|▍         | 2/41 [00:00<00:16,  2.40it/s, loss=82.4, v_num=0, train_loss_step=71.60, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:   5%|▍         | 2/41 [00:00<00:16,  2.40it/s, loss=82.3, v_num=0, train_loss_step=87.40, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:   7%|▋         | 3/41 [00:01<00:12,  2.95it/s, loss=82.3, v_num=0, train_loss_step=87.40, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:   7%|▋         | 3/41 [00:01<00:12,  2.95it/s, loss=82.3, v_num=0, train_loss_step=87.40, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:   7%|▋         | 3/41 [00:01<00:12,  2.95it/s, loss=83.5, v_num=0, train_loss_step=90.50, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  10%|▉         | 4/41 [00:01<00:11,  3.36it/s, loss=83.5, v_num=0, train_loss_step=90.50, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  10%|▉         | 4/41 [00:01<00:11,  3.36it/s, loss=83.5, v_num=0, train_loss_step=90.50, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  10%|▉         | 4/41 [00:01<00:11,  3.35it/s, loss=82.8, v_num=0, train_loss_step=87.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  12%|█▏        | 5/41 [00:01<00:09,  3.66it/s, loss=82.8, v_num=0, train_loss_step=87.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  12%|█▏        | 5/41 [00:01<00:09,  3.65it/s, loss=82.8, v_num=0, train_loss_step=87.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  12%|█▏        | 5/41 [00:01<00:09,  3.65it/s, loss=82.2, v_num=0, train_loss_step=79.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  15%|█▍        | 6/41 [00:01<00:08,  3.89it/s, loss=82.2, v_num=0, train_loss_step=79.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  15%|█▍        | 6/41 [00:01<00:09,  3.89it/s, loss=82.2, v_num=0, train_loss_step=79.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  15%|█▍        | 6/41 [00:01<00:09,  3.89it/s, loss=82.1, v_num=0, train_loss_step=63.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  17%|█▋        | 7/41 [00:01<00:08,  4.06it/s, loss=82.1, v_num=0, train_loss_step=63.70, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  17%|█▋        | 7/41 [00:01<00:08,  4.06it/s, loss=82.1, v_num=0, train_loss_step=63.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  17%|█▋        | 7/41 [00:01<00:08,  4.06it/s, loss=81.4, v_num=0, train_loss_step=61.90, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  20%|█▉        | 8/41 [00:01<00:07,  4.18it/s, loss=81.4, v_num=0, train_loss_step=61.90, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  20%|█▉        | 8/41 [00:01<00:07,  4.18it/s, loss=81.4, v_num=0, train_loss_step=61.90, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  20%|█▉        | 8/41 [00:01<00:07,  4.18it/s, loss=80.1, v_num=0, train_loss_step=76.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  22%|██▏       | 9/41 [00:02<00:07,  4.31it/s, loss=80.1, v_num=0, train_loss_step=76.30, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  22%|██▏       | 9/41 [00:02<00:07,  4.31it/s, loss=80.1, v_num=0, train_loss_step=76.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  22%|██▏       | 9/41 [00:02<00:07,  4.30it/s, loss=79.7, v_num=0, train_loss_step=82.00, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  24%|██▍       | 10/41 [00:02<00:07,  4.42it/s, loss=79.7, v_num=0, train_loss_step=82.00, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  24%|██▍       | 10/41 [00:02<00:07,  4.41it/s, loss=79.7, v_num=0, train_loss_step=82.00, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  24%|██▍       | 10/41 [00:02<00:07,  4.41it/s, loss=79.6, v_num=0, train_loss_step=92.40, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  27%|██▋       | 11/41 [00:02<00:06,  4.51it/s, loss=79.6, v_num=0, train_loss_step=92.40, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  27%|██▋       | 11/41 [00:02<00:06,  4.51it/s, loss=79.6, v_num=0, train_loss_step=92.40, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  27%|██▋       | 11/41 [00:02<00:06,  4.51it/s, loss=78.4, v_num=0, train_loss_step=73.60, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  29%|██▉       | 12/41 [00:02<00:06,  4.59it/s, loss=78.4, v_num=0, train_loss_step=73.60, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  29%|██▉       | 12/41 [00:02<00:06,  4.59it/s, loss=78.4, v_num=0, train_loss_step=73.60, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  29%|██▉       | 12/41 [00:02<00:06,  4.59it/s, loss=79.4, v_num=0, train_loss_step=81.10, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  32%|███▏      | 13/41 [00:02<00:06,  4.66it/s, loss=79.4, v_num=0, train_loss_step=81.10, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  32%|███▏      | 13/41 [00:02<00:06,  4.66it/s, loss=79.4, v_num=0, train_loss_step=81.10, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  32%|███▏      | 13/41 [00:02<00:06,  4.66it/s, loss=79.3, v_num=0, train_loss_step=82.40, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  34%|███▍      | 14/41 [00:02<00:05,  4.70it/s, loss=79.3, v_num=0, train_loss_step=82.40, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  34%|███▍      | 14/41 [00:02<00:05,  4.70it/s, loss=79.3, v_num=0, train_loss_step=82.40, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  34%|███▍      | 14/41 [00:02<00:05,  4.70it/s, loss=80.2, v_num=0, train_loss_step=107.0, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  37%|███▋      | 15/41 [00:03<00:05,  4.76it/s, loss=80.2, v_num=0, train_loss_step=107.0, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  37%|███▋      | 15/41 [00:03<00:05,  4.76it/s, loss=80.2, v_num=0, train_loss_step=107.0, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  37%|███▋      | 15/41 [00:03<00:05,  4.76it/s, loss=81.2, v_num=0, train_loss_step=78.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  39%|███▉      | 16/41 [00:03<00:05,  4.81it/s, loss=81.2, v_num=0, train_loss_step=78.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  39%|███▉      | 16/41 [00:03<00:05,  4.81it/s, loss=81.2, v_num=0, train_loss_step=78.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  39%|███▉      | 16/41 [00:03<00:05,  4.81it/s, loss=82.1, v_num=0, train_loss_step=92.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  41%|████▏     | 17/41 [00:03<00:04,  4.86it/s, loss=82.1, v_num=0, train_loss_step=92.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  41%|████▏     | 17/41 [00:03<00:04,  4.86it/s, loss=82.1, v_num=0, train_loss_step=92.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  41%|████▏     | 17/41 [00:03<00:04,  4.86it/s, loss=81.8, v_num=0, train_loss_step=69.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  44%|████▍     | 18/41 [00:03<00:04,  4.90it/s, loss=81.8, v_num=0, train_loss_step=69.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  44%|████▍     | 18/41 [00:03<00:04,  4.90it/s, loss=81.8, v_num=0, train_loss_step=69.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  44%|████▍     | 18/41 [00:03<00:04,  4.90it/s, loss=81.6, v_num=0, train_loss_step=81.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  46%|████▋     | 19/41 [00:03<00:04,  4.94it/s, loss=81.6, v_num=0, train_loss_step=81.30, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  46%|████▋     | 19/41 [00:03<00:04,  4.93it/s, loss=81.6, v_num=0, train_loss_step=81.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  46%|████▋     | 19/41 [00:03<00:04,  4.93it/s, loss=82.2, v_num=0, train_loss_step=87.10, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  49%|████▉     | 20/41 [00:04<00:04,  4.96it/s, loss=82.2, v_num=0, train_loss_step=87.10, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  49%|████▉     | 20/41 [00:04<00:04,  4.96it/s, loss=82.2, v_num=0, train_loss_step=87.10, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  49%|████▉     | 20/41 [00:04<00:04,  4.96it/s, loss=81.6, v_num=0, train_loss_step=86.60, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  51%|█████     | 21/41 [00:04<00:04,  4.99it/s, loss=81.6, v_num=0, train_loss_step=86.60, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  51%|█████     | 21/41 [00:04<00:04,  4.99it/s, loss=81.6, v_num=0, train_loss_step=86.60, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  51%|█████     | 21/41 [00:04<00:04,  4.99it/s, loss=82.4, v_num=0, train_loss_step=86.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  54%|█████▎    | 22/41 [00:04<00:03,  5.02it/s, loss=82.4, v_num=0, train_loss_step=86.30, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  54%|█████▎    | 22/41 [00:04<00:03,  5.02it/s, loss=82.4, v_num=0, train_loss_step=86.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  54%|█████▎    | 22/41 [00:04<00:03,  5.02it/s, loss=83.1, v_num=0, train_loss_step=102.0, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  56%|█████▌    | 23/41 [00:04<00:03,  5.04it/s, loss=83.1, v_num=0, train_loss_step=102.0, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  56%|█████▌    | 23/41 [00:04<00:03,  5.04it/s, loss=83.1, v_num=0, train_loss_step=102.0, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  56%|█████▌    | 23/41 [00:04<00:03,  5.04it/s, loss=82.9, v_num=0, train_loss_step=87.40, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  59%|█████▊    | 24/41 [00:04<00:03,  5.07it/s, loss=82.9, v_num=0, train_loss_step=87.40, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  59%|█████▊    | 24/41 [00:04<00:03,  5.07it/s, loss=82.9, v_num=0, train_loss_step=87.40, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  59%|█████▊    | 24/41 [00:04<00:03,  5.07it/s, loss=82.6, v_num=0, train_loss_step=81.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  61%|██████    | 25/41 [00:04<00:03,  5.09it/s, loss=82.6, v_num=0, train_loss_step=81.20, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  61%|██████    | 25/41 [00:04<00:03,  5.09it/s, loss=82.6, v_num=0, train_loss_step=81.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  61%|██████    | 25/41 [00:04<00:03,  5.09it/s, loss=82.8, v_num=0, train_loss_step=84.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  63%|██████▎   | 26/41 [00:05<00:02,  5.10it/s, loss=82.8, v_num=0, train_loss_step=84.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  63%|██████▎   | 26/41 [00:05<00:02,  5.10it/s, loss=82.8, v_num=0, train_loss_step=84.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  63%|██████▎   | 26/41 [00:05<00:02,  5.10it/s, loss=83.5, v_num=0, train_loss_step=76.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  66%|██████▌   | 27/41 [00:05<00:02,  5.12it/s, loss=83.5, v_num=0, train_loss_step=76.30, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  66%|██████▌   | 27/41 [00:05<00:02,  5.12it/s, loss=83.5, v_num=0, train_loss_step=76.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  66%|██████▌   | 27/41 [00:05<00:02,  5.12it/s, loss=84.7, v_num=0, train_loss_step=87.00, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  68%|██████▊   | 28/41 [00:05<00:02,  5.14it/s, loss=84.7, v_num=0, train_loss_step=87.00, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  68%|██████▊   | 28/41 [00:05<00:02,  5.14it/s, loss=84.7, v_num=0, train_loss_step=87.00, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  68%|██████▊   | 28/41 [00:05<00:02,  5.14it/s, loss=84.9, v_num=0, train_loss_step=78.90, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  71%|███████   | 29/41 [00:05<00:02,  5.16it/s, loss=84.9, v_num=0, train_loss_step=78.90, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  71%|███████   | 29/41 [00:05<00:02,  5.16it/s, loss=84.9, v_num=0, train_loss_step=78.90, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  71%|███████   | 29/41 [00:05<00:02,  5.16it/s, loss=85.6, v_num=0, train_loss_step=97.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  73%|███████▎  | 30/41 [00:05<00:02,  5.17it/s, loss=85.6, v_num=0, train_loss_step=97.20, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  73%|███████▎  | 30/41 [00:05<00:02,  5.17it/s, loss=85.6, v_num=0, train_loss_step=97.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  73%|███████▎  | 30/41 [00:05<00:02,  5.17it/s, loss=84.8, v_num=0, train_loss_step=75.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  76%|███████▌  | 31/41 [00:05<00:01,  5.19it/s, loss=84.8, v_num=0, train_loss_step=75.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  76%|███████▌  | 31/41 [00:05<00:01,  5.19it/s, loss=84.8, v_num=0, train_loss_step=75.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  76%|███████▌  | 31/41 [00:05<00:01,  5.19it/s, loss=86.1, v_num=0, train_loss_step=99.90, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  78%|███████▊  | 32/41 [00:06<00:01,  5.20it/s, loss=86.1, v_num=0, train_loss_step=99.90, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  78%|███████▊  | 32/41 [00:06<00:01,  5.20it/s, loss=86.1, v_num=0, train_loss_step=99.90, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  78%|███████▊  | 32/41 [00:06<00:01,  5.20it/s, loss=85.9, v_num=0, train_loss_step=78.10, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  80%|████████  | 33/41 [00:06<00:01,  5.21it/s, loss=85.9, v_num=0, train_loss_step=78.10, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  80%|████████  | 33/41 [00:06<00:01,  5.21it/s, loss=85.9, v_num=0, train_loss_step=78.10, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  80%|████████  | 33/41 [00:06<00:01,  5.21it/s, loss=86, v_num=0, train_loss_step=83.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  83%|████████▎ | 34/41 [00:06<00:01,  5.22it/s, loss=86, v_num=0, train_loss_step=83.30, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  83%|████████▎ | 34/41 [00:06<00:01,  5.22it/s, loss=86, v_num=0, train_loss_step=83.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  83%|████████▎ | 34/41 [00:06<00:01,  5.22it/s, loss=85.2, v_num=0, train_loss_step=91.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  85%|████████▌ | 35/41 [00:06<00:01,  5.23it/s, loss=85.2, v_num=0, train_loss_step=91.30, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  85%|████████▌ | 35/41 [00:06<00:01,  5.23it/s, loss=85.2, v_num=0, train_loss_step=91.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  85%|████████▌ | 35/41 [00:06<00:01,  5.23it/s, loss=85.6, v_num=0, train_loss_step=86.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  88%|████████▊ | 36/41 [00:06<00:00,  5.24it/s, loss=85.6, v_num=0, train_loss_step=86.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  88%|████████▊ | 36/41 [00:06<00:00,  5.24it/s, loss=85.6, v_num=0, train_loss_step=86.20, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  88%|████████▊ | 36/41 [00:06<00:00,  5.24it/s, loss=85.3, v_num=0, train_loss_step=86.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  90%|█████████ | 37/41 [00:07<00:00,  5.25it/s, loss=85.3, v_num=0, train_loss_step=86.70, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  90%|█████████ | 37/41 [00:07<00:00,  5.25it/s, loss=85.3, v_num=0, train_loss_step=86.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  90%|█████████ | 37/41 [00:07<00:00,  5.25it/s, loss=85.3, v_num=0, train_loss_step=69.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  93%|█████████▎| 38/41 [00:07<00:00,  5.25it/s, loss=85.3, v_num=0, train_loss_step=69.30, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  93%|█████████▎| 38/41 [00:07<00:00,  5.25it/s, loss=85.3, v_num=0, train_loss_step=69.30, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  93%|█████████▎| 38/41 [00:07<00:00,  5.25it/s, loss=85.2, v_num=0, train_loss_step=80.60, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  95%|█████████▌| 39/41 [00:07<00:00,  5.26it/s, loss=85.2, v_num=0, train_loss_step=80.60, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16:  95%|█████████▌| 39/41 [00:07<00:00,  5.26it/s, loss=85.2, v_num=0, train_loss_step=80.60, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  95%|█████████▌| 39/41 [00:07<00:00,  5.26it/s, loss=84.6, v_num=0, train_loss_step=73.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  98%|█████████▊| 40/41 [00:07<00:00,  5.26it/s, loss=84.6, v_num=0, train_loss_step=73.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  98%|█████████▊| 40/41 [00:07<00:00,  5.26it/s, loss=84.6, v_num=0, train_loss_step=73.70, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16:  98%|█████████▊| 40/41 [00:07<00:00,  5.26it/s, loss=83.7, v_num=0, train_loss_step=69.40, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.54it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 16: 100%|██████████| 41/41 [00:07<00:00,  5.20it/s, loss=83.7, v_num=0, train_loss_step=69.40, val_loss=148.0, train_loss_epoch=81.30]#015Epoch 16: 100%|██████████| 41/41 [00:07<00:00,  5.20it/s, loss=83.7, v_num=0, train_loss_step=69.40, val_loss=148.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34mEpoch 16: 100%|██████████| 41/41 [00:08<00:00,  4.86it/s, loss=83.7, v_num=0, train_loss_step=69.40, val_loss=147.0, train_loss_epoch=81.30]\u001b[0m\n",
      "\u001b[34m#015                                                                      #033[A\u001b[0m\n",
      "\u001b[34mEpoch 16: 100%|██████████| 41/41 [00:08<00:00,  4.86it/s, loss=83.7, v_num=0, train_loss_step=69.40, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 16:   0%|          | 0/41 [00:00<?, ?it/s, loss=83.7, v_num=0, train_loss_step=69.40, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:   0%|          | 0/41 [00:00<?, ?it/s, loss=83.7, v_num=0, train_loss_step=69.40, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:   2%|▏         | 1/41 [00:00<00:06,  5.85it/s, loss=83.7, v_num=0, train_loss_step=69.40, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:   2%|▏         | 1/41 [00:00<00:06,  5.84it/s, loss=83.7, v_num=0, train_loss_step=69.40, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:   2%|▏         | 1/41 [00:00<00:06,  5.83it/s, loss=83.5, v_num=0, train_loss_step=81.60, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:   5%|▍         | 2/41 [00:00<00:06,  5.84it/s, loss=83.5, v_num=0, train_loss_step=81.60, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:   5%|▍         | 2/41 [00:00<00:06,  5.83it/s, loss=83.5, v_num=0, train_loss_step=81.60, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:   5%|▍         | 2/41 [00:00<00:06,  5.83it/s, loss=82.1, v_num=0, train_loss_step=73.80, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:   7%|▋         | 3/41 [00:00<00:06,  5.65it/s, loss=82.1, v_num=0, train_loss_step=73.80, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:   7%|▋         | 3/41 [00:00<00:06,  5.64it/s, loss=82.1, v_num=0, train_loss_step=73.80, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:   7%|▋         | 3/41 [00:00<00:06,  5.64it/s, loss=81.2, v_num=0, train_loss_step=69.90, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  10%|▉         | 4/41 [00:00<00:06,  5.57it/s, loss=81.2, v_num=0, train_loss_step=69.90, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  10%|▉         | 4/41 [00:00<00:06,  5.57it/s, loss=81.2, v_num=0, train_loss_step=69.90, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  10%|▉         | 4/41 [00:00<00:06,  5.56it/s, loss=80.6, v_num=0, train_loss_step=68.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  12%|█▏        | 5/41 [00:00<00:06,  5.58it/s, loss=80.6, v_num=0, train_loss_step=68.30, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  12%|█▏        | 5/41 [00:00<00:06,  5.58it/s, loss=80.6, v_num=0, train_loss_step=68.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  12%|█▏        | 5/41 [00:00<00:06,  5.58it/s, loss=80.4, v_num=0, train_loss_step=81.20, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  15%|█▍        | 6/41 [00:01<00:06,  5.59it/s, loss=80.4, v_num=0, train_loss_step=81.20, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  15%|█▍        | 6/41 [00:01<00:06,  5.59it/s, loss=80.4, v_num=0, train_loss_step=81.20, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  15%|█▍        | 6/41 [00:01<00:06,  5.59it/s, loss=80.4, v_num=0, train_loss_step=76.80, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  17%|█▋        | 7/41 [00:01<00:06,  5.61it/s, loss=80.4, v_num=0, train_loss_step=76.80, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  17%|█▋        | 7/41 [00:01<00:06,  5.61it/s, loss=80.4, v_num=0, train_loss_step=76.80, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  17%|█▋        | 7/41 [00:01<00:06,  5.60it/s, loss=79.9, v_num=0, train_loss_step=75.60, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  20%|█▉        | 8/41 [00:01<00:05,  5.60it/s, loss=79.9, v_num=0, train_loss_step=75.60, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  20%|█▉        | 8/41 [00:01<00:05,  5.60it/s, loss=79.9, v_num=0, train_loss_step=75.60, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  20%|█▉        | 8/41 [00:01<00:05,  5.60it/s, loss=80, v_num=0, train_loss_step=82.00, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  22%|██▏       | 9/41 [00:01<00:05,  5.63it/s, loss=80, v_num=0, train_loss_step=82.00, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  22%|██▏       | 9/41 [00:01<00:05,  5.63it/s, loss=80, v_num=0, train_loss_step=82.00, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  22%|██▏       | 9/41 [00:01<00:05,  5.62it/s, loss=78.8, v_num=0, train_loss_step=72.60, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  24%|██▍       | 10/41 [00:01<00:05,  5.58it/s, loss=78.8, v_num=0, train_loss_step=72.60, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  24%|██▍       | 10/41 [00:01<00:05,  5.58it/s, loss=78.8, v_num=0, train_loss_step=72.60, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  24%|██▍       | 10/41 [00:01<00:05,  5.58it/s, loss=78.6, v_num=0, train_loss_step=71.10, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  27%|██▋       | 11/41 [00:01<00:05,  5.59it/s, loss=78.6, v_num=0, train_loss_step=71.10, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  27%|██▋       | 11/41 [00:01<00:05,  5.58it/s, loss=78.6, v_num=0, train_loss_step=71.10, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  27%|██▋       | 11/41 [00:01<00:05,  5.58it/s, loss=77.4, v_num=0, train_loss_step=75.60, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  29%|██▉       | 12/41 [00:02<00:05,  5.58it/s, loss=77.4, v_num=0, train_loss_step=75.60, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  29%|██▉       | 12/41 [00:02<00:05,  5.58it/s, loss=77.4, v_num=0, train_loss_step=75.60, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  29%|██▉       | 12/41 [00:02<00:05,  5.58it/s, loss=77.5, v_num=0, train_loss_step=81.20, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  32%|███▏      | 13/41 [00:02<00:05,  5.59it/s, loss=77.5, v_num=0, train_loss_step=81.20, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  32%|███▏      | 13/41 [00:02<00:05,  5.59it/s, loss=77.5, v_num=0, train_loss_step=81.20, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  32%|███▏      | 13/41 [00:02<00:05,  5.59it/s, loss=77, v_num=0, train_loss_step=72.90, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  34%|███▍      | 14/41 [00:02<00:04,  5.59it/s, loss=77, v_num=0, train_loss_step=72.90, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  34%|███▍      | 14/41 [00:02<00:04,  5.59it/s, loss=77, v_num=0, train_loss_step=72.90, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  34%|███▍      | 14/41 [00:02<00:04,  5.59it/s, loss=77, v_num=0, train_loss_step=90.90, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  37%|███▋      | 15/41 [00:02<00:04,  5.59it/s, loss=77, v_num=0, train_loss_step=90.90, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  37%|███▋      | 15/41 [00:02<00:04,  5.59it/s, loss=77, v_num=0, train_loss_step=90.90, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  37%|███▋      | 15/41 [00:02<00:04,  5.59it/s, loss=75.8, v_num=0, train_loss_step=62.60, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  39%|███▉      | 16/41 [00:02<00:04,  5.57it/s, loss=75.8, v_num=0, train_loss_step=62.60, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  39%|███▉      | 16/41 [00:02<00:04,  5.57it/s, loss=75.8, v_num=0, train_loss_step=62.60, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  39%|███▉      | 16/41 [00:02<00:04,  5.57it/s, loss=75.7, v_num=0, train_loss_step=84.70, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  41%|████▏     | 17/41 [00:03<00:04,  5.57it/s, loss=75.7, v_num=0, train_loss_step=84.70, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  41%|████▏     | 17/41 [00:03<00:04,  5.57it/s, loss=75.7, v_num=0, train_loss_step=84.70, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  41%|████▏     | 17/41 [00:03<00:04,  5.57it/s, loss=76.3, v_num=0, train_loss_step=82.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  44%|████▍     | 18/41 [00:03<00:04,  5.58it/s, loss=76.3, v_num=0, train_loss_step=82.30, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  44%|████▍     | 18/41 [00:03<00:04,  5.58it/s, loss=76.3, v_num=0, train_loss_step=82.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  44%|████▍     | 18/41 [00:03<00:04,  5.58it/s, loss=75.2, v_num=0, train_loss_step=57.50, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  46%|████▋     | 19/41 [00:03<00:03,  5.58it/s, loss=75.2, v_num=0, train_loss_step=57.50, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  46%|████▋     | 19/41 [00:03<00:03,  5.58it/s, loss=75.2, v_num=0, train_loss_step=57.50, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  46%|████▋     | 19/41 [00:03<00:03,  5.58it/s, loss=76, v_num=0, train_loss_step=89.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  49%|████▉     | 20/41 [00:03<00:03,  5.59it/s, loss=76, v_num=0, train_loss_step=89.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  49%|████▉     | 20/41 [00:03<00:03,  5.58it/s, loss=76, v_num=0, train_loss_step=89.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  49%|████▉     | 20/41 [00:03<00:03,  5.58it/s, loss=75.6, v_num=0, train_loss_step=62.60, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  51%|█████     | 21/41 [00:03<00:03,  5.59it/s, loss=75.6, v_num=0, train_loss_step=62.60, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  51%|█████     | 21/41 [00:03<00:03,  5.59it/s, loss=75.6, v_num=0, train_loss_step=62.60, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  51%|█████     | 21/41 [00:03<00:03,  5.59it/s, loss=75.6, v_num=0, train_loss_step=80.90, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  54%|█████▎    | 22/41 [00:03<00:03,  5.57it/s, loss=75.6, v_num=0, train_loss_step=80.90, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  54%|█████▎    | 22/41 [00:03<00:03,  5.57it/s, loss=75.6, v_num=0, train_loss_step=80.90, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  54%|█████▎    | 22/41 [00:03<00:03,  5.57it/s, loss=75.8, v_num=0, train_loss_step=78.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  56%|█████▌    | 23/41 [00:04<00:03,  5.58it/s, loss=75.8, v_num=0, train_loss_step=78.30, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  56%|█████▌    | 23/41 [00:04<00:03,  5.58it/s, loss=75.8, v_num=0, train_loss_step=78.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  56%|█████▌    | 23/41 [00:04<00:03,  5.58it/s, loss=76.2, v_num=0, train_loss_step=78.10, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  59%|█████▊    | 24/41 [00:04<00:03,  5.58it/s, loss=76.2, v_num=0, train_loss_step=78.10, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  59%|█████▊    | 24/41 [00:04<00:03,  5.58it/s, loss=76.2, v_num=0, train_loss_step=78.10, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  59%|█████▊    | 24/41 [00:04<00:03,  5.58it/s, loss=76.7, v_num=0, train_loss_step=78.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  61%|██████    | 25/41 [00:04<00:02,  5.58it/s, loss=76.7, v_num=0, train_loss_step=78.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  61%|██████    | 25/41 [00:04<00:02,  5.58it/s, loss=76.7, v_num=0, train_loss_step=78.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  61%|██████    | 25/41 [00:04<00:02,  5.58it/s, loss=76.9, v_num=0, train_loss_step=83.90, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  63%|██████▎   | 26/41 [00:04<00:02,  5.59it/s, loss=76.9, v_num=0, train_loss_step=83.90, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  63%|██████▎   | 26/41 [00:04<00:02,  5.59it/s, loss=76.9, v_num=0, train_loss_step=83.90, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  63%|██████▎   | 26/41 [00:04<00:02,  5.58it/s, loss=76.9, v_num=0, train_loss_step=78.20, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  66%|██████▌   | 27/41 [00:04<00:02,  5.59it/s, loss=76.9, v_num=0, train_loss_step=78.20, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  66%|██████▌   | 27/41 [00:04<00:02,  5.59it/s, loss=76.9, v_num=0, train_loss_step=78.20, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  66%|██████▌   | 27/41 [00:04<00:02,  5.59it/s, loss=76.3, v_num=0, train_loss_step=64.00, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  68%|██████▊   | 28/41 [00:05<00:02,  5.58it/s, loss=76.3, v_num=0, train_loss_step=64.00, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  68%|██████▊   | 28/41 [00:05<00:02,  5.58it/s, loss=76.3, v_num=0, train_loss_step=64.00, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  68%|██████▊   | 28/41 [00:05<00:02,  5.58it/s, loss=75.3, v_num=0, train_loss_step=60.40, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  71%|███████   | 29/41 [00:05<00:02,  5.59it/s, loss=75.3, v_num=0, train_loss_step=60.40, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  71%|███████   | 29/41 [00:05<00:02,  5.58it/s, loss=75.3, v_num=0, train_loss_step=60.40, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  71%|███████   | 29/41 [00:05<00:02,  5.58it/s, loss=75.3, v_num=0, train_loss_step=74.20, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  73%|███████▎  | 30/41 [00:05<00:01,  5.59it/s, loss=75.3, v_num=0, train_loss_step=74.20, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  73%|███████▎  | 30/41 [00:05<00:01,  5.59it/s, loss=75.3, v_num=0, train_loss_step=74.20, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  73%|███████▎  | 30/41 [00:05<00:01,  5.59it/s, loss=76.3, v_num=0, train_loss_step=89.90, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  76%|███████▌  | 31/41 [00:05<00:01,  5.59it/s, loss=76.3, v_num=0, train_loss_step=89.90, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  76%|███████▌  | 31/41 [00:05<00:01,  5.59it/s, loss=76.3, v_num=0, train_loss_step=89.90, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  76%|███████▌  | 31/41 [00:05<00:01,  5.59it/s, loss=77.1, v_num=0, train_loss_step=91.10, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  78%|███████▊  | 32/41 [00:05<00:01,  5.59it/s, loss=77.1, v_num=0, train_loss_step=91.10, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  78%|███████▊  | 32/41 [00:05<00:01,  5.59it/s, loss=77.1, v_num=0, train_loss_step=91.10, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  78%|███████▊  | 32/41 [00:05<00:01,  5.59it/s, loss=78, v_num=0, train_loss_step=99.50, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  80%|████████  | 33/41 [00:05<00:01,  5.59it/s, loss=78, v_num=0, train_loss_step=99.50, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  80%|████████  | 33/41 [00:05<00:01,  5.59it/s, loss=78, v_num=0, train_loss_step=99.50, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  80%|████████  | 33/41 [00:05<00:01,  5.59it/s, loss=78.6, v_num=0, train_loss_step=86.20, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  83%|████████▎ | 34/41 [00:06<00:01,  5.59it/s, loss=78.6, v_num=0, train_loss_step=86.20, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  83%|████████▎ | 34/41 [00:06<00:01,  5.59it/s, loss=78.6, v_num=0, train_loss_step=86.20, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  83%|████████▎ | 34/41 [00:06<00:01,  5.59it/s, loss=78.2, v_num=0, train_loss_step=81.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  85%|████████▌ | 35/41 [00:06<00:01,  5.59it/s, loss=78.2, v_num=0, train_loss_step=81.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  85%|████████▌ | 35/41 [00:06<00:01,  5.59it/s, loss=78.2, v_num=0, train_loss_step=81.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  85%|████████▌ | 35/41 [00:06<00:01,  5.59it/s, loss=78.6, v_num=0, train_loss_step=71.00, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  88%|████████▊ | 36/41 [00:06<00:00,  5.59it/s, loss=78.6, v_num=0, train_loss_step=71.00, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  88%|████████▊ | 36/41 [00:06<00:00,  5.59it/s, loss=78.6, v_num=0, train_loss_step=71.00, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  88%|████████▊ | 36/41 [00:06<00:00,  5.59it/s, loss=78.5, v_num=0, train_loss_step=82.50, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  90%|█████████ | 37/41 [00:06<00:00,  5.59it/s, loss=78.5, v_num=0, train_loss_step=82.50, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  90%|█████████ | 37/41 [00:06<00:00,  5.59it/s, loss=78.5, v_num=0, train_loss_step=82.50, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  90%|█████████ | 37/41 [00:06<00:00,  5.59it/s, loss=79.1, v_num=0, train_loss_step=94.70, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  93%|█████████▎| 38/41 [00:06<00:00,  5.60it/s, loss=79.1, v_num=0, train_loss_step=94.70, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  93%|█████████▎| 38/41 [00:06<00:00,  5.60it/s, loss=79.1, v_num=0, train_loss_step=94.70, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  93%|█████████▎| 38/41 [00:06<00:00,  5.59it/s, loss=80.9, v_num=0, train_loss_step=94.50, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  95%|█████████▌| 39/41 [00:06<00:00,  5.60it/s, loss=80.9, v_num=0, train_loss_step=94.50, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  95%|█████████▌| 39/41 [00:06<00:00,  5.60it/s, loss=80.9, v_num=0, train_loss_step=94.50, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  95%|█████████▌| 39/41 [00:06<00:00,  5.60it/s, loss=80.4, v_num=0, train_loss_step=78.50, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  98%|█████████▊| 40/41 [00:07<00:00,  5.59it/s, loss=80.4, v_num=0, train_loss_step=78.50, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17:  98%|█████████▊| 40/41 [00:07<00:00,  5.59it/s, loss=80.4, v_num=0, train_loss_step=78.50, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17:  98%|█████████▊| 40/41 [00:07<00:00,  5.59it/s, loss=81, v_num=0, train_loss_step=75.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.87it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 17: 100%|██████████| 41/41 [00:07<00:00,  5.53it/s, loss=81, v_num=0, train_loss_step=75.30, val_loss=147.0, train_loss_epoch=80.80]#015Epoch 17: 100%|██████████| 41/41 [00:07<00:00,  5.53it/s, loss=81, v_num=0, train_loss_step=75.30, val_loss=147.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34mEpoch 17: 100%|██████████| 41/41 [00:08<00:00,  5.05it/s, loss=81, v_num=0, train_loss_step=75.30, val_loss=148.0, train_loss_epoch=80.80]\u001b[0m\n",
      "\u001b[34m#015                                                                      #033[A\u001b[0m\n",
      "\u001b[34mEpoch 17: 100%|██████████| 41/41 [00:08<00:00,  5.05it/s, loss=81, v_num=0, train_loss_step=75.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 17:   0%|          | 0/41 [00:00<?, ?it/s, loss=81, v_num=0, train_loss_step=75.30, val_loss=148.0, train_loss_epoch=79.20]         #015Epoch 18:   0%|          | 0/41 [00:00<?, ?it/s, loss=81, v_num=0, train_loss_step=75.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:   2%|▏         | 1/41 [00:00<00:07,  5.03it/s, loss=81, v_num=0, train_loss_step=75.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:   2%|▏         | 1/41 [00:00<00:07,  5.02it/s, loss=81, v_num=0, train_loss_step=75.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:   2%|▏         | 1/41 [00:00<00:07,  5.01it/s, loss=81.3, v_num=0, train_loss_step=86.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:   5%|▍         | 2/41 [00:00<00:07,  5.28it/s, loss=81.3, v_num=0, train_loss_step=86.30, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:   5%|▍         | 2/41 [00:00<00:07,  5.28it/s, loss=81.3, v_num=0, train_loss_step=86.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:   5%|▍         | 2/41 [00:00<00:07,  5.28it/s, loss=81.6, v_num=0, train_loss_step=83.20, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:   7%|▋         | 3/41 [00:00<00:07,  5.37it/s, loss=81.6, v_num=0, train_loss_step=83.20, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:   7%|▋         | 3/41 [00:00<00:07,  5.37it/s, loss=81.6, v_num=0, train_loss_step=83.20, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:   7%|▋         | 3/41 [00:00<00:07,  5.36it/s, loss=81.6, v_num=0, train_loss_step=78.10, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  10%|▉         | 4/41 [00:00<00:06,  5.45it/s, loss=81.6, v_num=0, train_loss_step=78.10, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  10%|▉         | 4/41 [00:00<00:06,  5.45it/s, loss=81.6, v_num=0, train_loss_step=78.10, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  10%|▉         | 4/41 [00:00<00:06,  5.45it/s, loss=81.5, v_num=0, train_loss_step=77.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  12%|█▏        | 5/41 [00:00<00:06,  5.50it/s, loss=81.5, v_num=0, train_loss_step=77.90, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  12%|█▏        | 5/41 [00:00<00:06,  5.50it/s, loss=81.5, v_num=0, train_loss_step=77.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  12%|█▏        | 5/41 [00:00<00:06,  5.49it/s, loss=81.4, v_num=0, train_loss_step=81.80, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  15%|█▍        | 6/41 [00:01<00:06,  5.44it/s, loss=81.4, v_num=0, train_loss_step=81.80, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  15%|█▍        | 6/41 [00:01<00:06,  5.44it/s, loss=81.4, v_num=0, train_loss_step=81.80, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  15%|█▍        | 6/41 [00:01<00:06,  5.44it/s, loss=81.2, v_num=0, train_loss_step=73.60, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  17%|█▋        | 7/41 [00:01<00:06,  5.47it/s, loss=81.2, v_num=0, train_loss_step=73.60, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  17%|█▋        | 7/41 [00:01<00:06,  5.47it/s, loss=81.2, v_num=0, train_loss_step=73.60, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  17%|█▋        | 7/41 [00:01<00:06,  5.47it/s, loss=81.6, v_num=0, train_loss_step=72.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  20%|█▉        | 8/41 [00:01<00:06,  5.49it/s, loss=81.6, v_num=0, train_loss_step=72.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  20%|█▉        | 8/41 [00:01<00:06,  5.48it/s, loss=81.6, v_num=0, train_loss_step=72.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  20%|█▉        | 8/41 [00:01<00:06,  5.48it/s, loss=83.4, v_num=0, train_loss_step=95.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  22%|██▏       | 9/41 [00:01<00:05,  5.50it/s, loss=83.4, v_num=0, train_loss_step=95.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  22%|██▏       | 9/41 [00:01<00:05,  5.50it/s, loss=83.4, v_num=0, train_loss_step=95.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  22%|██▏       | 9/41 [00:01<00:05,  5.50it/s, loss=83.4, v_num=0, train_loss_step=73.80, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  24%|██▍       | 10/41 [00:01<00:05,  5.52it/s, loss=83.4, v_num=0, train_loss_step=73.80, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  24%|██▍       | 10/41 [00:01<00:05,  5.52it/s, loss=83.4, v_num=0, train_loss_step=73.80, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  24%|██▍       | 10/41 [00:01<00:05,  5.51it/s, loss=82.7, v_num=0, train_loss_step=76.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  27%|██▋       | 11/41 [00:01<00:05,  5.53it/s, loss=82.7, v_num=0, train_loss_step=76.90, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  27%|██▋       | 11/41 [00:01<00:05,  5.53it/s, loss=82.7, v_num=0, train_loss_step=76.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  27%|██▋       | 11/41 [00:01<00:05,  5.53it/s, loss=82.2, v_num=0, train_loss_step=79.70, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  29%|██▉       | 12/41 [00:02<00:05,  5.51it/s, loss=82.2, v_num=0, train_loss_step=79.70, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  29%|██▉       | 12/41 [00:02<00:05,  5.51it/s, loss=82.2, v_num=0, train_loss_step=79.70, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  29%|██▉       | 12/41 [00:02<00:05,  5.51it/s, loss=80.5, v_num=0, train_loss_step=66.40, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  32%|███▏      | 13/41 [00:02<00:05,  5.52it/s, loss=80.5, v_num=0, train_loss_step=66.40, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  32%|███▏      | 13/41 [00:02<00:05,  5.52it/s, loss=80.5, v_num=0, train_loss_step=66.40, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  32%|███▏      | 13/41 [00:02<00:05,  5.52it/s, loss=80.8, v_num=0, train_loss_step=91.20, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  34%|███▍      | 14/41 [00:02<00:04,  5.51it/s, loss=80.8, v_num=0, train_loss_step=91.20, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  34%|███▍      | 14/41 [00:02<00:04,  5.51it/s, loss=80.8, v_num=0, train_loss_step=91.20, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  34%|███▍      | 14/41 [00:02<00:04,  5.51it/s, loss=80.8, v_num=0, train_loss_step=82.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  37%|███▋      | 15/41 [00:02<00:04,  5.51it/s, loss=80.8, v_num=0, train_loss_step=82.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  37%|███▋      | 15/41 [00:02<00:04,  5.51it/s, loss=80.8, v_num=0, train_loss_step=82.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  37%|███▋      | 15/41 [00:02<00:04,  5.51it/s, loss=81.1, v_num=0, train_loss_step=77.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  39%|███▉      | 16/41 [00:02<00:04,  5.53it/s, loss=81.1, v_num=0, train_loss_step=77.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  39%|███▉      | 16/41 [00:02<00:04,  5.53it/s, loss=81.1, v_num=0, train_loss_step=77.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  39%|███▉      | 16/41 [00:02<00:04,  5.52it/s, loss=80.2, v_num=0, train_loss_step=63.20, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  41%|████▏     | 17/41 [00:03<00:04,  5.54it/s, loss=80.2, v_num=0, train_loss_step=63.20, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  41%|████▏     | 17/41 [00:03<00:04,  5.54it/s, loss=80.2, v_num=0, train_loss_step=63.20, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  41%|████▏     | 17/41 [00:03<00:04,  5.53it/s, loss=79.4, v_num=0, train_loss_step=79.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  44%|████▍     | 18/41 [00:03<00:04,  5.52it/s, loss=79.4, v_num=0, train_loss_step=79.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  44%|████▍     | 18/41 [00:03<00:04,  5.52it/s, loss=79.4, v_num=0, train_loss_step=79.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  44%|████▍     | 18/41 [00:03<00:04,  5.52it/s, loss=78.9, v_num=0, train_loss_step=83.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  46%|████▋     | 19/41 [00:03<00:03,  5.53it/s, loss=78.9, v_num=0, train_loss_step=83.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  46%|████▋     | 19/41 [00:03<00:03,  5.53it/s, loss=78.9, v_num=0, train_loss_step=83.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  46%|████▋     | 19/41 [00:03<00:03,  5.52it/s, loss=77.6, v_num=0, train_loss_step=52.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  49%|████▉     | 20/41 [00:03<00:03,  5.51it/s, loss=77.6, v_num=0, train_loss_step=52.90, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  49%|████▉     | 20/41 [00:03<00:03,  5.51it/s, loss=77.6, v_num=0, train_loss_step=52.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  49%|████▉     | 20/41 [00:03<00:03,  5.51it/s, loss=77.8, v_num=0, train_loss_step=79.40, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  51%|█████     | 21/41 [00:03<00:03,  5.51it/s, loss=77.8, v_num=0, train_loss_step=79.40, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  51%|█████     | 21/41 [00:03<00:03,  5.51it/s, loss=77.8, v_num=0, train_loss_step=79.40, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  51%|█████     | 21/41 [00:03<00:03,  5.51it/s, loss=76.4, v_num=0, train_loss_step=57.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  54%|█████▎    | 22/41 [00:03<00:03,  5.51it/s, loss=76.4, v_num=0, train_loss_step=57.90, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  54%|█████▎    | 22/41 [00:03<00:03,  5.51it/s, loss=76.4, v_num=0, train_loss_step=57.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  54%|█████▎    | 22/41 [00:03<00:03,  5.51it/s, loss=76.2, v_num=0, train_loss_step=78.50, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  56%|█████▌    | 23/41 [00:04<00:03,  5.51it/s, loss=76.2, v_num=0, train_loss_step=78.50, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  56%|█████▌    | 23/41 [00:04<00:03,  5.51it/s, loss=76.2, v_num=0, train_loss_step=78.50, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  56%|█████▌    | 23/41 [00:04<00:03,  5.51it/s, loss=77.2, v_num=0, train_loss_step=99.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  59%|█████▊    | 24/41 [00:04<00:03,  5.49it/s, loss=77.2, v_num=0, train_loss_step=99.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  59%|█████▊    | 24/41 [00:04<00:03,  5.49it/s, loss=77.2, v_num=0, train_loss_step=99.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  59%|█████▊    | 24/41 [00:04<00:03,  5.49it/s, loss=77.7, v_num=0, train_loss_step=86.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  61%|██████    | 25/41 [00:04<00:02,  5.49it/s, loss=77.7, v_num=0, train_loss_step=86.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  61%|██████    | 25/41 [00:04<00:02,  5.49it/s, loss=77.7, v_num=0, train_loss_step=86.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  61%|██████    | 25/41 [00:04<00:02,  5.49it/s, loss=77.7, v_num=0, train_loss_step=81.80, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  63%|██████▎   | 26/41 [00:04<00:02,  5.50it/s, loss=77.7, v_num=0, train_loss_step=81.80, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  63%|██████▎   | 26/41 [00:04<00:02,  5.50it/s, loss=77.7, v_num=0, train_loss_step=81.80, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  63%|██████▎   | 26/41 [00:04<00:02,  5.50it/s, loss=77.6, v_num=0, train_loss_step=72.20, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  66%|██████▌   | 27/41 [00:04<00:02,  5.51it/s, loss=77.6, v_num=0, train_loss_step=72.20, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  66%|██████▌   | 27/41 [00:04<00:02,  5.51it/s, loss=77.6, v_num=0, train_loss_step=72.20, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  66%|██████▌   | 27/41 [00:04<00:02,  5.51it/s, loss=77.8, v_num=0, train_loss_step=77.10, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  68%|██████▊   | 28/41 [00:05<00:02,  5.51it/s, loss=77.8, v_num=0, train_loss_step=77.10, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  68%|██████▊   | 28/41 [00:05<00:02,  5.51it/s, loss=77.8, v_num=0, train_loss_step=77.10, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  68%|██████▊   | 28/41 [00:05<00:02,  5.51it/s, loss=77.9, v_num=0, train_loss_step=97.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  71%|███████   | 29/41 [00:05<00:02,  5.52it/s, loss=77.9, v_num=0, train_loss_step=97.30, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  71%|███████   | 29/41 [00:05<00:02,  5.52it/s, loss=77.9, v_num=0, train_loss_step=97.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  71%|███████   | 29/41 [00:05<00:02,  5.52it/s, loss=78.9, v_num=0, train_loss_step=93.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  73%|███████▎  | 30/41 [00:05<00:01,  5.51it/s, loss=78.9, v_num=0, train_loss_step=93.90, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  73%|███████▎  | 30/41 [00:05<00:01,  5.51it/s, loss=78.9, v_num=0, train_loss_step=93.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  73%|███████▎  | 30/41 [00:05<00:01,  5.51it/s, loss=78.4, v_num=0, train_loss_step=67.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  76%|███████▌  | 31/41 [00:05<00:01,  5.52it/s, loss=78.4, v_num=0, train_loss_step=67.30, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  76%|███████▌  | 31/41 [00:05<00:01,  5.52it/s, loss=78.4, v_num=0, train_loss_step=67.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  76%|███████▌  | 31/41 [00:05<00:01,  5.52it/s, loss=78.4, v_num=0, train_loss_step=78.70, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  78%|███████▊  | 32/41 [00:05<00:01,  5.52it/s, loss=78.4, v_num=0, train_loss_step=78.70, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  78%|███████▊  | 32/41 [00:05<00:01,  5.52it/s, loss=78.4, v_num=0, train_loss_step=78.70, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  78%|███████▊  | 32/41 [00:05<00:01,  5.52it/s, loss=79.8, v_num=0, train_loss_step=94.10, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  80%|████████  | 33/41 [00:05<00:01,  5.53it/s, loss=79.8, v_num=0, train_loss_step=94.10, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  80%|████████  | 33/41 [00:05<00:01,  5.53it/s, loss=79.8, v_num=0, train_loss_step=94.10, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  80%|████████  | 33/41 [00:05<00:01,  5.53it/s, loss=78.6, v_num=0, train_loss_step=67.20, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  83%|████████▎ | 34/41 [00:06<00:01,  5.53it/s, loss=78.6, v_num=0, train_loss_step=67.20, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  83%|████████▎ | 34/41 [00:06<00:01,  5.53it/s, loss=78.6, v_num=0, train_loss_step=67.20, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  83%|████████▎ | 34/41 [00:06<00:01,  5.53it/s, loss=79, v_num=0, train_loss_step=91.40, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  85%|████████▌ | 35/41 [00:06<00:01,  5.54it/s, loss=79, v_num=0, train_loss_step=91.40, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  85%|████████▌ | 35/41 [00:06<00:01,  5.54it/s, loss=79, v_num=0, train_loss_step=91.40, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  85%|████████▌ | 35/41 [00:06<00:01,  5.54it/s, loss=78.6, v_num=0, train_loss_step=68.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  88%|████████▊ | 36/41 [00:06<00:00,  5.53it/s, loss=78.6, v_num=0, train_loss_step=68.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  88%|████████▊ | 36/41 [00:06<00:00,  5.53it/s, loss=78.6, v_num=0, train_loss_step=68.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  88%|████████▊ | 36/41 [00:06<00:00,  5.53it/s, loss=78.8, v_num=0, train_loss_step=67.10, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  90%|█████████ | 37/41 [00:06<00:00,  5.54it/s, loss=78.8, v_num=0, train_loss_step=67.10, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  90%|█████████ | 37/41 [00:06<00:00,  5.54it/s, loss=78.8, v_num=0, train_loss_step=67.10, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  90%|█████████ | 37/41 [00:06<00:00,  5.54it/s, loss=78.4, v_num=0, train_loss_step=71.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  93%|█████████▎| 38/41 [00:06<00:00,  5.54it/s, loss=78.4, v_num=0, train_loss_step=71.90, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  93%|█████████▎| 38/41 [00:06<00:00,  5.54it/s, loss=78.4, v_num=0, train_loss_step=71.90, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  93%|█████████▎| 38/41 [00:06<00:00,  5.54it/s, loss=78.2, v_num=0, train_loss_step=80.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  95%|█████████▌| 39/41 [00:07<00:00,  5.55it/s, loss=78.2, v_num=0, train_loss_step=80.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  95%|█████████▌| 39/41 [00:07<00:00,  5.55it/s, loss=78.2, v_num=0, train_loss_step=80.30, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  95%|█████████▌| 39/41 [00:07<00:00,  5.55it/s, loss=79.1, v_num=0, train_loss_step=70.00, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  98%|█████████▊| 40/41 [00:07<00:00,  5.55it/s, loss=79.1, v_num=0, train_loss_step=70.00, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18:  98%|█████████▊| 40/41 [00:07<00:00,  5.55it/s, loss=79.1, v_num=0, train_loss_step=70.00, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:  98%|█████████▊| 40/41 [00:07<00:00,  5.55it/s, loss=78.6, v_num=0, train_loss_step=69.40, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 18: 100%|██████████| 41/41 [00:07<00:00,  5.49it/s, loss=78.6, v_num=0, train_loss_step=69.40, val_loss=148.0, train_loss_epoch=79.20]#015Epoch 18: 100%|██████████| 41/41 [00:07<00:00,  5.49it/s, loss=78.6, v_num=0, train_loss_step=69.40, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34mEpoch 18: 100%|██████████| 41/41 [00:08<00:00,  5.12it/s, loss=78.6, v_num=0, train_loss_step=69.40, val_loss=148.0, train_loss_epoch=79.20]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 18: 100%|██████████| 41/41 [00:08<00:00,  5.11it/s, loss=78.6, v_num=0, train_loss_step=69.40, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 18:   0%|          | 0/41 [00:00<?, ?it/s, loss=78.6, v_num=0, train_loss_step=69.40, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:   0%|          | 0/41 [00:00<?, ?it/s, loss=78.6, v_num=0, train_loss_step=69.40, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:   2%|▏         | 1/41 [00:00<00:06,  5.79it/s, loss=78.6, v_num=0, train_loss_step=69.40, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:   2%|▏         | 1/41 [00:00<00:06,  5.78it/s, loss=78.6, v_num=0, train_loss_step=69.40, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:   2%|▏         | 1/41 [00:00<00:06,  5.77it/s, loss=79.1, v_num=0, train_loss_step=68.50, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:   5%|▍         | 2/41 [00:00<00:06,  5.61it/s, loss=79.1, v_num=0, train_loss_step=68.50, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:   5%|▍         | 2/41 [00:00<00:06,  5.60it/s, loss=79.1, v_num=0, train_loss_step=68.50, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:   5%|▍         | 2/41 [00:00<00:06,  5.60it/s, loss=79, v_num=0, train_loss_step=77.40, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:   7%|▋         | 3/41 [00:00<00:06,  5.60it/s, loss=79, v_num=0, train_loss_step=77.40, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:   7%|▋         | 3/41 [00:00<00:06,  5.60it/s, loss=79, v_num=0, train_loss_step=77.40, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:   7%|▋         | 3/41 [00:00<00:06,  5.59it/s, loss=78.2, v_num=0, train_loss_step=83.30, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  10%|▉         | 4/41 [00:00<00:06,  5.63it/s, loss=78.2, v_num=0, train_loss_step=83.30, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  10%|▉         | 4/41 [00:00<00:06,  5.62it/s, loss=78.2, v_num=0, train_loss_step=83.30, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  10%|▉         | 4/41 [00:00<00:06,  5.62it/s, loss=78.8, v_num=0, train_loss_step=98.40, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  12%|█▏        | 5/41 [00:00<00:06,  5.66it/s, loss=78.8, v_num=0, train_loss_step=98.40, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  12%|█▏        | 5/41 [00:00<00:06,  5.65it/s, loss=78.8, v_num=0, train_loss_step=98.40, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  12%|█▏        | 5/41 [00:00<00:06,  5.65it/s, loss=78, v_num=0, train_loss_step=64.60, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  15%|█▍        | 6/41 [00:01<00:06,  5.68it/s, loss=78, v_num=0, train_loss_step=64.60, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  15%|█▍        | 6/41 [00:01<00:06,  5.68it/s, loss=78, v_num=0, train_loss_step=64.60, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  15%|█▍        | 6/41 [00:01<00:06,  5.68it/s, loss=79.1, v_num=0, train_loss_step=96.00, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  17%|█▋        | 7/41 [00:01<00:06,  5.61it/s, loss=79.1, v_num=0, train_loss_step=96.00, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  17%|█▋        | 7/41 [00:01<00:06,  5.61it/s, loss=79.1, v_num=0, train_loss_step=96.00, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  17%|█▋        | 7/41 [00:01<00:06,  5.61it/s, loss=79.5, v_num=0, train_loss_step=85.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  20%|█▉        | 8/41 [00:01<00:05,  5.54it/s, loss=79.5, v_num=0, train_loss_step=85.10, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  20%|█▉        | 8/41 [00:01<00:05,  5.54it/s, loss=79.5, v_num=0, train_loss_step=85.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  20%|█▉        | 8/41 [00:01<00:05,  5.54it/s, loss=79.1, v_num=0, train_loss_step=89.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  22%|██▏       | 9/41 [00:01<00:05,  5.53it/s, loss=79.1, v_num=0, train_loss_step=89.10, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  22%|██▏       | 9/41 [00:01<00:05,  5.52it/s, loss=79.1, v_num=0, train_loss_step=89.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  22%|██▏       | 9/41 [00:01<00:05,  5.52it/s, loss=78.8, v_num=0, train_loss_step=87.30, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  24%|██▍       | 10/41 [00:01<00:05,  5.52it/s, loss=78.8, v_num=0, train_loss_step=87.30, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  24%|██▍       | 10/41 [00:01<00:05,  5.52it/s, loss=78.8, v_num=0, train_loss_step=87.30, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  24%|██▍       | 10/41 [00:01<00:05,  5.52it/s, loss=79.4, v_num=0, train_loss_step=78.60, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  27%|██▋       | 11/41 [00:01<00:05,  5.51it/s, loss=79.4, v_num=0, train_loss_step=78.60, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  27%|██▋       | 11/41 [00:01<00:05,  5.51it/s, loss=79.4, v_num=0, train_loss_step=78.60, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  27%|██▋       | 11/41 [00:01<00:05,  5.50it/s, loss=79.7, v_num=0, train_loss_step=84.50, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  29%|██▉       | 12/41 [00:02<00:05,  5.50it/s, loss=79.7, v_num=0, train_loss_step=84.50, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  29%|██▉       | 12/41 [00:02<00:05,  5.50it/s, loss=79.7, v_num=0, train_loss_step=84.50, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  29%|██▉       | 12/41 [00:02<00:05,  5.50it/s, loss=79, v_num=0, train_loss_step=80.30, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  32%|███▏      | 13/41 [00:02<00:05,  5.50it/s, loss=79, v_num=0, train_loss_step=80.30, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  32%|███▏      | 13/41 [00:02<00:05,  5.50it/s, loss=79, v_num=0, train_loss_step=80.30, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  32%|███▏      | 13/41 [00:02<00:05,  5.50it/s, loss=80.1, v_num=0, train_loss_step=89.80, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  34%|███▍      | 14/41 [00:02<00:04,  5.47it/s, loss=80.1, v_num=0, train_loss_step=89.80, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  34%|███▍      | 14/41 [00:02<00:04,  5.47it/s, loss=80.1, v_num=0, train_loss_step=89.80, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  34%|███▍      | 14/41 [00:02<00:04,  5.47it/s, loss=79, v_num=0, train_loss_step=70.20, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  37%|███▋      | 15/41 [00:02<00:04,  5.46it/s, loss=79, v_num=0, train_loss_step=70.20, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  37%|███▋      | 15/41 [00:02<00:04,  5.46it/s, loss=79, v_num=0, train_loss_step=70.20, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  37%|███▋      | 15/41 [00:02<00:04,  5.46it/s, loss=78.6, v_num=0, train_loss_step=60.40, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  39%|███▉      | 16/41 [00:02<00:04,  5.46it/s, loss=78.6, v_num=0, train_loss_step=60.40, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  39%|███▉      | 16/41 [00:02<00:04,  5.46it/s, loss=78.6, v_num=0, train_loss_step=60.40, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  39%|███▉      | 16/41 [00:02<00:04,  5.46it/s, loss=79, v_num=0, train_loss_step=75.00, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  41%|████▏     | 17/41 [00:03<00:04,  5.45it/s, loss=79, v_num=0, train_loss_step=75.00, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  41%|████▏     | 17/41 [00:03<00:04,  5.45it/s, loss=79, v_num=0, train_loss_step=75.00, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  41%|████▏     | 17/41 [00:03<00:04,  5.45it/s, loss=79.9, v_num=0, train_loss_step=90.00, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  44%|████▍     | 18/41 [00:03<00:04,  5.45it/s, loss=79.9, v_num=0, train_loss_step=90.00, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  44%|████▍     | 18/41 [00:03<00:04,  5.45it/s, loss=79.9, v_num=0, train_loss_step=90.00, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  44%|████▍     | 18/41 [00:03<00:04,  5.45it/s, loss=79.6, v_num=0, train_loss_step=74.70, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  46%|████▋     | 19/41 [00:03<00:04,  5.45it/s, loss=79.6, v_num=0, train_loss_step=74.70, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  46%|████▋     | 19/41 [00:03<00:04,  5.45it/s, loss=79.6, v_num=0, train_loss_step=74.70, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  46%|████▋     | 19/41 [00:03<00:04,  5.45it/s, loss=79.7, v_num=0, train_loss_step=72.00, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  49%|████▉     | 20/41 [00:03<00:03,  5.42it/s, loss=79.7, v_num=0, train_loss_step=72.00, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  49%|████▉     | 20/41 [00:03<00:03,  5.42it/s, loss=79.7, v_num=0, train_loss_step=72.00, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  49%|████▉     | 20/41 [00:03<00:03,  5.42it/s, loss=79.4, v_num=0, train_loss_step=63.70, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  51%|█████     | 21/41 [00:03<00:03,  5.44it/s, loss=79.4, v_num=0, train_loss_step=63.70, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  51%|█████     | 21/41 [00:03<00:03,  5.44it/s, loss=79.4, v_num=0, train_loss_step=63.70, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  51%|█████     | 21/41 [00:03<00:03,  5.44it/s, loss=79.4, v_num=0, train_loss_step=67.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  54%|█████▎    | 22/41 [00:04<00:03,  5.45it/s, loss=79.4, v_num=0, train_loss_step=67.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  54%|█████▎    | 22/41 [00:04<00:03,  5.45it/s, loss=79.4, v_num=0, train_loss_step=67.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  54%|█████▎    | 22/41 [00:04<00:03,  5.45it/s, loss=79.7, v_num=0, train_loss_step=84.30, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  56%|█████▌    | 23/41 [00:04<00:03,  5.46it/s, loss=79.7, v_num=0, train_loss_step=84.30, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  56%|█████▌    | 23/41 [00:04<00:03,  5.45it/s, loss=79.7, v_num=0, train_loss_step=84.30, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  56%|█████▌    | 23/41 [00:04<00:03,  5.45it/s, loss=79.5, v_num=0, train_loss_step=79.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  59%|█████▊    | 24/41 [00:04<00:03,  5.46it/s, loss=79.5, v_num=0, train_loss_step=79.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  59%|█████▊    | 24/41 [00:04<00:03,  5.46it/s, loss=79.5, v_num=0, train_loss_step=79.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  59%|█████▊    | 24/41 [00:04<00:03,  5.46it/s, loss=79.3, v_num=0, train_loss_step=94.20, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  61%|██████    | 25/41 [00:04<00:02,  5.47it/s, loss=79.3, v_num=0, train_loss_step=94.20, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  61%|██████    | 25/41 [00:04<00:02,  5.47it/s, loss=79.3, v_num=0, train_loss_step=94.20, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  61%|██████    | 25/41 [00:04<00:02,  5.47it/s, loss=78.8, v_num=0, train_loss_step=55.60, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  63%|██████▎   | 26/41 [00:04<00:02,  5.47it/s, loss=78.8, v_num=0, train_loss_step=55.60, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  63%|██████▎   | 26/41 [00:04<00:02,  5.47it/s, loss=78.8, v_num=0, train_loss_step=55.60, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  63%|██████▎   | 26/41 [00:04<00:02,  5.47it/s, loss=78.6, v_num=0, train_loss_step=90.90, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  66%|██████▌   | 27/41 [00:04<00:02,  5.47it/s, loss=78.6, v_num=0, train_loss_step=90.90, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  66%|██████▌   | 27/41 [00:04<00:02,  5.47it/s, loss=78.6, v_num=0, train_loss_step=90.90, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  66%|██████▌   | 27/41 [00:04<00:02,  5.47it/s, loss=78.8, v_num=0, train_loss_step=88.50, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  68%|██████▊   | 28/41 [00:05<00:02,  5.48it/s, loss=78.8, v_num=0, train_loss_step=88.50, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  68%|██████▊   | 28/41 [00:05<00:02,  5.48it/s, loss=78.8, v_num=0, train_loss_step=88.50, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  68%|██████▊   | 28/41 [00:05<00:02,  5.48it/s, loss=78.7, v_num=0, train_loss_step=87.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  71%|███████   | 29/41 [00:05<00:02,  5.49it/s, loss=78.7, v_num=0, train_loss_step=87.10, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  71%|███████   | 29/41 [00:05<00:02,  5.49it/s, loss=78.7, v_num=0, train_loss_step=87.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  71%|███████   | 29/41 [00:05<00:02,  5.49it/s, loss=77.8, v_num=0, train_loss_step=70.50, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  73%|███████▎  | 30/41 [00:05<00:02,  5.49it/s, loss=77.8, v_num=0, train_loss_step=70.50, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  73%|███████▎  | 30/41 [00:05<00:02,  5.49it/s, loss=77.8, v_num=0, train_loss_step=70.50, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  73%|███████▎  | 30/41 [00:05<00:02,  5.49it/s, loss=78, v_num=0, train_loss_step=81.90, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  76%|███████▌  | 31/41 [00:05<00:01,  5.50it/s, loss=78, v_num=0, train_loss_step=81.90, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  76%|███████▌  | 31/41 [00:05<00:01,  5.50it/s, loss=78, v_num=0, train_loss_step=81.90, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  76%|███████▌  | 31/41 [00:05<00:01,  5.50it/s, loss=78.2, v_num=0, train_loss_step=88.60, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  78%|███████▊  | 32/41 [00:05<00:01,  5.49it/s, loss=78.2, v_num=0, train_loss_step=88.60, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  78%|███████▊  | 32/41 [00:05<00:01,  5.49it/s, loss=78.2, v_num=0, train_loss_step=88.60, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  78%|███████▊  | 32/41 [00:05<00:01,  5.49it/s, loss=78.8, v_num=0, train_loss_step=91.90, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  80%|████████  | 33/41 [00:06<00:01,  5.49it/s, loss=78.8, v_num=0, train_loss_step=91.90, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  80%|████████  | 33/41 [00:06<00:01,  5.49it/s, loss=78.8, v_num=0, train_loss_step=91.90, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  80%|████████  | 33/41 [00:06<00:01,  5.49it/s, loss=79, v_num=0, train_loss_step=94.00, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  83%|████████▎ | 34/41 [00:06<00:01,  5.50it/s, loss=79, v_num=0, train_loss_step=94.00, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  83%|████████▎ | 34/41 [00:06<00:01,  5.50it/s, loss=79, v_num=0, train_loss_step=94.00, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  83%|████████▎ | 34/41 [00:06<00:01,  5.50it/s, loss=78.2, v_num=0, train_loss_step=54.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  85%|████████▌ | 35/41 [00:06<00:01,  5.50it/s, loss=78.2, v_num=0, train_loss_step=54.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  85%|████████▌ | 35/41 [00:06<00:01,  5.50it/s, loss=78.2, v_num=0, train_loss_step=54.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  85%|████████▌ | 35/41 [00:06<00:01,  5.50it/s, loss=78.9, v_num=0, train_loss_step=75.80, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  88%|████████▊ | 36/41 [00:06<00:00,  5.51it/s, loss=78.9, v_num=0, train_loss_step=75.80, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  88%|████████▊ | 36/41 [00:06<00:00,  5.51it/s, loss=78.9, v_num=0, train_loss_step=75.80, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  88%|████████▊ | 36/41 [00:06<00:00,  5.51it/s, loss=78.2, v_num=0, train_loss_step=60.30, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  90%|█████████ | 37/41 [00:06<00:00,  5.51it/s, loss=78.2, v_num=0, train_loss_step=60.30, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  90%|█████████ | 37/41 [00:06<00:00,  5.51it/s, loss=78.2, v_num=0, train_loss_step=60.30, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  90%|█████████ | 37/41 [00:06<00:00,  5.51it/s, loss=77.6, v_num=0, train_loss_step=77.00, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  93%|█████████▎| 38/41 [00:06<00:00,  5.51it/s, loss=77.6, v_num=0, train_loss_step=77.00, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  93%|█████████▎| 38/41 [00:06<00:00,  5.51it/s, loss=77.6, v_num=0, train_loss_step=77.00, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  93%|█████████▎| 38/41 [00:06<00:00,  5.51it/s, loss=77.4, v_num=0, train_loss_step=70.70, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  95%|█████████▌| 39/41 [00:07<00:00,  5.51it/s, loss=77.4, v_num=0, train_loss_step=70.70, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  95%|█████████▌| 39/41 [00:07<00:00,  5.51it/s, loss=77.4, v_num=0, train_loss_step=70.70, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  95%|█████████▌| 39/41 [00:07<00:00,  5.51it/s, loss=77.9, v_num=0, train_loss_step=83.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  98%|█████████▊| 40/41 [00:07<00:00,  5.52it/s, loss=77.9, v_num=0, train_loss_step=83.10, val_loss=148.0, train_loss_epoch=78.20]#015Epoch 19:  98%|█████████▊| 40/41 [00:07<00:00,  5.52it/s, loss=77.9, v_num=0, train_loss_step=83.10, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:  98%|█████████▊| 40/41 [00:07<00:00,  5.51it/s, loss=78.3, v_num=0, train_loss_step=70.40, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 19: 100%|██████████| 41/41 [00:07<00:00,  5.45it/s, loss=78.3, v_num=0, train_loss_step=70.40, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19: 100%|██████████| 41/41 [00:07<00:00,  5.45it/s, loss=78.3, v_num=0, train_loss_step=70.40, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34mEpoch 19: 100%|██████████| 41/41 [00:08<00:00,  5.08it/s, loss=78.3, v_num=0, train_loss_step=70.40, val_loss=148.0, train_loss_epoch=78.20]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 19: 100%|██████████| 41/41 [00:08<00:00,  5.07it/s, loss=78.3, v_num=0, train_loss_step=70.40, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 19:   0%|          | 0/41 [00:00<?, ?it/s, loss=78.3, v_num=0, train_loss_step=70.40, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:   0%|          | 0/41 [00:00<?, ?it/s, loss=78.3, v_num=0, train_loss_step=70.40, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:   2%|▏         | 1/41 [00:00<00:06,  5.71it/s, loss=78.3, v_num=0, train_loss_step=70.40, val_loss=148.0, train_loss_epoch=77.20]#015Epoch 20:   2%|▏         | 1/41 [00:00<00:07,  5.71it/s, loss=78.3, v_num=0, train_loss_step=70.40, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:   2%|▏         | 1/41 [00:00<00:07,  5.69it/s, loss=79.7, v_num=0, train_loss_step=95.80, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:   5%|▍         | 2/41 [00:00<00:06,  5.69it/s, loss=79.7, v_num=0, train_loss_step=95.80, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:   5%|▍         | 2/41 [00:00<00:06,  5.68it/s, loss=79.7, v_num=0, train_loss_step=95.80, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:   5%|▍         | 2/41 [00:00<00:06,  5.68it/s, loss=79.7, v_num=0, train_loss_step=84.70, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:   7%|▋         | 3/41 [00:00<00:06,  5.70it/s, loss=79.7, v_num=0, train_loss_step=84.70, val_loss=148.0, train_loss_epoch=77.20]#015Epoch 20:   7%|▋         | 3/41 [00:00<00:06,  5.69it/s, loss=79.7, v_num=0, train_loss_step=84.70, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:   7%|▋         | 3/41 [00:00<00:06,  5.69it/s, loss=79, v_num=0, train_loss_step=64.90, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  10%|▉         | 4/41 [00:00<00:06,  5.56it/s, loss=79, v_num=0, train_loss_step=64.90, val_loss=148.0, train_loss_epoch=77.20]#015Epoch 20:  10%|▉         | 4/41 [00:00<00:06,  5.56it/s, loss=79, v_num=0, train_loss_step=64.90, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  10%|▉         | 4/41 [00:00<00:06,  5.56it/s, loss=78, v_num=0, train_loss_step=75.20, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  12%|█▏        | 5/41 [00:00<00:06,  5.56it/s, loss=78, v_num=0, train_loss_step=75.20, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  12%|█▏        | 5/41 [00:00<00:06,  5.56it/s, loss=78, v_num=0, train_loss_step=75.20, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  12%|█▏        | 5/41 [00:00<00:06,  5.55it/s, loss=78.4, v_num=0, train_loss_step=62.30, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  15%|█▍        | 6/41 [00:01<00:06,  5.49it/s, loss=78.4, v_num=0, train_loss_step=62.30, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  15%|█▍        | 6/41 [00:01<00:06,  5.49it/s, loss=78.4, v_num=0, train_loss_step=62.30, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  15%|█▍        | 6/41 [00:01<00:06,  5.49it/s, loss=77.3, v_num=0, train_loss_step=70.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  17%|█▋        | 7/41 [00:01<00:06,  5.49it/s, loss=77.3, v_num=0, train_loss_step=70.00, val_loss=148.0, train_loss_epoch=77.20]#015Epoch 20:  17%|█▋        | 7/41 [00:01<00:06,  5.49it/s, loss=77.3, v_num=0, train_loss_step=70.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  17%|█▋        | 7/41 [00:01<00:06,  5.49it/s, loss=76.9, v_num=0, train_loss_step=80.10, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  20%|█▉        | 8/41 [00:01<00:06,  5.50it/s, loss=76.9, v_num=0, train_loss_step=80.10, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  20%|█▉        | 8/41 [00:01<00:06,  5.50it/s, loss=76.9, v_num=0, train_loss_step=80.10, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  20%|█▉        | 8/41 [00:01<00:06,  5.50it/s, loss=76.5, v_num=0, train_loss_step=79.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  22%|██▏       | 9/41 [00:01<00:05,  5.48it/s, loss=76.5, v_num=0, train_loss_step=79.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  22%|██▏       | 9/41 [00:01<00:05,  5.48it/s, loss=76.5, v_num=0, train_loss_step=79.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  22%|██▏       | 9/41 [00:01<00:05,  5.48it/s, loss=76.6, v_num=0, train_loss_step=72.50, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  24%|██▍       | 10/41 [00:01<00:05,  5.44it/s, loss=76.6, v_num=0, train_loss_step=72.50, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  24%|██▍       | 10/41 [00:01<00:05,  5.44it/s, loss=76.6, v_num=0, train_loss_step=72.50, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  24%|██▍       | 10/41 [00:01<00:05,  5.44it/s, loss=75.2, v_num=0, train_loss_step=54.30, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  27%|██▋       | 11/41 [00:02<00:05,  5.42it/s, loss=75.2, v_num=0, train_loss_step=54.30, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  27%|██▋       | 11/41 [00:02<00:05,  5.42it/s, loss=75.2, v_num=0, train_loss_step=54.30, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  27%|██▋       | 11/41 [00:02<00:05,  5.42it/s, loss=74.2, v_num=0, train_loss_step=67.50, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  29%|██▉       | 12/41 [00:02<00:05,  5.44it/s, loss=74.2, v_num=0, train_loss_step=67.50, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  29%|██▉       | 12/41 [00:02<00:05,  5.44it/s, loss=74.2, v_num=0, train_loss_step=67.50, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  29%|██▉       | 12/41 [00:02<00:05,  5.44it/s, loss=72.4, v_num=0, train_loss_step=57.30, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  32%|███▏      | 13/41 [00:02<00:05,  5.46it/s, loss=72.4, v_num=0, train_loss_step=57.30, val_loss=148.0, train_loss_epoch=77.20]#015Epoch 20:  32%|███▏      | 13/41 [00:02<00:05,  5.46it/s, loss=72.4, v_num=0, train_loss_step=57.30, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  32%|███▏      | 13/41 [00:02<00:05,  5.46it/s, loss=71.4, v_num=0, train_loss_step=73.30, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  34%|███▍      | 14/41 [00:02<00:04,  5.48it/s, loss=71.4, v_num=0, train_loss_step=73.30, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  34%|███▍      | 14/41 [00:02<00:04,  5.47it/s, loss=71.4, v_num=0, train_loss_step=73.30, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  34%|███▍      | 14/41 [00:02<00:04,  5.47it/s, loss=72.8, v_num=0, train_loss_step=82.10, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  37%|███▋      | 15/41 [00:02<00:04,  5.49it/s, loss=72.8, v_num=0, train_loss_step=82.10, val_loss=148.0, train_loss_epoch=77.20]#015Epoch 20:  37%|███▋      | 15/41 [00:02<00:04,  5.49it/s, loss=72.8, v_num=0, train_loss_step=82.10, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  37%|███▋      | 15/41 [00:02<00:04,  5.49it/s, loss=72.7, v_num=0, train_loss_step=74.20, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  39%|███▉      | 16/41 [00:02<00:04,  5.49it/s, loss=72.7, v_num=0, train_loss_step=74.20, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  39%|███▉      | 16/41 [00:02<00:04,  5.49it/s, loss=72.7, v_num=0, train_loss_step=74.20, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  39%|███▉      | 16/41 [00:02<00:04,  5.49it/s, loss=72.8, v_num=0, train_loss_step=61.90, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  41%|████▏     | 17/41 [00:03<00:04,  5.50it/s, loss=72.8, v_num=0, train_loss_step=61.90, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  41%|████▏     | 17/41 [00:03<00:04,  5.50it/s, loss=72.8, v_num=0, train_loss_step=61.90, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  41%|████▏     | 17/41 [00:03<00:04,  5.50it/s, loss=71.9, v_num=0, train_loss_step=58.80, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  44%|████▍     | 18/41 [00:03<00:04,  5.51it/s, loss=71.9, v_num=0, train_loss_step=58.80, val_loss=148.0, train_loss_epoch=77.20]#015Epoch 20:  44%|████▍     | 18/41 [00:03<00:04,  5.51it/s, loss=71.9, v_num=0, train_loss_step=58.80, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  44%|████▍     | 18/41 [00:03<00:04,  5.51it/s, loss=72.3, v_num=0, train_loss_step=78.10, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  46%|████▋     | 19/41 [00:03<00:03,  5.52it/s, loss=72.3, v_num=0, train_loss_step=78.10, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  46%|████▋     | 19/41 [00:03<00:03,  5.52it/s, loss=72.3, v_num=0, train_loss_step=78.10, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  46%|████▋     | 19/41 [00:03<00:03,  5.52it/s, loss=71.5, v_num=0, train_loss_step=67.20, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  49%|████▉     | 20/41 [00:03<00:03,  5.53it/s, loss=71.5, v_num=0, train_loss_step=67.20, val_loss=148.0, train_loss_epoch=77.20]#015Epoch 20:  49%|████▉     | 20/41 [00:03<00:03,  5.53it/s, loss=71.5, v_num=0, train_loss_step=67.20, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  49%|████▉     | 20/41 [00:03<00:03,  5.53it/s, loss=71.9, v_num=0, train_loss_step=78.80, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  51%|█████     | 21/41 [00:03<00:03,  5.54it/s, loss=71.9, v_num=0, train_loss_step=78.80, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  51%|█████     | 21/41 [00:03<00:03,  5.54it/s, loss=71.9, v_num=0, train_loss_step=78.80, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  51%|█████     | 21/41 [00:03<00:03,  5.54it/s, loss=70.2, v_num=0, train_loss_step=61.60, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  54%|█████▎    | 22/41 [00:03<00:03,  5.53it/s, loss=70.2, v_num=0, train_loss_step=61.60, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  54%|█████▎    | 22/41 [00:03<00:03,  5.53it/s, loss=70.2, v_num=0, train_loss_step=61.60, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  54%|█████▎    | 22/41 [00:03<00:03,  5.53it/s, loss=69.2, v_num=0, train_loss_step=64.10, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  56%|█████▌    | 23/41 [00:04<00:03,  5.53it/s, loss=69.2, v_num=0, train_loss_step=64.10, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  56%|█████▌    | 23/41 [00:04<00:03,  5.53it/s, loss=69.2, v_num=0, train_loss_step=64.10, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  56%|█████▌    | 23/41 [00:04<00:03,  5.53it/s, loss=69.6, v_num=0, train_loss_step=73.50, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  59%|█████▊    | 24/41 [00:04<00:03,  5.54it/s, loss=69.6, v_num=0, train_loss_step=73.50, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  59%|█████▊    | 24/41 [00:04<00:03,  5.54it/s, loss=69.6, v_num=0, train_loss_step=73.50, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  59%|█████▊    | 24/41 [00:04<00:03,  5.54it/s, loss=69.8, v_num=0, train_loss_step=78.90, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  61%|██████    | 25/41 [00:04<00:02,  5.54it/s, loss=69.8, v_num=0, train_loss_step=78.90, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  61%|██████    | 25/41 [00:04<00:02,  5.54it/s, loss=69.8, v_num=0, train_loss_step=78.90, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  61%|██████    | 25/41 [00:04<00:02,  5.54it/s, loss=70.4, v_num=0, train_loss_step=73.90, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  63%|██████▎   | 26/41 [00:04<00:02,  5.55it/s, loss=70.4, v_num=0, train_loss_step=73.90, val_loss=148.0, train_loss_epoch=77.20]#015Epoch 20:  63%|██████▎   | 26/41 [00:04<00:02,  5.55it/s, loss=70.4, v_num=0, train_loss_step=73.90, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  63%|██████▎   | 26/41 [00:04<00:02,  5.55it/s, loss=70.8, v_num=0, train_loss_step=78.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  66%|██████▌   | 27/41 [00:04<00:02,  5.56it/s, loss=70.8, v_num=0, train_loss_step=78.00, val_loss=148.0, train_loss_epoch=77.20]#015Epoch 20:  66%|██████▌   | 27/41 [00:04<00:02,  5.56it/s, loss=70.8, v_num=0, train_loss_step=78.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  66%|██████▌   | 27/41 [00:04<00:02,  5.56it/s, loss=70.2, v_num=0, train_loss_step=68.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  68%|██████▊   | 28/41 [00:05<00:02,  5.55it/s, loss=70.2, v_num=0, train_loss_step=68.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  68%|██████▊   | 28/41 [00:05<00:02,  5.55it/s, loss=70.2, v_num=0, train_loss_step=68.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  68%|██████▊   | 28/41 [00:05<00:02,  5.55it/s, loss=69.6, v_num=0, train_loss_step=69.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  71%|███████   | 29/41 [00:05<00:02,  5.56it/s, loss=69.6, v_num=0, train_loss_step=69.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  71%|███████   | 29/41 [00:05<00:02,  5.55it/s, loss=69.6, v_num=0, train_loss_step=69.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  71%|███████   | 29/41 [00:05<00:02,  5.55it/s, loss=69.6, v_num=0, train_loss_step=70.60, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  73%|███████▎  | 30/41 [00:05<00:01,  5.55it/s, loss=69.6, v_num=0, train_loss_step=70.60, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  73%|███████▎  | 30/41 [00:05<00:01,  5.55it/s, loss=69.6, v_num=0, train_loss_step=70.60, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  73%|███████▎  | 30/41 [00:05<00:01,  5.55it/s, loss=70.1, v_num=0, train_loss_step=65.90, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  76%|███████▌  | 31/41 [00:05<00:01,  5.56it/s, loss=70.1, v_num=0, train_loss_step=65.90, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  76%|███████▌  | 31/41 [00:05<00:01,  5.56it/s, loss=70.1, v_num=0, train_loss_step=65.90, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  76%|███████▌  | 31/41 [00:05<00:01,  5.56it/s, loss=70.9, v_num=0, train_loss_step=82.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  78%|███████▊  | 32/41 [00:05<00:01,  5.57it/s, loss=70.9, v_num=0, train_loss_step=82.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  78%|███████▊  | 32/41 [00:05<00:01,  5.57it/s, loss=70.9, v_num=0, train_loss_step=82.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  78%|███████▊  | 32/41 [00:05<00:01,  5.56it/s, loss=71.8, v_num=0, train_loss_step=76.50, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  80%|████████  | 33/41 [00:05<00:01,  5.56it/s, loss=71.8, v_num=0, train_loss_step=76.50, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  80%|████████  | 33/41 [00:05<00:01,  5.56it/s, loss=71.8, v_num=0, train_loss_step=76.50, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  80%|████████  | 33/41 [00:05<00:01,  5.56it/s, loss=71.4, v_num=0, train_loss_step=65.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  83%|████████▎ | 34/41 [00:06<00:01,  5.56it/s, loss=71.4, v_num=0, train_loss_step=65.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  83%|████████▎ | 34/41 [00:06<00:01,  5.56it/s, loss=71.4, v_num=0, train_loss_step=65.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  83%|████████▎ | 34/41 [00:06<00:01,  5.56it/s, loss=70.5, v_num=0, train_loss_step=64.40, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  85%|████████▌ | 35/41 [00:06<00:01,  5.56it/s, loss=70.5, v_num=0, train_loss_step=64.40, val_loss=148.0, train_loss_epoch=77.20]#015Epoch 20:  85%|████████▌ | 35/41 [00:06<00:01,  5.56it/s, loss=70.5, v_num=0, train_loss_step=64.40, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  85%|████████▌ | 35/41 [00:06<00:01,  5.56it/s, loss=70.8, v_num=0, train_loss_step=79.80, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  88%|████████▊ | 36/41 [00:06<00:00,  5.57it/s, loss=70.8, v_num=0, train_loss_step=79.80, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  88%|████████▊ | 36/41 [00:06<00:00,  5.57it/s, loss=70.8, v_num=0, train_loss_step=79.80, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  88%|████████▊ | 36/41 [00:06<00:00,  5.57it/s, loss=71.4, v_num=0, train_loss_step=73.70, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  90%|█████████ | 37/41 [00:06<00:00,  5.57it/s, loss=71.4, v_num=0, train_loss_step=73.70, val_loss=148.0, train_loss_epoch=77.20]#015Epoch 20:  90%|█████████ | 37/41 [00:06<00:00,  5.57it/s, loss=71.4, v_num=0, train_loss_step=73.70, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  90%|█████████ | 37/41 [00:06<00:00,  5.57it/s, loss=72.1, v_num=0, train_loss_step=73.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  93%|█████████▎| 38/41 [00:06<00:00,  5.58it/s, loss=72.1, v_num=0, train_loss_step=73.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  93%|█████████▎| 38/41 [00:06<00:00,  5.58it/s, loss=72.1, v_num=0, train_loss_step=73.00, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  93%|█████████▎| 38/41 [00:06<00:00,  5.57it/s, loss=71.3, v_num=0, train_loss_step=62.60, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  95%|█████████▌| 39/41 [00:06<00:00,  5.58it/s, loss=71.3, v_num=0, train_loss_step=62.60, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  95%|█████████▌| 39/41 [00:06<00:00,  5.58it/s, loss=71.3, v_num=0, train_loss_step=62.60, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  95%|█████████▌| 39/41 [00:06<00:00,  5.58it/s, loss=71.9, v_num=0, train_loss_step=79.30, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  98%|█████████▊| 40/41 [00:07<00:00,  5.58it/s, loss=71.9, v_num=0, train_loss_step=79.30, val_loss=148.0, train_loss_epoch=77.20]#015Epoch 20:  98%|█████████▊| 40/41 [00:07<00:00,  5.58it/s, loss=71.9, v_num=0, train_loss_step=79.30, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:  98%|█████████▊| 40/41 [00:07<00:00,  5.58it/s, loss=71.4, v_num=0, train_loss_step=66.90, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 20: 100%|██████████| 41/41 [00:07<00:00,  5.50it/s, loss=71.4, v_num=0, train_loss_step=66.90, val_loss=148.0, train_loss_epoch=77.20]#015Epoch 20: 100%|██████████| 41/41 [00:07<00:00,  5.50it/s, loss=71.4, v_num=0, train_loss_step=66.90, val_loss=148.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34mEpoch 20: 100%|██████████| 41/41 [00:08<00:00,  5.11it/s, loss=71.4, v_num=0, train_loss_step=66.90, val_loss=149.0, train_loss_epoch=77.20]\u001b[0m\n",
      "\u001b[34m#015                                                                      #033[A\u001b[0m\n",
      "\u001b[34mEpoch 20: 100%|██████████| 41/41 [00:08<00:00,  5.11it/s, loss=71.4, v_num=0, train_loss_step=66.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 20:   0%|          | 0/41 [00:00<?, ?it/s, loss=71.4, v_num=0, train_loss_step=66.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:   0%|          | 0/41 [00:00<?, ?it/s, loss=71.4, v_num=0, train_loss_step=66.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:   2%|▏         | 1/41 [00:00<00:07,  5.56it/s, loss=71.4, v_num=0, train_loss_step=66.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:   2%|▏         | 1/41 [00:00<00:07,  5.55it/s, loss=71.4, v_num=0, train_loss_step=66.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:   2%|▏         | 1/41 [00:00<00:07,  5.54it/s, loss=71.8, v_num=0, train_loss_step=71.10, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:   5%|▍         | 2/41 [00:00<00:06,  5.58it/s, loss=71.8, v_num=0, train_loss_step=71.10, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:   5%|▍         | 2/41 [00:00<00:06,  5.58it/s, loss=71.8, v_num=0, train_loss_step=71.10, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:   5%|▍         | 2/41 [00:00<00:06,  5.57it/s, loss=71.8, v_num=0, train_loss_step=64.10, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:   7%|▋         | 3/41 [00:00<00:07,  5.22it/s, loss=71.8, v_num=0, train_loss_step=64.10, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:   7%|▋         | 3/41 [00:00<00:07,  5.22it/s, loss=71.8, v_num=0, train_loss_step=64.10, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:   7%|▋         | 3/41 [00:00<00:07,  5.21it/s, loss=71.5, v_num=0, train_loss_step=66.10, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  10%|▉         | 4/41 [00:00<00:07,  5.19it/s, loss=71.5, v_num=0, train_loss_step=66.10, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  10%|▉         | 4/41 [00:00<00:07,  5.18it/s, loss=71.5, v_num=0, train_loss_step=66.10, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  10%|▉         | 4/41 [00:00<00:07,  5.18it/s, loss=71.6, v_num=0, train_loss_step=81.30, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  12%|█▏        | 5/41 [00:00<00:06,  5.25it/s, loss=71.6, v_num=0, train_loss_step=81.30, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  12%|█▏        | 5/41 [00:00<00:06,  5.25it/s, loss=71.6, v_num=0, train_loss_step=81.30, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  12%|█▏        | 5/41 [00:00<00:06,  5.25it/s, loss=71.4, v_num=0, train_loss_step=69.50, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  15%|█▍        | 6/41 [00:01<00:06,  5.10it/s, loss=71.4, v_num=0, train_loss_step=69.50, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  15%|█▍        | 6/41 [00:01<00:06,  5.10it/s, loss=71.4, v_num=0, train_loss_step=69.50, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  15%|█▍        | 6/41 [00:01<00:06,  5.10it/s, loss=71.6, v_num=0, train_loss_step=83.00, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  17%|█▋        | 7/41 [00:01<00:06,  5.05it/s, loss=71.6, v_num=0, train_loss_step=83.00, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  17%|█▋        | 7/41 [00:01<00:06,  5.05it/s, loss=71.6, v_num=0, train_loss_step=83.00, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  17%|█▋        | 7/41 [00:01<00:06,  5.05it/s, loss=71.9, v_num=0, train_loss_step=73.60, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  20%|█▉        | 8/41 [00:01<00:06,  5.02it/s, loss=71.9, v_num=0, train_loss_step=73.60, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  20%|█▉        | 8/41 [00:01<00:06,  5.02it/s, loss=71.9, v_num=0, train_loss_step=73.60, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  20%|█▉        | 8/41 [00:01<00:06,  5.01it/s, loss=72.9, v_num=0, train_loss_step=89.40, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  22%|██▏       | 9/41 [00:01<00:06,  4.99it/s, loss=72.9, v_num=0, train_loss_step=89.40, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  22%|██▏       | 9/41 [00:01<00:06,  4.99it/s, loss=72.9, v_num=0, train_loss_step=89.40, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  22%|██▏       | 9/41 [00:01<00:06,  4.99it/s, loss=73.4, v_num=0, train_loss_step=81.10, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  24%|██▍       | 10/41 [00:02<00:06,  4.97it/s, loss=73.4, v_num=0, train_loss_step=81.10, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  24%|██▍       | 10/41 [00:02<00:06,  4.97it/s, loss=73.4, v_num=0, train_loss_step=81.10, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  24%|██▍       | 10/41 [00:02<00:06,  4.97it/s, loss=73.6, v_num=0, train_loss_step=69.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  27%|██▋       | 11/41 [00:02<00:06,  4.95it/s, loss=73.6, v_num=0, train_loss_step=69.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  27%|██▋       | 11/41 [00:02<00:06,  4.95it/s, loss=73.6, v_num=0, train_loss_step=69.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  27%|██▋       | 11/41 [00:02<00:06,  4.95it/s, loss=73.1, v_num=0, train_loss_step=71.20, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  29%|██▉       | 12/41 [00:02<00:05,  4.91it/s, loss=73.1, v_num=0, train_loss_step=71.20, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  29%|██▉       | 12/41 [00:02<00:05,  4.91it/s, loss=73.1, v_num=0, train_loss_step=71.20, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  29%|██▉       | 12/41 [00:02<00:05,  4.91it/s, loss=72.9, v_num=0, train_loss_step=73.00, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  32%|███▏      | 13/41 [00:02<00:05,  4.90it/s, loss=72.9, v_num=0, train_loss_step=73.00, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  32%|███▏      | 13/41 [00:02<00:05,  4.90it/s, loss=72.9, v_num=0, train_loss_step=73.00, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  32%|███▏      | 13/41 [00:02<00:05,  4.90it/s, loss=74.2, v_num=0, train_loss_step=90.70, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  34%|███▍      | 14/41 [00:02<00:05,  4.95it/s, loss=74.2, v_num=0, train_loss_step=90.70, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  34%|███▍      | 14/41 [00:02<00:05,  4.95it/s, loss=74.2, v_num=0, train_loss_step=90.70, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  34%|███▍      | 14/41 [00:02<00:05,  4.95it/s, loss=75.7, v_num=0, train_loss_step=94.40, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  37%|███▋      | 15/41 [00:03<00:05,  5.00it/s, loss=75.7, v_num=0, train_loss_step=94.40, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  37%|███▋      | 15/41 [00:03<00:05,  5.00it/s, loss=75.7, v_num=0, train_loss_step=94.40, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  37%|███▋      | 15/41 [00:03<00:05,  5.00it/s, loss=75.1, v_num=0, train_loss_step=68.00, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  39%|███▉      | 16/41 [00:03<00:04,  5.04it/s, loss=75.1, v_num=0, train_loss_step=68.00, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  39%|███▉      | 16/41 [00:03<00:04,  5.04it/s, loss=75.1, v_num=0, train_loss_step=68.00, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  39%|███▉      | 16/41 [00:03<00:04,  5.04it/s, loss=74.8, v_num=0, train_loss_step=68.20, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  41%|████▏     | 17/41 [00:03<00:04,  5.07it/s, loss=74.8, v_num=0, train_loss_step=68.20, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  41%|████▏     | 17/41 [00:03<00:04,  5.07it/s, loss=74.8, v_num=0, train_loss_step=68.20, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  41%|████▏     | 17/41 [00:03<00:04,  5.07it/s, loss=75, v_num=0, train_loss_step=77.50, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  44%|████▍     | 18/41 [00:03<00:04,  5.09it/s, loss=75, v_num=0, train_loss_step=77.50, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  44%|████▍     | 18/41 [00:03<00:04,  5.09it/s, loss=75, v_num=0, train_loss_step=77.50, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  44%|████▍     | 18/41 [00:03<00:04,  5.09it/s, loss=75.6, v_num=0, train_loss_step=73.70, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  46%|████▋     | 19/41 [00:03<00:04,  5.12it/s, loss=75.6, v_num=0, train_loss_step=73.70, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  46%|████▋     | 19/41 [00:03<00:04,  5.12it/s, loss=75.6, v_num=0, train_loss_step=73.70, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  46%|████▋     | 19/41 [00:03<00:04,  5.12it/s, loss=76, v_num=0, train_loss_step=87.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  49%|████▉     | 20/41 [00:03<00:04,  5.15it/s, loss=76, v_num=0, train_loss_step=87.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  49%|████▉     | 20/41 [00:03<00:04,  5.15it/s, loss=76, v_num=0, train_loss_step=87.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  49%|████▉     | 20/41 [00:03<00:04,  5.15it/s, loss=76.9, v_num=0, train_loss_step=83.50, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  51%|█████     | 21/41 [00:04<00:03,  5.18it/s, loss=76.9, v_num=0, train_loss_step=83.50, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  51%|█████     | 21/41 [00:04<00:03,  5.18it/s, loss=76.9, v_num=0, train_loss_step=83.50, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  51%|█████     | 21/41 [00:04<00:03,  5.18it/s, loss=77, v_num=0, train_loss_step=73.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  54%|█████▎    | 22/41 [00:04<00:03,  5.20it/s, loss=77, v_num=0, train_loss_step=73.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  54%|█████▎    | 22/41 [00:04<00:03,  5.20it/s, loss=77, v_num=0, train_loss_step=73.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  54%|█████▎    | 22/41 [00:04<00:03,  5.20it/s, loss=77.4, v_num=0, train_loss_step=72.80, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  56%|█████▌    | 23/41 [00:04<00:03,  5.22it/s, loss=77.4, v_num=0, train_loss_step=72.80, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  56%|█████▌    | 23/41 [00:04<00:03,  5.22it/s, loss=77.4, v_num=0, train_loss_step=72.80, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  56%|█████▌    | 23/41 [00:04<00:03,  5.22it/s, loss=77.9, v_num=0, train_loss_step=75.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  59%|█████▊    | 24/41 [00:04<00:03,  5.23it/s, loss=77.9, v_num=0, train_loss_step=75.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  59%|█████▊    | 24/41 [00:04<00:03,  5.23it/s, loss=77.9, v_num=0, train_loss_step=75.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  59%|█████▊    | 24/41 [00:04<00:03,  5.23it/s, loss=77.6, v_num=0, train_loss_step=75.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  61%|██████    | 25/41 [00:04<00:03,  5.24it/s, loss=77.6, v_num=0, train_loss_step=75.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  61%|██████    | 25/41 [00:04<00:03,  5.24it/s, loss=77.6, v_num=0, train_loss_step=75.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  61%|██████    | 25/41 [00:04<00:03,  5.24it/s, loss=77.7, v_num=0, train_loss_step=70.20, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  63%|██████▎   | 26/41 [00:04<00:02,  5.26it/s, loss=77.7, v_num=0, train_loss_step=70.20, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  63%|██████▎   | 26/41 [00:04<00:02,  5.26it/s, loss=77.7, v_num=0, train_loss_step=70.20, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  63%|██████▎   | 26/41 [00:04<00:02,  5.26it/s, loss=76.3, v_num=0, train_loss_step=56.10, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  66%|██████▌   | 27/41 [00:05<00:02,  5.28it/s, loss=76.3, v_num=0, train_loss_step=56.10, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  66%|██████▌   | 27/41 [00:05<00:02,  5.28it/s, loss=76.3, v_num=0, train_loss_step=56.10, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  66%|██████▌   | 27/41 [00:05<00:02,  5.28it/s, loss=75.8, v_num=0, train_loss_step=62.20, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  68%|██████▊   | 28/41 [00:05<00:02,  5.29it/s, loss=75.8, v_num=0, train_loss_step=62.20, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  68%|██████▊   | 28/41 [00:05<00:02,  5.29it/s, loss=75.8, v_num=0, train_loss_step=62.20, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  68%|██████▊   | 28/41 [00:05<00:02,  5.29it/s, loss=75.1, v_num=0, train_loss_step=75.80, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  71%|███████   | 29/41 [00:05<00:02,  5.31it/s, loss=75.1, v_num=0, train_loss_step=75.80, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  71%|███████   | 29/41 [00:05<00:02,  5.31it/s, loss=75.1, v_num=0, train_loss_step=75.80, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  71%|███████   | 29/41 [00:05<00:02,  5.31it/s, loss=73.9, v_num=0, train_loss_step=57.80, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  73%|███████▎  | 30/41 [00:05<00:02,  5.31it/s, loss=73.9, v_num=0, train_loss_step=57.80, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  73%|███████▎  | 30/41 [00:05<00:02,  5.31it/s, loss=73.9, v_num=0, train_loss_step=57.80, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  73%|███████▎  | 30/41 [00:05<00:02,  5.31it/s, loss=73.7, v_num=0, train_loss_step=65.00, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  76%|███████▌  | 31/41 [00:05<00:01,  5.32it/s, loss=73.7, v_num=0, train_loss_step=65.00, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  76%|███████▌  | 31/41 [00:05<00:01,  5.32it/s, loss=73.7, v_num=0, train_loss_step=65.00, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  76%|███████▌  | 31/41 [00:05<00:01,  5.32it/s, loss=73.9, v_num=0, train_loss_step=76.00, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  78%|███████▊  | 32/41 [00:05<00:01,  5.33it/s, loss=73.9, v_num=0, train_loss_step=76.00, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  78%|███████▊  | 32/41 [00:05<00:01,  5.33it/s, loss=73.9, v_num=0, train_loss_step=76.00, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  78%|███████▊  | 32/41 [00:05<00:01,  5.33it/s, loss=74.5, v_num=0, train_loss_step=83.60, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  80%|████████  | 33/41 [00:06<00:01,  5.35it/s, loss=74.5, v_num=0, train_loss_step=83.60, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  80%|████████  | 33/41 [00:06<00:01,  5.35it/s, loss=74.5, v_num=0, train_loss_step=83.60, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  80%|████████  | 33/41 [00:06<00:01,  5.35it/s, loss=74.2, v_num=0, train_loss_step=86.20, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  83%|████████▎ | 34/41 [00:06<00:01,  5.36it/s, loss=74.2, v_num=0, train_loss_step=86.20, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  83%|████████▎ | 34/41 [00:06<00:01,  5.36it/s, loss=74.2, v_num=0, train_loss_step=86.20, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  83%|████████▎ | 34/41 [00:06<00:01,  5.36it/s, loss=73, v_num=0, train_loss_step=69.80, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  85%|████████▌ | 35/41 [00:06<00:01,  5.37it/s, loss=73, v_num=0, train_loss_step=69.80, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  85%|████████▌ | 35/41 [00:06<00:01,  5.37it/s, loss=73, v_num=0, train_loss_step=69.80, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  85%|████████▌ | 35/41 [00:06<00:01,  5.37it/s, loss=73.5, v_num=0, train_loss_step=77.40, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  88%|████████▊ | 36/41 [00:06<00:00,  5.37it/s, loss=73.5, v_num=0, train_loss_step=77.40, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  88%|████████▊ | 36/41 [00:06<00:00,  5.37it/s, loss=73.5, v_num=0, train_loss_step=77.40, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  88%|████████▊ | 36/41 [00:06<00:00,  5.37it/s, loss=73.3, v_num=0, train_loss_step=65.60, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  90%|█████████ | 37/41 [00:06<00:00,  5.38it/s, loss=73.3, v_num=0, train_loss_step=65.60, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  90%|█████████ | 37/41 [00:06<00:00,  5.38it/s, loss=73.3, v_num=0, train_loss_step=65.60, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  90%|█████████ | 37/41 [00:06<00:00,  5.38it/s, loss=73.4, v_num=0, train_loss_step=79.50, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  93%|█████████▎| 38/41 [00:07<00:00,  5.39it/s, loss=73.4, v_num=0, train_loss_step=79.50, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  93%|█████████▎| 38/41 [00:07<00:00,  5.39it/s, loss=73.4, v_num=0, train_loss_step=79.50, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  93%|█████████▎| 38/41 [00:07<00:00,  5.39it/s, loss=73, v_num=0, train_loss_step=65.50, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  95%|█████████▌| 39/41 [00:07<00:00,  5.39it/s, loss=73, v_num=0, train_loss_step=65.50, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21:  95%|█████████▌| 39/41 [00:07<00:00,  5.39it/s, loss=73, v_num=0, train_loss_step=65.50, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  95%|█████████▌| 39/41 [00:07<00:00,  5.39it/s, loss=72.5, v_num=0, train_loss_step=77.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  98%|█████████▊| 40/41 [00:07<00:00,  5.40it/s, loss=72.5, v_num=0, train_loss_step=77.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  98%|█████████▊| 40/41 [00:07<00:00,  5.40it/s, loss=72.5, v_num=0, train_loss_step=77.90, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:  98%|█████████▊| 40/41 [00:07<00:00,  5.40it/s, loss=72.5, v_num=0, train_loss_step=83.10, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 21: 100%|██████████| 41/41 [00:07<00:00,  5.34it/s, loss=72.5, v_num=0, train_loss_step=83.10, val_loss=149.0, train_loss_epoch=76.20]#015Epoch 21: 100%|██████████| 41/41 [00:07<00:00,  5.34it/s, loss=72.5, v_num=0, train_loss_step=83.10, val_loss=149.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34mEpoch 21: 100%|██████████| 41/41 [00:08<00:00,  4.98it/s, loss=72.5, v_num=0, train_loss_step=83.10, val_loss=153.0, train_loss_epoch=76.20]\u001b[0m\n",
      "\u001b[34m#015                                                                      #033[A\u001b[0m\n",
      "\u001b[34mEpoch 21: 100%|██████████| 41/41 [00:08<00:00,  4.98it/s, loss=72.5, v_num=0, train_loss_step=83.10, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 21:   0%|          | 0/41 [00:00<?, ?it/s, loss=72.5, v_num=0, train_loss_step=83.10, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:   0%|          | 0/41 [00:00<?, ?it/s, loss=72.5, v_num=0, train_loss_step=83.10, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:   2%|▏         | 1/41 [00:00<00:07,  5.59it/s, loss=72.5, v_num=0, train_loss_step=83.10, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:   2%|▏         | 1/41 [00:00<00:07,  5.58it/s, loss=72.5, v_num=0, train_loss_step=83.10, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:   2%|▏         | 1/41 [00:00<00:07,  5.57it/s, loss=73, v_num=0, train_loss_step=82.80, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:   5%|▍         | 2/41 [00:00<00:07,  5.47it/s, loss=73, v_num=0, train_loss_step=82.80, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:   5%|▍         | 2/41 [00:00<00:07,  5.47it/s, loss=73, v_num=0, train_loss_step=82.80, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:   5%|▍         | 2/41 [00:00<00:07,  5.46it/s, loss=72.4, v_num=0, train_loss_step=61.90, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:   7%|▋         | 3/41 [00:00<00:06,  5.60it/s, loss=72.4, v_num=0, train_loss_step=61.90, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:   7%|▋         | 3/41 [00:00<00:06,  5.60it/s, loss=72.4, v_num=0, train_loss_step=61.90, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:   7%|▋         | 3/41 [00:00<00:06,  5.59it/s, loss=72.8, v_num=0, train_loss_step=83.50, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  10%|▉         | 4/41 [00:00<00:06,  5.64it/s, loss=72.8, v_num=0, train_loss_step=83.50, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  10%|▉         | 4/41 [00:00<00:06,  5.64it/s, loss=72.8, v_num=0, train_loss_step=83.50, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  10%|▉         | 4/41 [00:00<00:06,  5.63it/s, loss=72.9, v_num=0, train_loss_step=77.30, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  12%|█▏        | 5/41 [00:00<00:06,  5.66it/s, loss=72.9, v_num=0, train_loss_step=77.30, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  12%|█▏        | 5/41 [00:00<00:06,  5.66it/s, loss=72.9, v_num=0, train_loss_step=77.30, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  12%|█▏        | 5/41 [00:00<00:06,  5.66it/s, loss=72.7, v_num=0, train_loss_step=67.80, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  15%|█▍        | 6/41 [00:01<00:06,  5.65it/s, loss=72.7, v_num=0, train_loss_step=67.80, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  15%|█▍        | 6/41 [00:01<00:06,  5.65it/s, loss=72.7, v_num=0, train_loss_step=67.80, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  15%|█▍        | 6/41 [00:01<00:06,  5.64it/s, loss=73.8, v_num=0, train_loss_step=77.40, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  17%|█▋        | 7/41 [00:01<00:06,  5.64it/s, loss=73.8, v_num=0, train_loss_step=77.40, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  17%|█▋        | 7/41 [00:01<00:06,  5.64it/s, loss=73.8, v_num=0, train_loss_step=77.40, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  17%|█▋        | 7/41 [00:01<00:06,  5.64it/s, loss=74.1, v_num=0, train_loss_step=67.20, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  20%|█▉        | 8/41 [00:01<00:05,  5.61it/s, loss=74.1, v_num=0, train_loss_step=67.20, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  20%|█▉        | 8/41 [00:01<00:05,  5.61it/s, loss=74.1, v_num=0, train_loss_step=67.20, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  20%|█▉        | 8/41 [00:01<00:05,  5.61it/s, loss=74.1, v_num=0, train_loss_step=75.60, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  22%|██▏       | 9/41 [00:01<00:05,  5.62it/s, loss=74.1, v_num=0, train_loss_step=75.60, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  22%|██▏       | 9/41 [00:01<00:05,  5.62it/s, loss=74.1, v_num=0, train_loss_step=75.60, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  22%|██▏       | 9/41 [00:01<00:05,  5.62it/s, loss=75, v_num=0, train_loss_step=77.50, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  24%|██▍       | 10/41 [00:01<00:05,  5.63it/s, loss=75, v_num=0, train_loss_step=77.50, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  24%|██▍       | 10/41 [00:01<00:05,  5.63it/s, loss=75, v_num=0, train_loss_step=77.50, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  24%|██▍       | 10/41 [00:01<00:05,  5.63it/s, loss=75.2, v_num=0, train_loss_step=68.70, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  27%|██▋       | 11/41 [00:01<00:05,  5.63it/s, loss=75.2, v_num=0, train_loss_step=68.70, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  27%|██▋       | 11/41 [00:01<00:05,  5.63it/s, loss=75.2, v_num=0, train_loss_step=68.70, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  27%|██▋       | 11/41 [00:01<00:05,  5.63it/s, loss=74.7, v_num=0, train_loss_step=66.60, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  29%|██▉       | 12/41 [00:02<00:05,  5.64it/s, loss=74.7, v_num=0, train_loss_step=66.60, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  29%|██▉       | 12/41 [00:02<00:05,  5.64it/s, loss=74.7, v_num=0, train_loss_step=66.60, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  29%|██▉       | 12/41 [00:02<00:05,  5.64it/s, loss=74.2, v_num=0, train_loss_step=73.00, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  32%|███▏      | 13/41 [00:02<00:04,  5.64it/s, loss=74.2, v_num=0, train_loss_step=73.00, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  32%|███▏      | 13/41 [00:02<00:04,  5.64it/s, loss=74.2, v_num=0, train_loss_step=73.00, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  32%|███▏      | 13/41 [00:02<00:04,  5.64it/s, loss=74.8, v_num=0, train_loss_step=97.00, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  34%|███▍      | 14/41 [00:02<00:04,  5.62it/s, loss=74.8, v_num=0, train_loss_step=97.00, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  34%|███▍      | 14/41 [00:02<00:04,  5.62it/s, loss=74.8, v_num=0, train_loss_step=97.00, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  34%|███▍      | 14/41 [00:02<00:04,  5.62it/s, loss=74.8, v_num=0, train_loss_step=69.70, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  37%|███▋      | 15/41 [00:02<00:04,  5.63it/s, loss=74.8, v_num=0, train_loss_step=69.70, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  37%|███▋      | 15/41 [00:02<00:04,  5.63it/s, loss=74.8, v_num=0, train_loss_step=69.70, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  37%|███▋      | 15/41 [00:02<00:04,  5.63it/s, loss=73.8, v_num=0, train_loss_step=58.00, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  39%|███▉      | 16/41 [00:02<00:04,  5.64it/s, loss=73.8, v_num=0, train_loss_step=58.00, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  39%|███▉      | 16/41 [00:02<00:04,  5.63it/s, loss=73.8, v_num=0, train_loss_step=58.00, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  39%|███▉      | 16/41 [00:02<00:04,  5.63it/s, loss=74.6, v_num=0, train_loss_step=82.50, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  41%|████▏     | 17/41 [00:03<00:04,  5.64it/s, loss=74.6, v_num=0, train_loss_step=82.50, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  41%|████▏     | 17/41 [00:03<00:04,  5.64it/s, loss=74.6, v_num=0, train_loss_step=82.50, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  41%|████▏     | 17/41 [00:03<00:04,  5.64it/s, loss=74.8, v_num=0, train_loss_step=82.00, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  44%|████▍     | 18/41 [00:03<00:04,  5.65it/s, loss=74.8, v_num=0, train_loss_step=82.00, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  44%|████▍     | 18/41 [00:03<00:04,  5.65it/s, loss=74.8, v_num=0, train_loss_step=82.00, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  44%|████▍     | 18/41 [00:03<00:04,  5.65it/s, loss=74.6, v_num=0, train_loss_step=62.30, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  46%|████▋     | 19/41 [00:03<00:03,  5.66it/s, loss=74.6, v_num=0, train_loss_step=62.30, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  46%|████▋     | 19/41 [00:03<00:03,  5.65it/s, loss=74.6, v_num=0, train_loss_step=62.30, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  46%|████▋     | 19/41 [00:03<00:03,  5.65it/s, loss=74.2, v_num=0, train_loss_step=69.50, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  49%|████▉     | 20/41 [00:03<00:03,  5.64it/s, loss=74.2, v_num=0, train_loss_step=69.50, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  49%|████▉     | 20/41 [00:03<00:03,  5.64it/s, loss=74.2, v_num=0, train_loss_step=69.50, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  49%|████▉     | 20/41 [00:03<00:03,  5.64it/s, loss=73.4, v_num=0, train_loss_step=67.30, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  51%|█████     | 21/41 [00:03<00:03,  5.65it/s, loss=73.4, v_num=0, train_loss_step=67.30, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  51%|█████     | 21/41 [00:03<00:03,  5.65it/s, loss=73.4, v_num=0, train_loss_step=67.30, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  51%|█████     | 21/41 [00:03<00:03,  5.65it/s, loss=73.1, v_num=0, train_loss_step=76.50, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  54%|█████▎    | 22/41 [00:03<00:03,  5.64it/s, loss=73.1, v_num=0, train_loss_step=76.50, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  54%|█████▎    | 22/41 [00:03<00:03,  5.64it/s, loss=73.1, v_num=0, train_loss_step=76.50, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  54%|█████▎    | 22/41 [00:03<00:03,  5.64it/s, loss=73.4, v_num=0, train_loss_step=68.10, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  56%|█████▌    | 23/41 [00:04<00:03,  5.60it/s, loss=73.4, v_num=0, train_loss_step=68.10, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  56%|█████▌    | 23/41 [00:04<00:03,  5.59it/s, loss=73.4, v_num=0, train_loss_step=68.10, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  56%|█████▌    | 23/41 [00:04<00:03,  5.59it/s, loss=74.1, v_num=0, train_loss_step=98.40, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  59%|█████▊    | 24/41 [00:04<00:03,  5.56it/s, loss=74.1, v_num=0, train_loss_step=98.40, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  59%|█████▊    | 24/41 [00:04<00:03,  5.56it/s, loss=74.1, v_num=0, train_loss_step=98.40, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  59%|█████▊    | 24/41 [00:04<00:03,  5.56it/s, loss=73, v_num=0, train_loss_step=53.90, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  61%|██████    | 25/41 [00:04<00:02,  5.52it/s, loss=73, v_num=0, train_loss_step=53.90, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  61%|██████    | 25/41 [00:04<00:02,  5.52it/s, loss=73, v_num=0, train_loss_step=53.90, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  61%|██████    | 25/41 [00:04<00:02,  5.52it/s, loss=72.8, v_num=0, train_loss_step=65.30, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  63%|██████▎   | 26/41 [00:04<00:02,  5.48it/s, loss=72.8, v_num=0, train_loss_step=65.30, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  63%|██████▎   | 26/41 [00:04<00:02,  5.48it/s, loss=72.8, v_num=0, train_loss_step=65.30, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  63%|██████▎   | 26/41 [00:04<00:02,  5.48it/s, loss=71.9, v_num=0, train_loss_step=58.20, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  66%|██████▌   | 27/41 [00:04<00:02,  5.44it/s, loss=71.9, v_num=0, train_loss_step=58.20, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  66%|██████▌   | 27/41 [00:04<00:02,  5.44it/s, loss=71.9, v_num=0, train_loss_step=58.20, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  66%|██████▌   | 27/41 [00:04<00:02,  5.44it/s, loss=72.2, v_num=0, train_loss_step=72.90, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  68%|██████▊   | 28/41 [00:05<00:02,  5.41it/s, loss=72.2, v_num=0, train_loss_step=72.90, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  68%|██████▊   | 28/41 [00:05<00:02,  5.41it/s, loss=72.2, v_num=0, train_loss_step=72.90, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  68%|██████▊   | 28/41 [00:05<00:02,  5.41it/s, loss=71.4, v_num=0, train_loss_step=61.00, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  71%|███████   | 29/41 [00:05<00:02,  5.39it/s, loss=71.4, v_num=0, train_loss_step=61.00, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  71%|███████   | 29/41 [00:05<00:02,  5.39it/s, loss=71.4, v_num=0, train_loss_step=61.00, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  71%|███████   | 29/41 [00:05<00:02,  5.39it/s, loss=70.9, v_num=0, train_loss_step=67.40, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  73%|███████▎  | 30/41 [00:05<00:02,  5.37it/s, loss=70.9, v_num=0, train_loss_step=67.40, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  73%|███████▎  | 30/41 [00:05<00:02,  5.37it/s, loss=70.9, v_num=0, train_loss_step=67.40, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  73%|███████▎  | 30/41 [00:05<00:02,  5.37it/s, loss=72.2, v_num=0, train_loss_step=94.60, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  76%|███████▌  | 31/41 [00:05<00:01,  5.36it/s, loss=72.2, v_num=0, train_loss_step=94.60, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  76%|███████▌  | 31/41 [00:05<00:01,  5.36it/s, loss=72.2, v_num=0, train_loss_step=94.60, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  76%|███████▌  | 31/41 [00:05<00:01,  5.36it/s, loss=72.6, v_num=0, train_loss_step=75.20, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  78%|███████▊  | 32/41 [00:06<00:01,  5.33it/s, loss=72.6, v_num=0, train_loss_step=75.20, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  78%|███████▊  | 32/41 [00:06<00:01,  5.33it/s, loss=72.6, v_num=0, train_loss_step=75.20, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  78%|███████▊  | 32/41 [00:06<00:01,  5.33it/s, loss=72.4, v_num=0, train_loss_step=67.90, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  80%|████████  | 33/41 [00:06<00:01,  5.31it/s, loss=72.4, v_num=0, train_loss_step=67.90, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  80%|████████  | 33/41 [00:06<00:01,  5.31it/s, loss=72.4, v_num=0, train_loss_step=67.90, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  80%|████████  | 33/41 [00:06<00:01,  5.31it/s, loss=70.7, v_num=0, train_loss_step=63.10, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  83%|████████▎ | 34/41 [00:06<00:01,  5.29it/s, loss=70.7, v_num=0, train_loss_step=63.10, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  83%|████████▎ | 34/41 [00:06<00:01,  5.29it/s, loss=70.7, v_num=0, train_loss_step=63.10, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  83%|████████▎ | 34/41 [00:06<00:01,  5.29it/s, loss=71.1, v_num=0, train_loss_step=78.20, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  85%|████████▌ | 35/41 [00:06<00:01,  5.28it/s, loss=71.1, v_num=0, train_loss_step=78.20, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  85%|████████▌ | 35/41 [00:06<00:01,  5.28it/s, loss=71.1, v_num=0, train_loss_step=78.20, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  85%|████████▌ | 35/41 [00:06<00:01,  5.28it/s, loss=72, v_num=0, train_loss_step=74.70, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  88%|████████▊ | 36/41 [00:07<00:00,  5.03it/s, loss=72, v_num=0, train_loss_step=74.70, val_loss=153.0, train_loss_epoch=74.20]#015Epoch 22:  88%|████████▊ | 36/41 [00:07<00:00,  5.03it/s, loss=72, v_num=0, train_loss_step=74.70, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  88%|████████▊ | 36/41 [00:07<00:00,  5.03it/s, loss=71.1, v_num=0, train_loss_step=65.40, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  90%|█████████ | 37/41 [00:07<00:00,  5.04it/s, loss=71.1, v_num=0, train_loss_step=65.40, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  90%|█████████ | 37/41 [00:07<00:00,  5.04it/s, loss=71.1, v_num=0, train_loss_step=65.40, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  90%|█████████ | 37/41 [00:07<00:00,  5.04it/s, loss=71, v_num=0, train_loss_step=80.90, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  93%|█████████▎| 38/41 [00:07<00:00,  5.03it/s, loss=71, v_num=0, train_loss_step=80.90, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  93%|█████████▎| 38/41 [00:07<00:00,  5.03it/s, loss=71, v_num=0, train_loss_step=80.90, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  93%|█████████▎| 38/41 [00:07<00:00,  5.03it/s, loss=72.6, v_num=0, train_loss_step=93.90, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  95%|█████████▌| 39/41 [00:07<00:00,  5.02it/s, loss=72.6, v_num=0, train_loss_step=93.90, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  95%|█████████▌| 39/41 [00:07<00:00,  5.02it/s, loss=72.6, v_num=0, train_loss_step=93.90, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  95%|█████████▌| 39/41 [00:07<00:00,  5.02it/s, loss=72.2, v_num=0, train_loss_step=61.30, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  98%|█████████▊| 40/41 [00:07<00:00,  5.01it/s, loss=72.2, v_num=0, train_loss_step=61.30, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  98%|█████████▊| 40/41 [00:07<00:00,  5.01it/s, loss=72.2, v_num=0, train_loss_step=61.30, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22:  98%|█████████▊| 40/41 [00:07<00:00,  5.01it/s, loss=72.5, v_num=0, train_loss_step=73.80, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 22: 100%|██████████| 41/41 [00:08<00:00,  4.97it/s, loss=72.5, v_num=0, train_loss_step=73.80, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22: 100%|██████████| 41/41 [00:08<00:00,  4.97it/s, loss=72.5, v_num=0, train_loss_step=73.80, val_loss=153.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34mEpoch 22: 100%|██████████| 41/41 [00:08<00:00,  4.66it/s, loss=72.5, v_num=0, train_loss_step=73.80, val_loss=152.0, train_loss_epoch=74.20]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 22: 100%|██████████| 41/41 [00:08<00:00,  4.62it/s, loss=72.5, v_num=0, train_loss_step=73.80, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 22:   0%|          | 0/41 [00:00<?, ?it/s, loss=72.5, v_num=0, train_loss_step=73.80, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:   0%|          | 0/41 [00:00<?, ?it/s, loss=72.5, v_num=0, train_loss_step=73.80, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:   2%|▏         | 1/41 [00:00<00:06,  5.83it/s, loss=72.5, v_num=0, train_loss_step=73.80, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:   2%|▏         | 1/41 [00:00<00:06,  5.82it/s, loss=72.5, v_num=0, train_loss_step=73.80, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:   2%|▏         | 1/41 [00:00<00:06,  5.81it/s, loss=72.5, v_num=0, train_loss_step=76.50, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:   5%|▍         | 2/41 [00:00<00:06,  5.78it/s, loss=72.5, v_num=0, train_loss_step=76.50, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:   5%|▍         | 2/41 [00:00<00:06,  5.78it/s, loss=72.5, v_num=0, train_loss_step=76.50, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:   5%|▍         | 2/41 [00:00<00:06,  5.77it/s, loss=72.6, v_num=0, train_loss_step=69.80, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:   7%|▋         | 3/41 [00:00<00:06,  5.43it/s, loss=72.6, v_num=0, train_loss_step=69.80, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:   7%|▋         | 3/41 [00:00<00:06,  5.43it/s, loss=72.6, v_num=0, train_loss_step=69.80, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:   7%|▋         | 3/41 [00:00<00:07,  5.43it/s, loss=71.7, v_num=0, train_loss_step=80.20, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  10%|▉         | 4/41 [00:00<00:07,  5.21it/s, loss=71.7, v_num=0, train_loss_step=80.20, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  10%|▉         | 4/41 [00:00<00:07,  5.20it/s, loss=71.7, v_num=0, train_loss_step=80.20, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  10%|▉         | 4/41 [00:00<00:07,  5.20it/s, loss=72.4, v_num=0, train_loss_step=67.60, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  12%|█▏        | 5/41 [00:00<00:07,  5.10it/s, loss=72.4, v_num=0, train_loss_step=67.60, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  12%|█▏        | 5/41 [00:00<00:07,  5.10it/s, loss=72.4, v_num=0, train_loss_step=67.60, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  12%|█▏        | 5/41 [00:00<00:07,  5.10it/s, loss=72.8, v_num=0, train_loss_step=73.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  15%|█▍        | 6/41 [00:01<00:06,  5.06it/s, loss=72.8, v_num=0, train_loss_step=73.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  15%|█▍        | 6/41 [00:01<00:06,  5.06it/s, loss=72.8, v_num=0, train_loss_step=73.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  15%|█▍        | 6/41 [00:01<00:06,  5.06it/s, loss=73.7, v_num=0, train_loss_step=76.00, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  17%|█▋        | 7/41 [00:01<00:06,  5.02it/s, loss=73.7, v_num=0, train_loss_step=76.00, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  17%|█▋        | 7/41 [00:01<00:06,  5.02it/s, loss=73.7, v_num=0, train_loss_step=76.00, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  17%|█▋        | 7/41 [00:01<00:06,  5.02it/s, loss=73.2, v_num=0, train_loss_step=62.70, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  20%|█▉        | 8/41 [00:01<00:06,  4.93it/s, loss=73.2, v_num=0, train_loss_step=62.70, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  20%|█▉        | 8/41 [00:01<00:06,  4.93it/s, loss=73.2, v_num=0, train_loss_step=62.70, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  20%|█▉        | 8/41 [00:01<00:06,  4.92it/s, loss=73.3, v_num=0, train_loss_step=62.50, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  22%|██▏       | 9/41 [00:01<00:06,  4.90it/s, loss=73.3, v_num=0, train_loss_step=62.50, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  22%|██▏       | 9/41 [00:01<00:06,  4.90it/s, loss=73.3, v_num=0, train_loss_step=62.50, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  22%|██▏       | 9/41 [00:01<00:06,  4.90it/s, loss=73.5, v_num=0, train_loss_step=72.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  24%|██▍       | 10/41 [00:02<00:06,  4.93it/s, loss=73.5, v_num=0, train_loss_step=72.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  24%|██▍       | 10/41 [00:02<00:06,  4.93it/s, loss=73.5, v_num=0, train_loss_step=72.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  24%|██▍       | 10/41 [00:02<00:06,  4.93it/s, loss=71.6, v_num=0, train_loss_step=56.20, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  27%|██▋       | 11/41 [00:02<00:06,  4.91it/s, loss=71.6, v_num=0, train_loss_step=56.20, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  27%|██▋       | 11/41 [00:02<00:06,  4.91it/s, loss=71.6, v_num=0, train_loss_step=56.20, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  27%|██▋       | 11/41 [00:02<00:06,  4.91it/s, loss=72.5, v_num=0, train_loss_step=93.30, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  29%|██▉       | 12/41 [00:02<00:05,  4.91it/s, loss=72.5, v_num=0, train_loss_step=93.30, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  29%|██▉       | 12/41 [00:02<00:05,  4.91it/s, loss=72.5, v_num=0, train_loss_step=93.30, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  29%|██▉       | 12/41 [00:02<00:05,  4.91it/s, loss=72.5, v_num=0, train_loss_step=68.80, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  32%|███▏      | 13/41 [00:02<00:05,  4.90it/s, loss=72.5, v_num=0, train_loss_step=68.80, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  32%|███▏      | 13/41 [00:02<00:05,  4.90it/s, loss=72.5, v_num=0, train_loss_step=68.80, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  32%|███▏      | 13/41 [00:02<00:05,  4.90it/s, loss=73.8, v_num=0, train_loss_step=88.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  34%|███▍      | 14/41 [00:02<00:05,  4.89it/s, loss=73.8, v_num=0, train_loss_step=88.40, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  34%|███▍      | 14/41 [00:02<00:05,  4.89it/s, loss=73.8, v_num=0, train_loss_step=88.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  34%|███▍      | 14/41 [00:02<00:05,  4.89it/s, loss=73.9, v_num=0, train_loss_step=79.30, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  37%|███▋      | 15/41 [00:03<00:05,  4.89it/s, loss=73.9, v_num=0, train_loss_step=79.30, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  37%|███▋      | 15/41 [00:03<00:05,  4.89it/s, loss=73.9, v_num=0, train_loss_step=79.30, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  37%|███▋      | 15/41 [00:03<00:05,  4.89it/s, loss=74.3, v_num=0, train_loss_step=84.30, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  39%|███▉      | 16/41 [00:03<00:05,  4.87it/s, loss=74.3, v_num=0, train_loss_step=84.30, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  39%|███▉      | 16/41 [00:03<00:05,  4.87it/s, loss=74.3, v_num=0, train_loss_step=84.30, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  39%|███▉      | 16/41 [00:03<00:05,  4.87it/s, loss=74.8, v_num=0, train_loss_step=74.30, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  41%|████▏     | 17/41 [00:03<00:04,  4.87it/s, loss=74.8, v_num=0, train_loss_step=74.30, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  41%|████▏     | 17/41 [00:03<00:04,  4.87it/s, loss=74.8, v_num=0, train_loss_step=74.30, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  41%|████▏     | 17/41 [00:03<00:04,  4.86it/s, loss=74.8, v_num=0, train_loss_step=81.00, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  44%|████▍     | 18/41 [00:03<00:04,  4.87it/s, loss=74.8, v_num=0, train_loss_step=81.00, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  44%|████▍     | 18/41 [00:03<00:04,  4.87it/s, loss=74.8, v_num=0, train_loss_step=81.00, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  44%|████▍     | 18/41 [00:03<00:04,  4.87it/s, loss=74.7, v_num=0, train_loss_step=92.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  46%|████▋     | 19/41 [00:03<00:04,  4.87it/s, loss=74.7, v_num=0, train_loss_step=92.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  46%|████▋     | 19/41 [00:03<00:04,  4.86it/s, loss=74.7, v_num=0, train_loss_step=92.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  46%|████▋     | 19/41 [00:03<00:04,  4.86it/s, loss=75.2, v_num=0, train_loss_step=70.70, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  49%|████▉     | 20/41 [00:04<00:04,  4.86it/s, loss=75.2, v_num=0, train_loss_step=70.70, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  49%|████▉     | 20/41 [00:04<00:04,  4.86it/s, loss=75.2, v_num=0, train_loss_step=70.70, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  49%|████▉     | 20/41 [00:04<00:04,  4.86it/s, loss=74.3, v_num=0, train_loss_step=56.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  51%|█████     | 21/41 [00:04<00:04,  4.86it/s, loss=74.3, v_num=0, train_loss_step=56.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  51%|█████     | 21/41 [00:04<00:04,  4.86it/s, loss=74.3, v_num=0, train_loss_step=56.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  51%|█████     | 21/41 [00:04<00:04,  4.86it/s, loss=74.1, v_num=0, train_loss_step=72.10, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  54%|█████▎    | 22/41 [00:04<00:03,  4.85it/s, loss=74.1, v_num=0, train_loss_step=72.10, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  54%|█████▎    | 22/41 [00:04<00:03,  4.85it/s, loss=74.1, v_num=0, train_loss_step=72.10, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  54%|█████▎    | 22/41 [00:04<00:03,  4.85it/s, loss=73.7, v_num=0, train_loss_step=61.80, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  56%|█████▌    | 23/41 [00:04<00:03,  4.84it/s, loss=73.7, v_num=0, train_loss_step=61.80, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  56%|█████▌    | 23/41 [00:04<00:03,  4.84it/s, loss=73.7, v_num=0, train_loss_step=61.80, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  56%|█████▌    | 23/41 [00:04<00:03,  4.84it/s, loss=73.9, v_num=0, train_loss_step=85.00, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  59%|█████▊    | 24/41 [00:04<00:03,  4.84it/s, loss=73.9, v_num=0, train_loss_step=85.00, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  59%|█████▊    | 24/41 [00:04<00:03,  4.84it/s, loss=73.9, v_num=0, train_loss_step=85.00, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  59%|█████▊    | 24/41 [00:04<00:03,  4.84it/s, loss=74.4, v_num=0, train_loss_step=77.50, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  61%|██████    | 25/41 [00:05<00:03,  4.84it/s, loss=74.4, v_num=0, train_loss_step=77.50, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  61%|██████    | 25/41 [00:05<00:03,  4.84it/s, loss=74.4, v_num=0, train_loss_step=77.50, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  61%|██████    | 25/41 [00:05<00:03,  4.84it/s, loss=74.9, v_num=0, train_loss_step=83.20, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  63%|██████▎   | 26/41 [00:05<00:03,  4.84it/s, loss=74.9, v_num=0, train_loss_step=83.20, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  63%|██████▎   | 26/41 [00:05<00:03,  4.84it/s, loss=74.9, v_num=0, train_loss_step=83.20, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  63%|██████▎   | 26/41 [00:05<00:03,  4.84it/s, loss=74.1, v_num=0, train_loss_step=58.90, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  66%|██████▌   | 27/41 [00:05<00:02,  4.84it/s, loss=74.1, v_num=0, train_loss_step=58.90, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  66%|██████▌   | 27/41 [00:05<00:02,  4.84it/s, loss=74.1, v_num=0, train_loss_step=58.90, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  66%|██████▌   | 27/41 [00:05<00:02,  4.84it/s, loss=74.4, v_num=0, train_loss_step=70.30, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  68%|██████▊   | 28/41 [00:05<00:02,  4.84it/s, loss=74.4, v_num=0, train_loss_step=70.30, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  68%|██████▊   | 28/41 [00:05<00:02,  4.84it/s, loss=74.4, v_num=0, train_loss_step=70.30, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  68%|██████▊   | 28/41 [00:05<00:02,  4.83it/s, loss=75.5, v_num=0, train_loss_step=84.70, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  71%|███████   | 29/41 [00:06<00:02,  4.83it/s, loss=75.5, v_num=0, train_loss_step=84.70, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  71%|███████   | 29/41 [00:06<00:02,  4.83it/s, loss=75.5, v_num=0, train_loss_step=84.70, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  71%|███████   | 29/41 [00:06<00:02,  4.83it/s, loss=75.7, v_num=0, train_loss_step=76.30, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  73%|███████▎  | 30/41 [00:06<00:02,  4.83it/s, loss=75.7, v_num=0, train_loss_step=76.30, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  73%|███████▎  | 30/41 [00:06<00:02,  4.83it/s, loss=75.7, v_num=0, train_loss_step=76.30, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  73%|███████▎  | 30/41 [00:06<00:02,  4.83it/s, loss=77.5, v_num=0, train_loss_step=91.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  76%|███████▌  | 31/41 [00:06<00:02,  4.83it/s, loss=77.5, v_num=0, train_loss_step=91.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  76%|███████▌  | 31/41 [00:06<00:02,  4.83it/s, loss=77.5, v_num=0, train_loss_step=91.40, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  76%|███████▌  | 31/41 [00:06<00:02,  4.83it/s, loss=76.4, v_num=0, train_loss_step=71.90, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  78%|███████▊  | 32/41 [00:06<00:01,  4.83it/s, loss=76.4, v_num=0, train_loss_step=71.90, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  78%|███████▊  | 32/41 [00:06<00:01,  4.83it/s, loss=76.4, v_num=0, train_loss_step=71.90, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  78%|███████▊  | 32/41 [00:06<00:01,  4.83it/s, loss=76.4, v_num=0, train_loss_step=68.00, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  80%|████████  | 33/41 [00:06<00:01,  4.83it/s, loss=76.4, v_num=0, train_loss_step=68.00, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  80%|████████  | 33/41 [00:06<00:01,  4.83it/s, loss=76.4, v_num=0, train_loss_step=68.00, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  80%|████████  | 33/41 [00:06<00:01,  4.83it/s, loss=76.4, v_num=0, train_loss_step=89.20, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  83%|████████▎ | 34/41 [00:07<00:01,  4.83it/s, loss=76.4, v_num=0, train_loss_step=89.20, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  83%|████████▎ | 34/41 [00:07<00:01,  4.83it/s, loss=76.4, v_num=0, train_loss_step=89.20, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  83%|████████▎ | 34/41 [00:07<00:01,  4.83it/s, loss=76.5, v_num=0, train_loss_step=80.20, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  85%|████████▌ | 35/41 [00:07<00:01,  4.83it/s, loss=76.5, v_num=0, train_loss_step=80.20, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  85%|████████▌ | 35/41 [00:07<00:01,  4.82it/s, loss=76.5, v_num=0, train_loss_step=80.20, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  85%|████████▌ | 35/41 [00:07<00:01,  4.82it/s, loss=75.6, v_num=0, train_loss_step=67.20, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  88%|████████▊ | 36/41 [00:07<00:01,  4.82it/s, loss=75.6, v_num=0, train_loss_step=67.20, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  88%|████████▊ | 36/41 [00:07<00:01,  4.82it/s, loss=75.6, v_num=0, train_loss_step=67.20, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  88%|████████▊ | 36/41 [00:07<00:01,  4.82it/s, loss=74.7, v_num=0, train_loss_step=56.60, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  90%|█████████ | 37/41 [00:07<00:00,  4.82it/s, loss=74.7, v_num=0, train_loss_step=56.60, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  90%|█████████ | 37/41 [00:07<00:00,  4.82it/s, loss=74.7, v_num=0, train_loss_step=56.60, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  90%|█████████ | 37/41 [00:07<00:00,  4.82it/s, loss=74.2, v_num=0, train_loss_step=70.10, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  93%|█████████▎| 38/41 [00:07<00:00,  4.83it/s, loss=74.2, v_num=0, train_loss_step=70.10, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23:  93%|█████████▎| 38/41 [00:07<00:00,  4.83it/s, loss=74.2, v_num=0, train_loss_step=70.10, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  93%|█████████▎| 38/41 [00:07<00:00,  4.83it/s, loss=71.9, v_num=0, train_loss_step=46.70, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  95%|█████████▌| 39/41 [00:08<00:00,  4.83it/s, loss=71.9, v_num=0, train_loss_step=46.70, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  95%|█████████▌| 39/41 [00:08<00:00,  4.83it/s, loss=71.9, v_num=0, train_loss_step=46.70, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  95%|█████████▌| 39/41 [00:08<00:00,  4.83it/s, loss=71.4, v_num=0, train_loss_step=59.90, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  98%|█████████▊| 40/41 [00:08<00:00,  4.82it/s, loss=71.4, v_num=0, train_loss_step=59.90, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  98%|█████████▊| 40/41 [00:08<00:00,  4.82it/s, loss=71.4, v_num=0, train_loss_step=59.90, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23:  98%|█████████▊| 40/41 [00:08<00:00,  4.82it/s, loss=72.8, v_num=0, train_loss_step=84.70, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.93it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 23: 100%|██████████| 41/41 [00:08<00:00,  4.76it/s, loss=72.8, v_num=0, train_loss_step=84.70, val_loss=152.0, train_loss_epoch=73.00]#015Epoch 23: 100%|██████████| 41/41 [00:08<00:00,  4.76it/s, loss=72.8, v_num=0, train_loss_step=84.70, val_loss=152.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34mEpoch 23: 100%|██████████| 41/41 [00:09<00:00,  4.40it/s, loss=72.8, v_num=0, train_loss_step=84.70, val_loss=150.0, train_loss_epoch=73.00]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 23: 100%|██████████| 41/41 [00:09<00:00,  4.40it/s, loss=72.8, v_num=0, train_loss_step=84.70, val_loss=150.0, train_loss_epoch=72.60]\u001b[0m\n",
      "\u001b[34mEpoch 23: 100%|██████████| 41/41 [00:09<00:00,  4.36it/s, loss=72.8, v_num=0, train_loss_step=84.70, val_loss=150.0, train_loss_epoch=72.60]\u001b[0m\n",
      "\u001b[34m2023-04-03 14:54:38,606 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-04-03 14:54:38,606 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-04-03 14:54:38,607 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-04-03 14:55:37 Uploading - Uploading generated training model\n",
      "2023-04-03 14:56:00 Completed - Training job completed\n",
      "ProfilerReport: NoIssuesFound\n",
      "Training seconds: 560\n",
      "Billable seconds: 560\n"
     ]
    }
   ],
   "source": [
    "estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb0cd08-bf35-4fb4-96cf-b89ae2f8f421",
   "metadata": {},
   "source": [
    "# 4. 모델 가중치 파일 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6f718a0-c680-4570-8113-1a3c27521d27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model artifact: \n",
      " s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-03-07-06-33-33-202/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(\"model artifact: \\n\", estimator.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a03149-1b2f-4038-9890-27a163c7bc94",
   "metadata": {},
   "source": [
    "# 5. SageMaker Debug Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4e2be-fd11-406a-8bf2-7f166638e481",
   "metadata": {},
   "source": [
    "SageMaker Studio 에 로긴하여 Experiment 메뉴 클릭 후에 Unassigned runs 클릭\n",
    "![sm_debug_01.png](img/sm_debug_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d15d10c-d4e2-4bc6-8fcb-e6ec5b48bddd",
   "metadata": {},
   "source": [
    "실행한 실험을 클릭. 아래의 예시는 가장 최근의 실험을 클릭 함. 그리고 왼쪽 메뉴에서 Debug 를 클릭 후에 하단의 Training job 을 클릭\n",
    "![sm_debug_02.png](img/sm_debug_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff1951a-0f8c-4250-a8fb-7f5e5051ff8e",
   "metadata": {},
   "source": [
    "\"Download report\" 를 클릭하여 리포트를 다운로드 함.\n",
    "![sm_debug_03.png](img/sm_debug_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff726123-4fb7-40de-937b-92a923d1b790",
   "metadata": {},
   "source": [
    "## Debug Profiler Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e126c-0b0e-4d12-8d6d-194cbbd54889",
   "metadata": {},
   "source": [
    "클릭하여 다운로드 --> [profiler-report.pdf](img/profiler-report.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
