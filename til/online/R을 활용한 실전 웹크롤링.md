# 커리큘럼 .



## 웹크롤링 구조 이해부터 실전 웹크롤링, 자동화까지! R을 활용한 웹 크롤링의 A-Z가 담긴 꽉찬 4주 커리큘럼



| PART1. 웹 크롤링을 위한 기초 학습 (HTTP 요청과 응답, HTML 기초) |                                                              |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 웹 사이트의 구조를 이해하고 크롬 개발자도구를 활용하여 원하는 정보를 쉽고 빠르게 찾을 수 있습니다. |                                                              |                                                              |
| 1회차                                                        | 이론                                                         | - R 프로그래밍 기초 리뷰 - 웹 크롤링 프로세스의 이해 및 크롤링을 할 때의 주의사항  - 웹사이트에서 어떻게 데이터를 추출하는 것일까? - 웹 사이트는 어떻게 만들어져있을까? (HTML의 이해) |
| 실습                                                         | - 네이버 홈페이지의 구조 뜯어보기  - HTTP 요청 방식 중 하나인 GET 방식 이해하기 - 원하는 데이터 정보를 손쉽게 찾아주는 크롤링을 도와주는 크롬 개발자도구 사용법 익히기 - 네이버와 다음의 실시간 검색어 수집하기 - HTTP 요청 및 응답 상태코드를 확인할 수 있는 함수활용법 익히기 (httr 패키지) |                                                              |
| 2회차                                                        | 이론                                                         | - 크롤링에 필요한 국가, 시간 등 사용자가 원하는 인터페이스로 설정하기 (로케일) - 수집한 한글 데이터가 깨지지 않도록 한글 인코딩 방식 이해하기 |
| 실습                                                         | - 모바일 쇼핑몰에서 상품 정보 수집하기 (GS SHOP, 홈&쇼핑) - 위키백과 서울의 대학 목록 크롤링하기  - 네이버 증권 업종별 시세별 시세 크롤링하기 - 현재 로케일 / 한글 인코딩 방식을 확인하고 변경하기 |                                                              |
| Part2. 웹 크롤링에서 발생하는 에러 대처하기                  |                                                              |                                                              |
| 웹 크롤링을 진행함에 있어 자주 겪게 되는 다양한 종류의 에러를 이해하고 이를 해결할 수 있는 방법을 알아봅니다. |                                                              |                                                              |
| 3회차                                                        | 이론                                                         | - HTTP 요청 방식 중 하나인 POST 방식 이해하기  - 웹 서버가 문자를 인식하는 방법인 ‘퍼센트 인코딩’에 대한 이해 |
| 실습                                                         | - 사업체 전화번호 크롤링하기 (isuperpage)  - 온라인 서점에서 특정 단어로 검색되는 책 목록 수집하기 (교보문고) |                                                              |
| 4회차                                                        | 이론                                                         | - HTTP 요청 및 응답 과정에서 발생하는 다양한 에러 대처 방법 - 로그인한 상태로 웹 크롤링하기 위한 쿠키 사용법 이해 |
| 실습                                                         | - HTTP 요청 시 User-agent를 추가하여 에러 회피하기 (네이버 부동산) - 반복문으로 여러 페이지에 있는 전체 데이터 수집하기 (네이버 부동산) - HTTP 요청 시 referer를 추가하여 정상적으로 응답 받기 (네이버 카페) - 반복문 안에 에러 회피 방법 설정하기 (네이버 카페) - 로그인한 상태로 기업리뷰 수집하기 (잡플래닛) |                                                              |
| Part3. 실전! 원하는 페이지를 크롤링해보자                    |                                                              |                                                              |
| JavaScript와 RSelenium을 활용한 웹 크롤링 방법에 대해서 배웁니다. API가 제공되었을 경우의 크롤링도 해봅니다. |                                                              |                                                              |
| 5회차                                                        | 이론                                                         | - JavaScript가 사용된 웹사이트의 크롤링을 위한 Ajax와 XHR의 이해 - 수집하려는 데이터가 여러 페이지에 나뉘어 있는 Page Navigation 활용법 알아보기 |
| 실습                                                         | - HTTP 요청 및 응답 과정에서 클라이언트와 웹 서버 간 상호작용의 결과로 생성되는 다양한 파일 중에서 필요한 파일을 크롬 개발자도구에서 찾는 방법 - 여러 페이지에 걸쳐 있는 전체 데이터 수집하기 (네이버 카페) - JavaScript를 활용하여 데이터 수집하기 (네이버 스포츠 뉴스, 네이버 블로그) - 2018 프로야구 타자 스탯 정보 수집하기 (KBReport) |                                                              |
| 6회차                                                        | 이론                                                         | - Selenium 소개 및 RSelenium을 활용한 웹 크롤링 과정 이해 - Open API 소개 및 활용법에 대한 이해 - Open API의 주요 응답 데이터 형식인 XML과 JSON에 대한 이해 |
| 실습                                                         | - 원격으로 웹 브라우저를 구동시키기 위한 환경 설정 : Java 설치 및 경로 설정, Selenium Server Standalone 및 Chrome driver 설치 - RSelenium 패키지의 주요 함수 활용방법 익히기 - 네이버 로그인이 필요한 데이터 수집하기 (네이버 카페) - 공공데이터 포털에서 수집할 데이터 항목을 찾고 활용신청하기 - Open API를 활용한 데이터 수집 (아파트 실거래가, 나라장터 낙찰리스트) |                                                              |
| Part4. 텍스트 전처리하기 & 웹크롤링 자동화하기               |                                                              |                                                              |
| 크롤링한 데이터를 stringr 패키지 함수와 정규표현식을 활용하여 전처리합니다. 또, 웹 크롤링을 정기적으로 자동실행하고 결과를 메신저로 전송하는 방법을 배웁니다. |                                                              |                                                              |
| 7회차                                                        | 이론                                                         | - 텍스트 데이터 전처리의 필요성 -  텍스트 데이터 전처리를 빠르게 하기 위한 정규표현식의 이해 |
| 실습                                                         | - 텍스트 처리에 특화된 stringr 패키지의 주요 함수 활용법 익히기 - 정규표현식을 활용하여 특정 패턴을 찾고, 대체하고, 삭제하기 훈련 - 그동안 수집한 데이터의 전처리 (네이버 부동산, 네이버 카페, 네이버 블로그) |                                                              |
| 8회차                                                        | 이론                                                         | - 웹 크롤링 자동 실행을 위한 정기 작업의 이해 (Taskscheduler 및 Crontab) - 정기 작업 설정을 위한 R 패키지 소개 (taskschdeuleR 및 cronR) |
| 실습                                                         | - 가상화폐 거래소에서 제공하는 API를 활용한 현재시세 수집하기 - 메신저 봇 설정하기 (Slack 및 Telegram) - 웹 크롤러 실행 결과를 메신저 봇으로 전송하기 (가상화폐 시세 + Telegram) - 웹 크롤러 실행을 정기 작업으로 설정하기 (분, 시간, 일, 주, 월 단위 등) |                                                              |